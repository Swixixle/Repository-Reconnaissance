Below is a **copy/paste spec** for Replit Agents. It is written as **precise implementation instructions** for a “Live Static CI Feed” (event-driven static analysis) using **GitHub webhooks → job queue → analyzer worker → stored runs → UI feed**.

No runtime telemetry. **Only static re-analysis on repo events**.

---

## Replit Agent Instructions: Live Static CI Feed (GitHub Webhook → Queue → Worker → Feed)

### Objective

Implement a “Live Static CI Feed” so that when GitHub emits events (push/PR), the system:

1. receives the webhook
2. validates the signature
3. enqueues an analysis job
4. a background worker pulls jobs and runs the existing analyzer (static)
5. stores run metadata + artifacts pointers
6. UI shows a feed of runs + status + links to results

### Non-goals

* No runtime tracing/log ingestion
* No full CI runner orchestration
* No live code execution
* No “security scanner” claims
* No long-polling websockets required (simple polling is fine)

---

# Phase 0 — Inventory and Constraints (DO THIS FIRST)

1. Locate the current backend server entrypoint and routing structure (e.g. `server/index.ts`, `routes.ts`).
2. Locate how “analysis runs” are currently triggered (existing endpoint, CLI invocation, file output to `out/`).
3. Locate storage layer (Postgres via Drizzle or similar). Identify where to add tables.
4. Confirm deployment environment supports:

   * persistent DB (Postgres recommended)
   * persistent filesystem OR artifact storage strategy (can keep artifacts in DB or on disk with pointers)

**Deliverable:** A short internal note in repo comments or README-dev describing:

* command used to run analyzer
* where outputs go
* how to load/view results today

---

# Phase 1 — Data Model (DB tables)

Add minimal tables to support queue + feed.

### Table: `ci_runs`

Fields:

* `id` (uuid, pk)
* `repo_owner` (text, not null)
* `repo_name` (text, not null)
* `ref` (text, not null) — branch name or PR head ref
* `commit_sha` (text, not null)
* `event_type` (text, not null) — `push` | `pull_request`
* `status` (text, not null) — `QUEUED` | `RUNNING` | `SUCCEEDED` | `FAILED`
* `created_at` (timestamptz, not null, default now)
* `started_at` (timestamptz, nullable)
* `finished_at` (timestamptz, nullable)
* `error` (text, nullable)
* `out_dir` (text, nullable) — pointer like `out/ci/<id>` or external URL
* `summary_json` (jsonb, nullable) — lightweight run summary (counts, DCI, RCI, etc.)

Indexes:

* `(repo_owner, repo_name, created_at desc)`
* `(commit_sha)`
* `(status, created_at)`

### Table: `ci_jobs`

(Queue table; separate from runs so retries don’t overwrite run metadata.)
Fields:

* `id` (uuid pk)
* `run_id` (uuid fk -> ci_runs.id, not null)
* `status` (text not null) — `READY` | `LEASED` | `DONE` | `DEAD`
* `attempts` (int not null default 0)
* `leased_until` (timestamptz nullable)
* `last_error` (text nullable)
* `created_at` (timestamptz not null default now)

Indexes:

* `(status, created_at)`
* `(leased_until)`

**Acceptance criteria:**

* Migrations apply cleanly
* Can insert a dummy run + job row and query them

---

# Phase 2 — GitHub Webhook Receiver (Backend)

Add endpoint: `POST /api/webhooks/github`

### Required security

* Verify GitHub webhook signature using `X-Hub-Signature-256`
* Secret stored in env: `GITHUB_WEBHOOK_SECRET`
* Reject if:

  * missing signature header
  * invalid HMAC
  * unsupported event

### Supported events

1. `push`

* Extract:

  * repo owner/name
  * branch (`ref` like `refs/heads/main` → `main`)
  * `after` commit SHA

2. `pull_request`

* Only handle actions: `opened`, `synchronize`, `reopened`
* Extract:

  * repo owner/name
  * PR number
  * head ref (`pull_request.head.ref`)
  * head sha (`pull_request.head.sha`)

### Behavior

On valid event:

1. Create `ci_runs` row with `QUEUED`
2. Create `ci_jobs` row with `READY` linked to run_id
3. Return `200 {"ok":true,"run_id":"..."}` quickly (no analysis inline)

Also implement idempotency for push events:

* If `(repo_owner, repo_name, commit_sha)` already has a run within last N hours, do not create duplicate; return existing run_id.

**Acceptance criteria:**

* Invalid signature → 401
* Unsupported event → 202 (ignored) with `{"ok":true,"ignored":true}`
* Valid push/PR event creates rows and responds fast (<200ms typical)

---

# Phase 3 — Worker (Background job processor)

Implement a worker loop that:

* polls `ci_jobs` for `READY` or expired `LEASED`
* leases one job at a time (transaction + SELECT FOR UPDATE)
* sets job `LEASED` with `leased_until = now + 5 minutes`
* updates corresponding run to `RUNNING`, sets `started_at`
* runs analyzer
* stores result pointers
* marks run `SUCCEEDED` or `FAILED`, sets finished_at
* marks job `DONE` or `DEAD`

### Leasing logic (must be correct)

In a DB transaction:

1. Select one job:

   * status `READY` OR (status `LEASED` AND `leased_until < now`)
   * order by created_at asc
   * `FOR UPDATE SKIP LOCKED`
2. Update it to `LEASED`, increment attempts, set leased_until
3. Return the job row

Max attempts: 3

* If attempts exceed 3 and still failing, mark job `DEAD` and run `FAILED`

### Analyzer execution contract

Do not guess; use the existing CLI entrypoint used elsewhere.
Typical pattern:

* checkout repo (see Phase 4)
* run python analyzer with args that specify output dir and deterministic mode

The worker must:

* write outputs to a stable location: `out/ci/<run_id>/...`
* save `out_dir` to `ci_runs.out_dir`
* parse/compute a small `summary_json` (counts + key scores already produced by analyzer if available)

### Run-time constraints

* Worker loop runs continuously in Replit “Always On” or as a separate “worker” process.
* If you cannot keep a separate worker running reliably, implement a fallback:

  * `POST /api/ci/worker/tick` that processes **at most 1 job**
  * UI can call tick every 10–30 seconds as a hacky but reliable runner

**Acceptance criteria:**

* Enqueued jobs get processed
* Runs transition QUEUED → RUNNING → SUCCEEDED/FAILED
* Jobs cannot be double-processed concurrently

---

# Phase 4 — Repo Fetch Strategy (How the worker gets code)

Implement one of these strategies (choose the one that matches current repo):

### Option A (Simplest): Analyze by GitHub URL + commit SHA

If analyzer already accepts GitHub URL:

* Use `https://github.com/<owner>/<repo>.git`
* Checkout commit SHA into temp dir
* Run analyzer against that directory

### Option B: GitHub API archive download

* Use GitHub tarball/zipball for the commit SHA
* Requires `GITHUB_TOKEN` (fine-grained token)
* Download and extract to temp dir

### Mandatory: shallow + pinned

* Always fetch the exact commit SHA
* Use shallow clone or tarball to reduce time

**Env vars:**

* `GITHUB_TOKEN` (if using API downloads)
* `GITHUB_WEBHOOK_SECRET`
* Optional: `CI_TMP_DIR=/tmp/ci` default

**Acceptance criteria:**

* Worker can retrieve code for a given SHA reliably
* Worker cleans temp dir after run

---

# Phase 5 — Feed API (Backend)

Add endpoints:

### `GET /api/ci/runs?owner=...&repo=...&limit=50`

Returns latest runs ordered by created_at desc:

```json
{
  "ok": true,
  "runs": [
    {
      "id": "...",
      "commit_sha": "...",
      "ref": "main",
      "event_type": "push",
      "status": "SUCCEEDED",
      "created_at": "...",
      "started_at": "...",
      "finished_at": "...",
      "out_dir": "out/ci/<id>",
      "summary_json": {...}
    }
  ]
}
```

### `GET /api/ci/runs/:id`

Returns run details + error if any.

### `POST /api/ci/enqueue`

Manual trigger (for demos). Body:

```json
{"owner":"...","repo":"...","ref":"main","commit_sha":"...","event_type":"manual"}
```

**Acceptance criteria:**

* Feed endpoint returns data for UI
* Manual enqueue works and processes

---

# Phase 6 — UI: Live Feed Page

Add a new page/route: **CI Feed**

* Repo selector (owner/repo)
* List of runs:

  * status badge
  * commit SHA (short)
  * ref
  * created time
  * duration
  * link “View dossier” (opens the existing result viewer for that out_dir/run_id)

Refresh strategy:

* Poll `GET /api/ci/runs` every 10 seconds
* If any run is RUNNING/QUEUED, poll every 3 seconds

**Acceptance criteria:**

* You can see new runs appear after webhook events
* Clicking run opens the artifacts/results already supported by existing viewer

---

# Phase 7 — Integration: GitHub Webhook Setup Guidance (Docs)

Add a short docs section:

Webhook URL:

* `https://<your-replit-app-domain>/api/webhooks/github`

Events:

* Push
* Pull request

Secret:

* matches `GITHUB_WEBHOOK_SECRET`

**Acceptance criteria:**

* A user can configure webhook without guessing anything

---

# Phase 8 — Test Plan (must exist)

Add tests for:

1. Signature verification:

* valid signature passes
* invalid signature fails 401

2. Webhook parsing:

* push payload creates run+job
* PR payload creates run+job
* unsupported event ignored

3. Worker leasing:

* two parallel workers do not process same job (simulate with transactions or mock)

4. Happy path (can be semi-integration):

* enqueue → worker tick → run SUCCEEDED (mock analyzer call)

**Acceptance criteria:**

* Test suite runs green
* Worker logic is deterministic and concurrency-safe

---

# Phase 9 — Operational Hardening (Minimum)

Add:

* structured logs for webhook receipt + job lease + job completion
* a `/api/ci/health` endpoint:

  * counts jobs by status
  * last run time
* rate limiting on webhook endpoint (basic, even in-memory)

**Acceptance criteria:**

* You can diagnose “why feed stopped” in under 2 minutes from logs + health endpoint

---

## Implementation Notes (DO NOT IGNORE)

* Webhook handler must never run analyzer inline.
* Always pin analysis to a specific commit SHA.
* Store a stable pointer (`out_dir`) so results can be served later.
* Keep the claim posture honest: this is *live static feed*, not runtime truth.

---

## Final Definition of Done

1. GitHub push triggers webhook → a new run appears in UI feed within seconds
2. Worker processes job and completes within expected time
3. Clicking run opens the dossier artifacts
4. Failures show error text, and job retries up to 3 times
5. Duplicate push events for same SHA don’t create duplicate runs

---

If you tell me **which repo codebase this is going into** (PTA? HALO? Lantern?) and **how the analyzer currently runs** (command + output path), I’ll rewrite the same instructions as a *repo-specific* agent plan with exact filenames and function names.
