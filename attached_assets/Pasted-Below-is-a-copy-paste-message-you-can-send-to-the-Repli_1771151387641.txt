Below is a **copy/paste message** you can send to the Replit agents. It’s written as “do exactly this” instructions, with guardrails that protect **integrity (no drift, no silent failures)** and **value (auditability, defensible outputs, repeatable demos)**.

---

## MESSAGE TO REPLIT AGENTS (COPY/PASTE)

### Objective

Harden the analyzer pipeline and admin endpoints so that:

* analysis runs are **deterministic, reproducible, and fail-closed**
* logs are **persistent, parsable, and audit-friendly**
* reset/clear actions are **explicitly gated** and **logged**
* no handler races can produce **double finalization** or **ghost status flips**
* stale filesystem artifacts can **never** contaminate a run

### 0) Non-negotiables (rules)

1. **No fallback to system python.** Analyzer must run only via `.pythonlibs/bin/python3` or it fails fast.
2. **Hard reset output dir per run.** `rm -rf out/<id>` then `mkdir`. No stale carryover.
3. **Exactly one finalization.** Timeout, spawn error, close handler may race; only one wins.
4. **Require artifacts on success.** Do not mark completed unless required artifacts exist.
5. **Persistent NDJSON logbook.** Append-only log to `out/_log/analyzer.ndjson`.
6. **Admin endpoints gated.** In production always 403. In dev require `x-admin-key` if `ADMIN_KEY` exists; if not set, allow (back-compat) but emit a loud log event so it’s obvious.

---

## 1) Analyzer logbook (ndjson) — implement exactly

In `server/routes.ts`, define:

* `LOG_DIR = path.join(process.cwd(), "out", "_log")`
* `LOG_FILE = path.join(LOG_DIR, "analyzer.ndjson")`

Add helper:

* `async function logEvent(projectId, event, detail?)`

  * ensure `LOG_DIR` exists (`mkdir recursive`)
  * append a single JSON line with:

    * `ts` ISO string
    * `projectId`
    * `event`
    * spread `detail`
  * always end with `\n`
  * never throw: wrap in try/catch, but if it fails, print console error (do not crash server)

Minimum events to emit:

* `start` `{ mode, source }`
* `spawn` `{ cmd }`
* `exit` `{ code }`
* `finalize` `{ status, reason, durationMs }`
* error events as needed:

  * `fatal` (e.g., python missing)
  * `spawn_error`
  * `timeout`
  * `missing_artifact`
  * `save_error`
  * `nonzero_exit` (include last 500 chars of stderr)

**Reason strings** must be stable enums (for dashboards later), e.g.:

* `python_not_found`
* `spawn_error`
* `timeout_10m`
* `missing_artifact:<name>`
* `save_error`
* `exit_code_<n>`

---

## 2) finishOnce / race-proof finalization — implement exactly

In `runAnalysis()`:

* create `const startedAt = Date.now();`
* create `let finished = false;`
* create `const finishOnce = async (status: "completed" | "failed", reason?: string) => {`

  * if `finished` return immediately
  * set `finished = true`
  * compute `durationMs = Date.now() - startedAt`
  * `await storage.updateProjectStatus(projectId, status)`
  * `await logEvent(projectId, "finalize", { status, reason, durationMs })`
    `}`

Every terminal path (timeout, spawn error, close nonzero, missing artifact, save error, success) **must call `finishOnce`** and never call `updateProjectStatus` directly again.

Also ensure `clearTimeout(timeout)` is called in both `error` and `close` handlers, but those handlers must respect `finished` (finishOnce protects it).

---

## 3) Analyzer spawn command — enforce module invocation + python pin

Spawn must be:

* python binary: `const pythonBin = path.join(process.cwd(), ".pythonlibs/bin/python3")`
* if missing:

  * `logEvent(projectId, "fatal", { reason: "python_not_found", path: pythonBin })`
  * `await finishOnce("failed", "python_not_found")`
  * return

args must be module invocation:

* `["-m", "server.analyzer.analyzer_cli", "analyze", ...(mode flags), "--output-dir", outputDir]`

Command string logged:

* `const cmd = `${pythonBin} ${args.join(" ")}`;`
* `logEvent(projectId, "spawn", { cmd })`

Spawn options:

* `cwd: process.cwd()`
* `env: { ...process.env }`

---

## 4) Output directory hygiene (anti-contamination)

Before spawning:

* `await fs.rm(outputDir, { recursive: true, force: true })`
* `await fs.mkdir(outputDir, { recursive: true })`

Also ensure `out/_log/` is NOT wiped by per-run cleanup (only `out/<projectId>` is wiped).

---

## 5) Timeout policy (configurable, default 10 minutes)

Implement:

* `const timeoutMs = Number(process.env.ANALYZER_TIMEOUT_MS) || 10 * 60 * 1000;`
* `setTimeout(() => { logEvent("timeout"); kill process; finishOnce("failed","timeout_10m"); }, timeoutMs)`

On timeout:

* emit `logEvent(projectId, "timeout", { timeoutMs })`
* kill process with `SIGKILL`

---

## 6) Required artifact validation

On `close` code === 0:

* require these files exist **before saving DB analysis**:

  * `operate.json`
  * `DOSSIER.md`
  * `claims.json`
    If any missing:
* `logEvent(projectId, "missing_artifact", { artifact })`
* `await finishOnce("failed", `missing_artifact:${artifact}`)`
* return

Then read/parse and `storage.createAnalysis(...)`.
If save fails:

* `logEvent(projectId, "save_error", { error: String(err) })`
* `await finishOnce("failed", "save_error")`

On nonzero exit:

* `logEvent(projectId, "nonzero_exit", { code, stderr: stderr.slice(-500) })`
* `await finishOnce("failed", `exit_code_${code}`)`

---

## 7) Admin endpoints (log viewer, log clear, reset) — gate + audit

Implement helper `requireDevAdmin(req, res)`:

Rules:

* if `NODE_ENV === "production"` => return 403 always
* if `ADMIN_KEY` is set:

  * require header `x-admin-key` matches exactly
  * else 401
* if `ADMIN_KEY` is not set:

  * allow but emit `logEvent(0, "admin_unguarded", { path: req.path, ip, ua })` once per request (projectId can be 0)

Endpoints:

1. `GET /api/admin/analyzer-log`

   * gated by requireDevAdmin
   * reads `out/_log/analyzer.ndjson` (if missing => `[]`)
   * parse each line as JSON; skip blank lines; return JSON array
2. `POST /api/admin/analyzer-log/clear`

   * gated by requireDevAdmin
   * deletes ONLY `out/_log/analyzer.ndjson` (keep folders)
   * recreates empty file (optional)
   * emits log event: `logEvent(0, "log_cleared", { ip, ua })`
3. `POST /api/admin/reset-analyzer`

   * gated by requireDevAdmin
   * calls `await storage.resetAnalyzerLogbook()` (delete analyses then projects)
   * wipes `out/` entirely (`rm -rf out/` then `mkdir out/`)
   * emits log event: `logEvent(0, "reset_analyzer", { ip, ua })`

**Important ordering**: delete analyses before projects (FK-safe).

---

## 8) Storage hardening (DB reset)

In `server/storage.ts`:

* add interface method `resetAnalyzerLogbook(): Promise<void>`
* implementation deletes from analyses first, then projects
* (preferred) wrap in transaction if available; otherwise sequential deletes are acceptable in single-process dev

---

## 9) Verification commands (must pass exactly)

After implementation, run these checks and paste outputs:

1. Reset endpoint:

```bash
curl -s -X POST http://localhost:5000/api/admin/reset-analyzer -H "x-admin-key: $ADMIN_KEY"; echo
curl -s http://localhost:5000/api/projects; echo
ls -la out | head
```

Expected: `{"ok":true}`, then `[]`, then empty `out/`

2. Trigger analysis and confirm logbook:

```bash
ID=$(curl -s -X POST http://localhost:5000/api/projects/analyze-replit | python3 -c "import sys,json; print(json.load(sys.stdin)['id'])")
echo "ID=$ID"
sleep 1
tail -n 20 out/_log/analyzer.ndjson
curl -s http://localhost:5000/api/admin/analyzer-log -H "x-admin-key: $ADMIN_KEY" | head -c 300; echo
```

Expected: start/spawn events appear; viewer returns JSON array

3. Clear logbook only:

```bash
curl -s -X POST http://localhost:5000/api/admin/analyzer-log/clear -H "x-admin-key: $ADMIN_KEY"; echo
wc -l out/_log/analyzer.ndjson
curl -s http://localhost:5000/api/projects | head -c 120; echo
```

Expected: `{"ok":true}`, then `0`, and projects list still exists (not wiped)

4. Fail-fast proof (python missing):

```bash
mv .pythonlibs/bin/python3 .pythonlibs/bin/python3.BAK
curl -s -X POST http://localhost:5000/api/projects/analyze-replit | cat; echo
sleep 1
curl -s http://localhost:5000/api/projects | head -c 200; echo
tail -n 30 out/_log/analyzer.ndjson
mv .pythonlibs/bin/python3.BAK .pythonlibs/bin/python3
```

Expected: project status -> failed, log contains `fatal python_not_found` + single `finalize`, no double-finalize.

---

## 10) Value protection note (why we’re doing this)

This is not “nice to have.” These controls protect the product’s credibility:

* **deterministic run hygiene** prevents false demos and “it worked yesterday” rot
* **ndjson event trail** enables operator-grade debugging and later compliance framing
* **fail-closed behavior** prevents silently wrong “completed” analyses
* **admin gating + auditable admin actions** prevents accidental self-sabotage and preserves forensic integrity

Deliver this as implemented, then we can add the next value layer: a “receipt” object for each run that references the logbook span + evidence pack hash.

---
