Send this **verbatim** to the Replit agent. It’s written as an **execution checklist + exact code edits**.

---

## REPLIT AGENT TASK: Harden audit block rendering (fail-fast + no-drift contract tests)

### Goal

1. **Fail fast** if `save_evidence_pack()` doesn’t actually write `evidence_pack.v1.json` to disk.
2. Add **no-drift tests** that lock the audit block headings + disclaimers + key metrics rendering so future edits can’t silently degrade the report.

---

# A) Fail-fast guard in the render pipeline

### 1) Locate the render pipeline entry

Open:

* `server/analyzer/src/render.py` (or wherever `render_report(...)` / `render_engineer_report(...)` is invoked)

Find the call chain that does:

* `pack_path = save_evidence_pack(...)`
* then renders `REPORT_ENGINEER.md` (or similar)

### 2) Add the fail-fast check immediately after save

Right after the line that assigns `pack_path` (Path returned), add:

```python
# FAIL-FAST: evidence pack must exist on disk or we refuse to render
expected = pack_path
if expected is None:
    raise RuntimeError("save_evidence_pack() returned None; refusing to render report")

# Path returned may be a directory or file depending on implementation; normalize expectation
if expected.is_dir():
    expected_file = expected / "evidence_pack.v1.json"
else:
    expected_file = expected

if not expected_file.exists() or not expected_file.is_file():
    raise RuntimeError(
        f"Evidence pack missing on disk: expected {expected_file}. "
        "Refusing to render report to prevent silent partial output."
    )
```

**Important:** do not swallow this error. This should crash the run and make the failure obvious (prevents the out/13 silent-bad mode).

---

# B) Add “no-drift” contract tests for the audit block

### 1) Create new test file

Create:

* `server/analyzer/tests/test_contract_audit_block.py`

### 2) Implement tests that render from a fixture pack and assert structure

Add the following test scaffold (adjust imports to match repo layout; keep assertions exactly):

```python
import json
from pathlib import Path

def _load_fixture_pack() -> dict:
    # Use a stable, checked-in fixture pack (create it if missing).
    # Put it in: server/analyzer/tests/fixtures/evidence_pack.v1.fixture.json
    p = Path(__file__).parent / "fixtures" / "evidence_pack.v1.fixture.json"
    assert p.exists(), f"Missing fixture pack: {p}"
    return json.loads(p.read_text(encoding="utf-8"))

def _render_engineer_report_from_pack(pack: dict) -> str:
    # Import the actual function used to render REPORT_ENGINEER.md content.
    # Update these imports to match your code.
    from analyzer.render import render_engineer_report  # <-- adjust
    return render_engineer_report(pack)

def test_audit_block_required_headings_present():
    pack = _load_fixture_pack()
    md = _render_engineer_report_from_pack(pack)

    # Required section headings 1-5 (LOCKED CONTRACT)
    for h in [
        "## 1. Audit Snapshot",
        "## 2. Determinism & Coverage",
        "## 3. Integrity & Verification",
        "## 4. Unknowns & Missing Evidence",
        "## 5. Operator Notes",
    ]:
        assert h in md, f"Missing required heading: {h}"

def test_audit_block_disclaimers_locked():
    pack = _load_fixture_pack()
    md = _render_engineer_report_from_pack(pack)

    # Lock the anti-marketing posture / disclaimers
    for s in [
        "not a security score",
        "reported as null rather than estimated",
        "no inference-based promotion",
    ]:
        assert s.lower() in md.lower(), f"Missing disclaimer phrase: {s}"

def test_audit_block_metrics_rendered():
    pack = _load_fixture_pack()
    md = _render_engineer_report_from_pack(pack)

    # These must match fixture pack values (update fixture to the expected numbers)
    assert "DCI" in md
    assert "RCI" in md
    # Example locked values (edit these to your fixture’s exact values)
    assert "DCI: 88.24%" in md
    assert "RCI: 50.08%" in md
    assert "DCI v2: not_implemented" in md

def test_audit_block_component_breakdown_present():
    pack = _load_fixture_pack()
    md = _render_engineer_report_from_pack(pack)

    # Ensure component/bucket breakdown table exists
    assert "Component" in md and "Score" in md, "Missing component breakdown table headers"

def test_audit_block_snapshot_table_present():
    pack = _load_fixture_pack()
    md = _render_engineer_report_from_pack(pack)

    # Minimal check for a snapshot table structure
    assert "|" in md and "Snapshot" in md, "Missing snapshot table"

def test_audit_block_run_id_present():
    pack = _load_fixture_pack()
    md = _render_engineer_report_from_pack(pack)

    # Run ID must appear (ensure fixture pack includes it)
    assert "Run ID" in md or "run_id" in md.lower(), "Missing run id in audit heading/block"
```

### 3) Add fixtures directory + fixture pack

Create:

* `server/analyzer/tests/fixtures/`

Add:

* `server/analyzer/tests/fixtures/evidence_pack.v1.fixture.json`

This fixture should be a **known-good** evidence pack from a successful run (e.g., one from `out/16/` or `out/17/`), trimmed if needed but must include the fields your renderer reads (metrics, unknowns, run_id, etc.).

---

# C) Add a “pipeline refuses to render if pack missing” test

In the same test file, add:

```python
import pytest
from pathlib import Path

def test_fail_fast_if_evidence_pack_missing(tmp_path: Path):
    # Import the function that saves then renders (or simulate the check if it’s embedded)
    from analyzer.render import _assert_pack_written  # <-- if you factor it out
    missing = tmp_path / "evidence_pack.v1.json"
    with pytest.raises(RuntimeError):
        _assert_pack_written(missing)
```

**If you don’t have `_assert_pack_written`,** factor the fail-fast logic into a tiny helper in `render.py` so this test is clean and stable.

---

# D) Run commands (must be pasted back with output)

1. Run unit tests:

```bash
pytest -q
```

2. Run one end-to-end analysis to regenerate a fresh out dir (use your real CLI command):

```bash
# Example (adjust to your CLI):
pta analyze --replit -o out/17 --no-llm
```

3. Confirm:

* `out/17/evidence_pack.v1.json` exists
* `out/17/REPORT_ENGINEER.md` contains the full audit block
* No-drift tests pass

---

# E) Definition of done

* If pack write fails, the run terminates with RuntimeError (no silent partial report).
* `pytest` passes with the new contract tests.
* A fresh `out/` run produces correct audit block and correct metrics.

---

If you paste back **the exact import paths** for the renderer function (what the code actually calls to render `REPORT_ENGINEER.md`), I can tighten the test imports to match your layout precisely—but the agent can also just adjust the two import lines marked “adjust”.
