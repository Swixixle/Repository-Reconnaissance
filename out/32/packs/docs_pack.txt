
--- FILE: replit.md ---
L1: # Overview
L2: 
L3: **Program Totality Analyzer** — a full-stack web application that ingests software projects (via GitHub URL, local path, or live Replit workspace) and produces evidence-cited technical dossiers. The dossier covers what a target system is, how it works, how to use it, and what risks/unknowns exist. It combines a React frontend for submitting analysis requests and viewing results with an Express backend that manages projects/analyses in PostgreSQL and spawns a Python-based analyzer CLI for the actual code analysis.
L4: 
L5: ## User Preferences
L6: 
L7: Preferred communication style: Simple, everyday language.
L8: 
L9: ## System Architecture
L10: 
L11: ### Monorepo Structure
L12: 
L13: The project follows a three-zone monorepo pattern:
L14: 
L15: - **`client/`** — React SPA (frontend)
L16: - **`server/`** — Express API (backend)
L17: - **`shared/`** — Shared types, schemas, and route definitions used by both client and server
L18: 
L19: This avoids type drift between frontend and backend by sharing Zod schemas and TypeScript types from a single source of truth.
L20: 
L21: ### Frontend (`client/src/`)
L22: 
L23: - **Framework**: React 18 with TypeScript
L24: - **Routing**: Wouter (lightweight client-side router)
L25: - **State/Data Fetching**: TanStack React Query with polling for analysis status updates
L26: - **UI Components**: shadcn/ui (new-york style) built on Radix UI primitives
L27: - **Styling**: Tailwind CSS with CSS variables for theming (dark mode, cyan/neon aesthetic)
L28: - **Animations**: Framer Motion for page transitions and loading states
L29: - **Markdown Rendering**: react-markdown for displaying analysis dossiers
L30: - **Build Tool**: Vite with React plugin
L31: 
L32: Key pages:
L33: - `/` — Home page with URL input form and "Analyze Replit" button
L34: - `/projects` — List of previous analyses
L35: - `/projects/:id` — Detailed view of a specific analysis with tabs for dossier, claims, operator dashboard (operate.json), coverage, and unknowns
L36: 
L37: Path aliases: `@/` maps to `client/src/`, `@shared/` maps to `shared/`, `@assets/` maps to `attached_assets/`.
L38: 
L39: ### Backend (`server/`)
L40: 
L41: - **Framework**: Express 5 on Node.js
L42: - **Language**: TypeScript, run via `tsx` in dev
L43: - **API Pattern**: REST API under `/api/` prefix, route definitions shared via `shared/routes.ts`
L44: - **Dev Server**: Vite middleware in development (HMR via `server/vite.ts`), static file serving in production (`server/static.ts`)
L45: - **Build**: esbuild bundles server to `dist/index.cjs`; Vite builds client to `dist/public/`
L46: 
L47: Key API routes (defined in `server/routes.ts`):
L48: - `GET /api/projects` — List all projects
L49: - `POST /api/projects` — Create a new project (with mode: github/local/replit)
L50: - `GET /api/projects/:id` — Get project details
L51: - `GET /api/projects/:id/analysis` — Get analysis results
L52: - `POST /api/projects/:id/analyze` — Trigger analysis (spawns Python CLI)
L53: 
L54: ### Python Analyzer (`server/analyzer/`)
L55: 
L56: - **CLI**: `analyzer_cli.py` using Typer, supports three input modes:
L57:   - GitHub URL (`analyze <url>`)
L58:   - Local path (`analyze <path>`)
L59:   - Replit workspace (`analyze --replit`)
L60: - **Core**: `server/analyzer/src/analyzer.py` — orchestrates file acquisition, indexing, and LLM-powered analysis
L61: - **Operate Module**: `server/analyzer/src/core/operate.py` — deterministic (no LLM) extraction of operational data into `operate.json`
L62:   - Extracts boot commands, ports, integration points (endpoints, env vars, auth), deployment config, and runbook steps
L63:   - Uses three evidence tiers: EVIDENCED (file:line + SHA-256 snippet hash), INFERRED, UNKNOWN (with unknown_reason)
L64:   - Computes readiness scores (0-100) for boot, integrate, deploy categories
L65:   - Identifies operational gaps with severity ratings
L66: - **LLM Integration**: OpenAI API (via Replit AI Integrations env vars: `AI_INTEGRATIONS_OPENAI_API_KEY`, `AI_INTEGRATIONS_OPENAI_BASE_URL`)
L67: - The Express server spawns the Python analyzer as a child process
L68: 
L69: ### Database
L70: 
L71: - **Engine**: PostgreSQL (required, referenced via `DATABASE_URL` env var)
L72: - **ORM**: Drizzle ORM with `drizzle-zod` for schema-to-Zod validation
L73: - **Schema** (`shared/schema.ts`):
L74:   - `projects` — id, url, name, mode (github/local/replit), status (pending/analyzing/completed/failed), createdAt
L75:   - `analyses` — id, projectId, dossier (markdown text), claims (jsonb), howto (jsonb), coverage (jsonb), unknowns (jsonb), operate (jsonb), createdAt
L76: - **Chat models** (`shared/models/chat.ts`):
L77:   - `conversations` — id, title, createdAt
L78:   - `messages` — id, conversationId, role, content, createdAt
L79: - **Migrations**: Drizzle Kit with `drizzle-kit push` for schema sync
L80: - **Storage Layer**: `server/storage.ts` implements `IStorage` interface with `DatabaseStorage` class
L81: 
L82: ### Replit Integrations (`server/replit_integrations/` and `client/replit_integrations/`)
L83: 
L84: Pre-built integration modules for AI features:
L85: - **Chat** — Text-based conversation routes and storage using OpenAI
L86: - **Audio** — Voice recording, playback, speech-to-text, text-to-speech with AudioWorklet
L87: - **Image** — Image generation and editing via `gpt-image-1`
L88: - **Batch** — Rate-limited batch processing with retries for LLM calls
L89: 
L90: These are utility modules that can be registered on the Express app as needed.
L91: 
L92: ### Key Design Decisions
L93: 
L94: 1. **Shared route definitions** — `shared/routes.ts` defines API contracts (paths, input schemas, response schemas) used by both frontend hooks and backend handlers. This ensures type safety across the stack.
L95: 
L96: 2. **Python + Node hybrid** — The analyzer logic lives in Python (better ecosystem for code analysis, rich CLI output) while the web layer is Node/Express. The server spawns Python as a child process rather than using a microservice architecture, keeping deployment simple.
L97: 
L98: 3. **Evidence-first analysis** — The analyzer is designed to cite file paths and line ranges for every claim. When evidence is missing, it must label findings as inference/unknown rather than hallucinate.
L99: 
L100: 4. **Polling for status** — The frontend polls project status every 2 seconds while analysis is in progress, switching to static once completed/failed.
L101: 
L102: ## External Dependencies
L103: 
L104: ### Required Services
L105: - **PostgreSQL** — Primary database, must be provisioned with `DATABASE_URL` environment variable
L106: - **OpenAI API** (via Replit AI Integrations) — Powers the code analysis LLM calls
L107:   - `AI_INTEGRATIONS_OPENAI_API_KEY` — API key
L108:   - `AI_INTEGRATIONS_OPENAI_BASE_URL` — Base URL for API
L109: 
L110: ### Key NPM Packages
L111: - `express` v5 — HTTP server
L112: - `drizzle-orm` + `drizzle-kit` — Database ORM and migrations
L113: - `@tanstack/react-query` — Client-side data fetching and caching
L114: - `wouter` — Client-side routing
L115: - `react-markdown` — Markdown rendering for dossiers
L116: - `framer-motion` — Animations
L117: - `zod` + `drizzle-zod` — Runtime validation
L118: - `vite` — Frontend build and dev server
L119: - `esbuild` — Server build
L120: 
L121: ### Key Python Packages
L122: - `typer` — CLI framework
L123: - `openai` — LLM API client
L124: - `rich` — Console output formatting
L125: - `python-dotenv` — Environment variable loading
L126: 
L127: ### Dev/Build Tools
L128: - `tsx` — TypeScript execution for development
L129: - `tailwindcss` + `postcss` + `autoprefixer` — CSS toolchain
L130: - `@replit/vite-plugin-runtime-error-modal` — Dev error overlay

--- FILE: README.md ---
L1: # Program Totality Analyzer
L2: 
L3: A static-artifact-anchored analysis tool that generates technical dossiers for software projects. It extracts what a system is, how to run it, what it needs, and what it cannot determine — with every claim citing `file:line` evidence backed by SHA-256 snippet hashes.
L4: 
L5: **Scope limitation:** PTA analyzes static artifacts only (source files, config, lockfiles). It does not observe runtime behavior, prove correctness, or guarantee security. Claims labeled VERIFIED mean "anchored to a hash-verified source snippet," not "proven true at runtime."
L6: 
L7: ## What It Does
L8: 
L9: Given a software project (GitHub repo, local folder, or Replit workspace), the analyzer produces:
L10: 
L11: | File | Contents |
L12: |------|----------|
L13: | `operate.json` | Operator dashboard: boot commands, integration points, deployment config, readiness scores, gaps with severity. Deterministic, evidence-bound. Every item is EVIDENCED, INFERRED, or UNKNOWN. |
L14: | `target_howto.json` | Legacy: evidence-scoped run steps. Prefer `operate.json` for operator workflows. |
L15: | `claims.json` | Verifiable claims about the system, each with file:line evidence and confidence scores |
L16: | `coverage.json` | Scan metadata: files scanned, files skipped, Replit detection evidence |
L17: | `replit_profile.json` | Replit-specific: port binding, secrets, external APIs, observability (only in Replit mode) |
L18: | `DOSSIER.md` | Human-readable markdown dossier summarizing all findings |
L19: | `index.json` | Full file index of scanned files |
L20: | `packs/` | Evidence packs (docs, config, code, ops) used during analysis |
L21: 
L22: ## Install
L23: 
L24: ```bash
L25: pip install -e .
L26: ```
L27: 
L28: This registers the `pta` command. Alternatively, run as a module or directly:
L29: 
L30: ```bash
L31: python -m server.analyzer.src --help
L32: python server/analyzer/analyzer_cli.py --help
L33: ```
L34: 
L35: ### Dependencies
L36: 
L37: - Python 3.11+
L38: - Required packages: `typer`, `rich`, `openai`, `gitpython`, `jsonschema`, `python-dotenv`, `pydantic`
L39: 
L40: ## Usage
L41: 
L42: ### Three Modes
L43: 
L44: **GitHub repository:**
L45: ```bash
L46: pta analyze https://github.com/user/repo -o ./output
L47: ```
L48: 
L49: **Local folder:**
L50: ```bash
L51: pta analyze ./path/to/project -o ./output
L52: ```
L53: 
L54: **Replit workspace (run from inside the workspace):**
L55: ```bash
L56: pta analyze --replit -o ./output
L57: ```
L58: 
L59: ### Deterministic Mode (`--no-llm`)
L60: 
L61: Skip all LLM calls and produce only deterministic, structurally-extracted outputs:
L62: 
L63: ```bash
L64: pta analyze --replit --no-llm -o ./output
L65: ```
L66: 
L67: This mode requires no API keys and produces reproducible results. It generates `operate.json` and readiness scoring without any LLM involvement. It extracts:
L68: - Package scripts (dev, build, start)
L69: - Lockfile-based install commands
L70: - Environment variable references (names only, never values)
L71: - Port binding configuration
L72: - External API usage
L73: - Replit platform detection
L74: - Operational gaps with severity ratings
L75: - Readiness scores (boot, integrate, deploy)
L76: 
L77: ### With LLM Analysis
L78: 
L79: For semantic analysis (architecture understanding, risk assessment, integration patterns):
L80: 
L81: ```bash
L82: pta analyze --replit -o ./output
L83: ```
L84: 
L85: Requires `AI_INTEGRATIONS_OPENAI_API_KEY` and `AI_INTEGRATIONS_OPENAI_BASE_URL` environment variables.
L86: 
L87: ### Scoping a Subdirectory
L88: 
L89: ```bash
L90: pta analyze https://github.com/user/monorepo --root packages/api -o ./output
L91: ```
L92: 
L93: ## Evidence Model
L94: 
L95: Every claim in the output cites structured evidence:
L96: 
L97: ```json
L98: {
L99:   "path": "server/index.ts",
L100:   "line_start": 92,
L101:   "line_end": 92,
L102:   "snippet_hash": "75d345a78f84",
L103:   "display": "server/index.ts:92"
L104: }
L105: ```
L106: 
L107: - `path` -- file path relative to project root
L108: - `line_start` / `line_end` -- 1-indexed line range (never 0)
L109: - `snippet_hash` -- first 12 hex chars of SHA-256 of the stripped line(s)
L110: - `display` -- human-readable location string
L111: 
L112: For file-existence evidence (e.g., lockfile detection):
L113: 
L114: ```json
L115: {
L116:   "kind": "file_exists",
L117:   "path": "package-lock.json",
L118:   "snippet_hash": "053150b640a7",
L119:   "display": "package-lock.json (file exists)"
L120: }
L121: ```
L122: 
L123: ### Gap Severity
L124: 
L125: Operational gaps in `operate.json` include a severity rating:
L126: - **high** — blocks boot or core execution
L127: - **medium** — impacts deployment maturity
L128: - **low** — best-practice or observability improvements
L129: 
L130: ### Verification
L131: 
L132: Snippet hashes are re-checked against source files: the analyzer re-reads the cited line range, strips whitespace, hashes the result, and confirms it matches the claimed hash. Claims that fail hash verification are capped at confidence 0.20 and marked `"status": "unverified"`.
L133: 
L134: **Important:** Hash verification confirms that a snippet exists at the cited location. It does not prove that the code behaves as described, is secure, or is free of bugs. PTA is not a security scanner, compliance certification tool, or correctness prover.
L135: 
L136: ### Whitespace Policy
L137: 
L138: Lines are stripped (trimmed) before hashing. This normalizes indentation differences across editors and formatters. Both evidence creation and verification use the same canonicalization.
L139: 
L140: ## Security
L141: 
L142: - **Symlink protection**: Every path component is checked. If any component in the path tree is a symlink, the file is rejected.
L143: - **Path containment**: All resolved paths must remain within the project root (`relative_to()` check after `resolve()`).
L144: - **Binary detection**: Null bytes in the first 4KB trigger rejection before text decoding.
L145: - **Traversal prevention**: `..` segments and absolute paths are rejected.
L146: - **Secret safety**: Only environment variable names are extracted, never their values.
L147: - **Self-skip**: The analyzer excludes its own source files from analysis to prevent false-positive pattern matches.
L148: 
L149: ## Output Files
L150: 
L151: ### `operate.json`
L152: 
L153: Operator dashboard model with:
L154: - `boot` -- install, dev, prod commands and port bindings, each with evidence tier
L155: - `integrate` -- endpoints, env vars, auth mechanisms with evidence
L156: - `deploy` -- Docker, platform hints, CI/CD, build commands with evidence
L157: - `readiness` -- scores (0-100) for boot, integrate, deploy categories with reasons
L158: - `gaps` -- operational gaps with severity (high/medium/low), rank, and action
L159: - `runbooks` -- numbered step sequences for local_dev, production, and integration
L160: - `snapshot` -- observability, migrations, and architectural metadata
L161: 
L162: All items carry a status of EVIDENCED, INFERRED, or UNKNOWN. EVIDENCED items include file:line references and SHA-256 snippet hashes. UNKNOWN items include an `unknown_reason`.
L163: 
L164: ### `target_howto.json` (legacy)
L165: 
L166: Legacy operator manual. Prefer `operate.json` for operator workflows. Contains:
L167: - `prereqs` -- required runtimes
L168: - `install_steps` -- with commands and evidence
L169: - `config` -- environment variables with file:line references
L170: - `run_dev` / `run_prod` -- start commands with evidence
L171: - `replit_execution_profile` -- port binding, secrets, external APIs (Replit mode)
L172: - `unknowns` -- things the analyzer could not determine
L173: - `completeness` -- scoring with missing items
L174: 
L175: ### `claims.json`
L176: 
L177: Array of verifiable claims:
L178: - `statement` -- what is claimed
L179: - `confidence` -- 0.0 to 1.0
L180: - `evidence` -- array of evidence objects with `snippet_hash_verified: true/false`
L181: - `status` -- `"evidenced"` or `"unverified"`
L182: 
L183: ### `coverage.json`
L184: 
L185: - `mode_requested` / `mode` -- analysis mode
L186: - `scanned` / `skipped` -- file counts
L187: - `replit_detected` -- boolean
L188: - `replit_detection_evidence` -- evidence for Replit detection
L189: - `self_skip` -- analyzer self-exclusion details
L190: 
L191: ## Troubleshooting
L192: 
L193: ### "No module named 'core'"
L194: 
L195: Run from the repo root, or install with `pip install -e .`
L196: 
L197: ### Missing `DATABASE_URL`
L198: 
L199: The analyzer itself does not need a database. `DATABASE_URL` appears in outputs because it detects the target project's database configuration. No action needed for the analyzer.
L200: 
L201: ### Missing OpenAI environment variables
L202: 
L203: Only required when running without `--no-llm`. Set:
L204: ```bash
L205: export AI_INTEGRATIONS_OPENAI_API_KEY=your-key
L206: export AI_INTEGRATIONS_OPENAI_BASE_URL=https://api.openai.com/v1
L207: ```
L208: 
L209: ### "Port already in use"
L210: 
L211: The analyzer does not bind any ports. If you see port errors, they come from the target project's web server, not the analyzer.
L212: 
L213: ## Architecture
L214: 
L215: Two strictly separated layers:
L216: 
L217: 1. **Structural layer** (deterministic) — file indexing, pattern matching, evidence extraction. Outputs are reproducible and hash-verified against source artifacts.
L218: 2. **Semantic layer** (LLM-powered, optional) — architecture interpretation, risk assessment, integration analysis. Outputs are labeled as LLM-generated and carry confidence scores, not deterministic guarantees.
L219: 
L220: The `--no-llm` flag gives you only the structural layer. The semantic layer adds interpretation but is namespaced separately and never contaminates structural evidence.
L221: 
L222: ## Running Tests
L223: 
L224: ```bash
L225: bash scripts/smoke_test.sh
L226: ```
L227: 
L228: ## License
L229: 
L230: MIT

--- FILE: Dockerfile ---
L1: FROM node:20-slim AS base
L2: WORKDIR /app
L3: 
L4: FROM base AS deps
L5: COPY package.json package-lock.json ./
L6: RUN npm ci --ignore-scripts
L7: 
L8: FROM base AS build
L9: COPY --from=deps /app/node_modules ./node_modules
L10: COPY . .
L11: RUN npm run build
L12: 
L13: FROM base AS runtime
L14: COPY --from=deps /app/node_modules ./node_modules
L15: COPY --from=build /app/dist ./dist
L16: COPY package.json ./
L17: ENV NODE_ENV=production
L18: EXPOSE 5000
L19: HEALTHCHECK --interval=30s --timeout=5s --start-period=10s \
L20:   CMD curl -f http://localhost:5000/health || exit 1
L21: CMD ["npm", "start"]

--- FILE: client/requirements.md ---
L1: ## Packages
L2: react-markdown | For rendering the analysis dossier
L3: framer-motion | For smooth page transitions and loading effects
L4: clsx | Utility for constructing className strings conditionally
L5: tailwind-merge | Utility for merging Tailwind classes efficiently
L6: 
L7: ## Notes
L8: - Theme: Technical, dark mode, "Program Totality Analyzer" aesthetic.
L9: - Fonts: Space Grotesk (headers), JetBrains Mono (code/data), Inter (UI).
L10: - Polling: Project status needs polling to show "Analyzing..." vs "Completed".

--- FILE: examples/README.md ---
L1: # Example Outputs
L2: 
L3: These are sample outputs from the Program Totality Analyzer running in `--no-llm` (deterministic) mode against its own workspace.
L4: 
L5: ## Files
L6: 
L7: - `out/operate.sample.json` — Operator dashboard: boot commands, integration points, deployment config, readiness scores, gaps with severity. Deterministic, evidence-bound.
L8: - `out/target_howto.sample.json` — Legacy operator manual. Prefer `operate.json` for operator workflows.
L9: - `out/coverage.sample.json` — Scan metadata: mode requested, files scanned/skipped, Replit detection evidence, self-skip configuration.
L10: 
L11: ## Generating Your Own
L12: 
L13: ```bash
L14: pta analyze --replit --no-llm -o ./my_output
L15: ```
L16: 
L17: Or for a GitHub repo:
L18: 
L19: ```bash
L20: pta analyze https://github.com/user/repo -o ./my_output
L21: ```
L22: 
L23: Or for a local folder:
L24: 
L25: ```bash
L26: pta analyze ./path/to/project -o ./my_output
L27: ```

--- FILE: docs/dossiers/lantern_program_totality_dossier.md ---
L1: ---
L2: title: Lantern Program Totality Dossier
L3: generated_by: PTA / Lantern
L4: mode: deterministic+curated
L5: date: 2026-02-14
L6: ---
L7: 
L8: # HALO-RECEIPTS: Program Totality Analyzer Dossier
L9: 
L10: ---
L11: 
L12: ## 1. **Identity of Target System**
L13: 
L14: **What it IS:**  
L15: HALO-RECEIPTS (AI Receipts) is a forensic verification system specifically designed for AI conversation transcripts. It provides cryptographic verification (SHA-256, Ed25519), immutable receipt storage, tamper-evident audit trails, forensic export capabilities, and extensive auditing for post-hoc analysis. This system is both the backend API server (Node.js/Express/PostgreSQL/Drizzle ORM) and a modern React UI, bundling forensic guarantees directly into receipt management and export (README.md:3,9,60–61; replit.md:4,11–12,14–24).
L16: 
L17: **What it is NOT:**  
L18: - **Not a real-time monitoring, content moderation, or multi-operator platform:** It is not designed for live chat moderation, direct truth judgment, or multi-user concurrency at the enforcement level (replit.md:55).
L19: - **Not a database engine:** Instead, it relies on PostgreSQL for durable state.
L20: - **Not a data lake or generic file archival platform.**
L21: - **Not a replacement for WORM-compliant log systems, but can integrate via checkpoint anchoring (see below).**
L22: - **Not a deployment framework:** The system expects to be deployed behind a reverse proxy or PaaS (README.md:24; SECURITY.md:54).
L23: 
L24: ---
L25: 
L26: ## 2. **Purpose & Jobs-to-be-done**
L27: 
L28: - **Forensic Conversation Integrity:** Operators can guarantee, via public proofs, that a transcript or "receipt" has not been tampered with since its recording date (README.md:9; replit.md:4).
L29: - **Immutable Logging & Audit Trails:** Ensures all receipt actions (append, lock, kill switch, export, audit actions) are tracked in an append-only, hash-linked audit log, detecting insertions, deletions, reordering, and version tampering (SECURITY.md:8–10; STATE.md:165–169).
L30: - **Cryptographic Verification:** Verifies that every receipt chain and audit log entry is both hash-linked (SHA-256) and checkpoint signed (Ed25519), with optional external anchoring (replit.md:36–46; STATE.md:116–121).
L31: - **Forensic Export and Proof Packs:** Allows export of forensic packs (JSON) that can be offline verified and admitted as tamper-evident evidence (STATE.md:122–125; scripts/ci-forensic-gate.sh).
L32: - **Regulatory Alignment:** System features mapped to compliance goals (21 CFR, HIPAA, SOC2, etc.) (replit.md:50, STATE.md:137).
L33: - **Operator/Evidence Reliability:** Designed to provide demonstrable evidence for courts, regulators, or internal review.
L34: 
L35: ---
L36: 
L37: ## 3. **Capability Map**
L38: 
L39: | Capability             | Mechanism / Implementation | Evidence                                    |
L40: |------------------------|---------------------------|---------------------------------------------|
L41: | SHA-256 Hash Verification   | Canonicalized (c14n-v1) JSON hash | README.md:73–74; STATE.md:17,20             |
L42: | Ed25519 Signatures     | Checkpoint signing, chain   | STATE.md:18,116–121; replit.md:36           |
L43: | Immutable Storage      | Receipt lock, no mutation  | README.md:75; STATE.md:21,158–159           |
L44: | Kill Switch            | Irreversible flag, disables outputs | README.md:76; STATE.md:22,157                |
L45: | Audit Logs             | Append-only, hash-chained table | STATE.md:25–29,162                           |
L46: | Forensic Export/Import | export_forensic_pack, verify_forensic_pack scripts | STATE.md:123–126                              |
L47: | Forensic Sensors       | Interpreter, summarizer, claim extractor | README.md:78; STATE.md:80,85                  |
L48: | Policy Enforcement     | Zod schemas, request shape limits | SECURITY.md:20–23                            |
L49: | API Rate Limiting      | Per-IP, in-memory          | SECURITY.md:26–29; STATE.md:94; package.json:61 |
L50: | Key Rotation & Anchoring | Multi-key support, anchor backends | replit.md:43–47                            |
L51: | Secure API Structure   | API_KEY in x-api-key header; private/public endpoints | SECURITY.md:15–16,72                           |
L52: | Client Auth Isolation  | LLMs see only transcript content | SECURITY.md:39–40; STATE.md:160,20           |
L53: | UI Export & Compare    | Side-by-side comparison, JSONL/CSV export | README.md:143; replit.md:25,23                |
L54: | Structured Logging     | JSON logs, in-memory counters | STATE.md:106–107,184                          |
L55: | Health Checks          | /api/health, /api/ready    | STATE.md:37,43,100–101; docs/API_CONTRACTS.md:11,24 |
L56: 
L57: ---
L58: 
L59: ## 4. **Architecture Snapshot**
L60: 
L61: - **Frontend:** React (wouter router), Tailwind, shadcn/ui (README.md:59, client/src/App.tsx)
L62: - **Backend:** Node.js 20, Express, Drizzle ORM (README.md:60–61; package.json)
L63: - **Database:** PostgreSQL 14+ (README.md:24, .env.example:5)
L64: - **Cryptography:** Node.js crypto (SHA-256), Ed25519 (STATE.md:18,116)
L65: - **Session:** express-session, connect-pg-simple (package.json:51,57)
L66: - **Audit Trail:** Hash-linking via prev_hash and payload_v, audit_head singleton row (STATE.md:9,25–29,48,52; drizzle.config.ts:9)
L67: - **Forensic Export:** TypeScript scripts in `/scripts`, results in JSON proof packs (STATE.md:123–126)
L68: - **CI/CD:** GitHub Actions, drift guard scripts, reproducible verifier zips (replit.md:39–42,57)
L69: 
L70: ---
L71: 
L72: ## 5. **How to Use the Target System**
L73: 
L74: ### **Operator Manual**
L75: 
L76: #### **A. Prerequisites**
L77: 
L78: 1. **Install:**
L79:    - Node.js 20+, npm (`npm -v`/`node -v`)
L80:    - PostgreSQL 14+ running and accessible (README.md:23–24)
L81:    - jq, unzip, python3 for export/ops scripts (see replit.nix, scripts/ci-forensic-gate.sh)
L82:    - TypeScript, tsx, drizzle-kit, installed via npm as devDependencies (package.json)
L83: 
L84: #### **B. Installation**
L85: 
L86: 1. **Clone Repository**
L87: 2. `npm install`  
L88:    Installs all dependencies (README.md:29)
L89: 3. `cp .env.example .env`  
L90:    Copy template env config (README.md:33)
L91: 4. **Edit `.env`**:  
L92:    Set `DATABASE_URL`, `API_KEY`, `SESSION_SECRET` and all other required variables suitably (README.md:34; .env.example:5,10,23)
L93: 5. **(Optional: Replit):**
L94:    - Use the "Run on Replit" badge or Replit sidebar GUI (README.md:17)
L95: 
L96: #### **C. Configuration**
L97: 
L98: Set the following in `.env` (names only, do not provide values):
L99: - **DATABASE_URL:** PostgreSQL connection string (.env.example:5)
L100: - **API_KEY:** Required for private endpoints (.env.example:10; SECURITY.md:15)
L101: - **SESSION_SECRET:** Strong, random string (.env.example:23; SECURITY.md:71)
L102: - **NODE_ENV:** development/production (.env.example:13)
L103: - **PORT:** Default 5000 (.env.example:14)
L104: - Other optional: TRANSCRIPT_MODE, CHECKPOINT_INTERVAL, CHECKPOINT_ANCHOR_TYPE, etc. (.env.example, STATE.md:118, replit.md:45)
L105: 
L106: #### **D. Database Init**
L107: 
L108: - Run: `npm run db:push`  
L109:   This applies the schema to PostgreSQL (README.md:39)
L110: 
L111: #### **E. Development Server**
L112: 
L113: - Run: `npm run dev`  
L114:   Runs server in development mode (README.md:44)
L115: - Visit: [http://localhost:5000](http://localhost:5000) (README.md:47)
L116: 
L117: #### **F. Production Build**
L118: 
L119: - Run: `npm run build`  
L120:   Compile frontend and backend (README.md:52)
L121: - Set `NODE_ENV=production`, then  
L122:   `npm run start` (README.md:53)
L123: - Server now on port specified in env (default 5000) (README.md:47, .replit:10,14, .env.example:14)
L124: 
L125: #### **G. Example API Usage**
L126: 
L127: - Health Check:  
L128:   `curl http://localhost:5000/api/health` (docs/API_CONTRACTS.md:11)
L129: - Readiness:  
L130:   `curl http://localhost:5000/api/ready` (docs/API_CONTRACTS.md:24)
L131: - Verify Audit (requires API_KEY):  
L132:   `curl -H "x-api-key: <API_KEY>" http://localhost:5000/api/audit/verify` (docs/API_CONTRACTS.md:64)
L133: - Create & Lock Receipts, Get all Receipts, Kill Switch, etc.:  
L134:   See usage_examples in HOWTO (docs/API_CONTRACTS.md)
L135: 
L136: #### **H. Verification & Forensics**
L137: 
L138: - Check schema: `npm run db:push` (README.md:39)
L139: - Verify audit chain:  
L140:   `curl -H "x-api-key: <API_KEY>" http://localhost:5000/api/audit/verify` (STATE.md:206)
L141: - Export forensic pack:**  
L142:   `npx tsx scripts/export_forensic_pack.ts --output <pack.json>`
L143: - Offline verify pack:  
L144:   `npx tsx scripts/verify_forensic_pack.ts <pack.json>` (scripts/ci-forensic-gate.sh:42)
L145: - Tamper detection: run the same after editing pack, expect fail (scripts/ci-forensic-gate.sh:61–84)
L146:   
L147: #### **I. Common Failures**
L148: 
L149: | Symptom                        | Cause                              | Fix                                                  | Evidence                |
L150: |--------------------------------|------------------------------------|------------------------------------------------------|-------------------------|
L151: | 401 Unauthorized               | Wrong/missing API_KEY              | Set correct `x-api-key` header, check .env           | SECURITY.md:15          |
L152: | DB connection errors/crash     | Bad DATABASE_URL, DB offline/wrong | Check credentials, service, version                  | drizzle.config.ts:3     |
L153: | Server not running on port     | App not started, port conflict     | Check logs, ensure PORT=5000, check if in use        | .replit:10              |
L154: | Forensic pack tamper undetected| Bug or script not installed        | Re-export, ensure proper script in place             | scripts/ci-forensic-gate.sh:61–84 |
L155: 
L156: ---
L157: 
L158: ## 6. **Integration Surface**
L159: 
L160: - **REST API:**  
L161:   Well-documented REST endpoints (`/api/health`, `/api/ready`, `/api/receipts`, `/api/audit/verify`, `/api/receipts/:id/lock`, `/api/receipts/:id/kill-switch`, etc.) (docs/API_CONTRACTS.md)
L162: - **API Authentication:**  
L163:   API key required for all non-public (write or sensitive) endpoints via `x-api-key` HTTP header (SECURITY.md:15, .env.example:10)
L164: - **Webhooks:**  
L165:   Unknown — evidence needed: No explicit webhook example or config found for outbound push/integrations.
L166: - **Data Formats:**  
L167:   JSON REST, all API schemas validated by Zod (STATE.md:95; SECURITY.md:20).
L168: - **Export/Import:**  
L169:   Forensic packs as canonical JSON, offline verifier script can process proof packs and verify signatures (STATE.md:123–125)
L170: - **SDKs:**  
L171:   None provided; interaction via HTTP API and TypeScript scripts.
L172: 
L173: ---
L174: 
L175: ## 7. **Data & Security Posture**
L176: 
L177: - **Data Storage:**  
L178:   All core state in PostgreSQL (receipts, audit trail, checkpoints; .env.example:5; README.md:61)
L179: - **Immutable guarantees:**  
L180:   Lock and kill-switch state prevent subsequent edits (README.md:75,76; STATE.md:21–22)
L181: - **Audit Log:**  
L182:   Hash-linked, append-only, verified at both write and operator demand (STATE.md:25–29)
L183: - **Cryptography:**  
L184:   Canonical JSON SHA-256 for all payloads, Ed25519 for checkpoint signing (STATE.md:17–19,116)
L185: - **External Anchoring:**  
L186:   Pluggable anchors (LogOnly, S3WormAnchor, Rfc3161TsaAnchor), config via `CHECKPOINT_ANCHOR_TYPE` (replit.md:45; STATE.md:170–173)
L187: - **Authentication:**  
L188:   API_KEY via header for all writes and sensitive queries, stored only as secret (SECURITY.md:15–17,72,84)
L189: - **Input Validation:**  
L190:   Zod schemas, 1MB max body, JSON only, UTF-8 validation (SECURITY.md:20–23)
L191: - **Rate Limiting:**  
L192:   Per-IP, endpoint and overall, in-memory only (SECURITY.md:26–29; STATE.md:94)
L193: - **Headers:**  
L194:   X-Content-Type-Options, X-Frame-Options, Referrer-Policy, Permissions-Policy (SECURITY.md:32–36; STATE.md:97)
L195: - **Session Security:**  
L196:   express-session with SESSION_SECRET, connect-pg-simple store (package.json:51,57; .env.example:23)
L197: - **No Logging of Secrets:**  
L198:   Explicitly forbidden, audit logs never include `API_KEY`/secrets (SECURITY.md:17,90)
L199: 
L200: ---
L201: 
L202: ## 8. **Operational Reality**
L203: 
L204: - **Server:**  
L205:   Runs via `npm run dev` (development) or production `npm run build && npm run start` (README.md:44,52–53; .replit:2,18)
L206: - **Port:**  
L207:   Default 5000 (README.md:47; .env.example:14; .replit:10,14)
L208: - **Database:**  
L209:   PostgreSQL 14+ required and always available; credentials in env (README.md:24; drizzle.config.ts)
L210: - **Secrets:**  
L211:   All secrets via `.env`; no default session or API key in production (SECURITY.md:84)
L212: - **No Built-in TLS:**  
L213:   Not exposed directly; deploy behind HTTPS proxy (SECURITY.md:54; README.md:24)
L214: - **CI/CD:**  
L215:   Github Actions runs type check, db:push, tests, drift guards, build, releases artifacts (STATE.md:111; replit.md:39–42)
L216: - **Counters/Rate-Limits:**  
L217:   In-memory, reset on process restart (STATE.md:107; SECURITY.md:62–64)
L218: - **Persistent Storage:**  
L219:   No special data directory—state in PostgreSQL only.
L220: - **Logs:**  
L221:   Structured JSON to console; location for interactive review unknown — evidence needed (STATE.md:106)
L222: 
L223: ---
L224: 
L225: ## 9. **Maintainability & Change Risk**
L226: 
L227: - **Well Bounded:**  
L228:   Security, business logic, and cryptography are isolated and formalized (STATE.md:149–162)
L229: - **Explicit Invariants and Forbidden Practices:**  
L230:   Canonicalization pipeline, no mock data, strict version/hashing rules (STATE.md:149–162,186–193)
L231: - **Codebase Size:**  
L232:   Large (`routes.ts` >2k lines); risk for routing/merge conflicts (STATE.md:183)
L233: - **Rate Limiter Limitation:**  
L234:   In-memory counters/rate-limiting, resets on restart; persistence is a punchlist item (STATE.md:184,219)
L235: - **Key rotation and signature abstraction:**  
L236:   Documented plan, proof-tested, rotation protocol in THREAT_MODEL.md (replit.md:43–44,60)
L237: - **Danger Areas:**  
L238:   Fully-privileged DB admin can bypass all checks without external anchor; risk documented (STATE.md:172–173).
L239: - **Tests:**  
L240:   42 tests (STATE.md:194). CI runs coverage, canonicalization, audit drift/adapter boundary guards (STATE.md:111–114)
L241: 
L242: ---
L243: 
L244: ## 10. **Replit Execution Profile**
L245: 
L246: - **Default Replit run:** `npm run dev` ([.replit:2])
L247: - **Modules:** Node 20, PostgreSQL 16, web via Replit ([.replit:1])
L248: - **PORT:** 5000 (mapped to external 80) ([.replit:10–11,14])
L249: - **Build on Deploy:** Runs `npm run build` ([.replit:19])
L250: - **Production Entrypoint:** `node ./dist/index.cjs` ([.replit:18])
L251: - **Replit Special Integration:** VITE_DEV_API_KEY set for development ([.replit:45]), GitHub integration possible ([.env.example:27–29])
L252: - **Nix Packages:** jq, unzip for ops scripts ([.replit:7])
L253: 
L254: ---
L255: 
L256: ## 11. **Unknowns / Missing Evidence**
L257: 
L258: | What is Missing | Why It Matters | Evidence Needed |
L259: |-----------------|----------------|----------------|
L260: | Production deployment details outside Replit (systemd/Docker/PM2) | Real-world ops, non-Replit platforms | Dockerfile, systemd service, or reverse proxy config |
L261: | Database initialization beyond drizzle-kit | Custom DB/user privileges, stateful setups | Database schema, role guides, SQL init scripts |
L262: | Key rotation/generation procedure for API_KEY or SESSION_SECRET | Long-term ops, secops | Step-by-step key management documentation |
L263: | Log path or viewing commands | Troubleshooting, support | Log file locations or sample log tail commands |
L264: | Standalone front-end hosting commands | Non-Replit, split hosting | Front-end build/start manual, production hosting instructions |
L265: | Webhooks or outbound events | Integration with SIEM/alerting | Outbound webhook config or documentation |
L266: 
L267: ---
L268: 
L269: ## 12. **Receipts (Evidence Index)**
L270: 
L271: **All claims above are strictly supported by:**
L272: 
L273: - **README.md:** lines 3,9,17–47,52–56,59–79,90.
L274: - **replit.md:** lines 4,11–61 (core capabilities, architecture).
L275: - **.env.example:** lines 5,10,13,14,18,23,27–29.
L276: - **STATE.md:** lines 17–23,25–48,49–212,149–162,165–193,194–214 (invariants, tests, capability inventory, ops, threat model, audit chain).
L277: - **SECURITY.md:** lines 8–17,20–29,32–36,39–41,54,62–64,71–90.
L278: - **drizzle.config.ts:** line 3 (database connectivity).
L279: - **.replit:** lines 2,10,14,18–19,45 (Replit settings).
L280: - **package.json:** dev/prod dependencies, scripts: "dev", "build", "start", "db:push" (lines 7–12,51,57,61).
L281: - **docs/API_CONTRACTS.md:** endpoints, usage examples.
L282: - **scripts/ci-forensic-gate.sh:** lines 42,61–84 (forensic scripts).
L283: - **client/src/App.tsx & components:** UI routes, health/audit banners.
L284:   
L285: **Specific references cross-index with HOWTO JSON, which extracts and hashes the underlying source text (e.g., README.md:39).**
L286: 
L287: ---
L288: 
L289: **End of Dossier.**

--- FILE: output/DOSSIER.md ---
L1: # Program Totality Analyzer — Deterministic Dossier
L2: 
L3: **Mode:** `--no-llm` (deterministic extraction only, no LLM calls)
L4: 
L5: ## 1. File Index Summary
L6: - Files scanned: see index.json
L7: - Self-skip: 26 analyzer files excluded
L8: 
L9: ## 2. Replit Execution Profile
L10: - **Is Replit:** True
L11: - **Run command:** `npm run dev`
L12: - **Language:** nodejs
L13: - **Port:** Uses PORT env var; actual port determined at runtime. In Replit, PORT is injected.
L14: - **Secrets (3):** DATABASE_URL, AI_INTEGRATIONS_OPENAI_API_KEY, AI_INTEGRATIONS_OPENAI_BASE_URL
L15: - **External APIs:** OpenAI
L16: 
L17: ## 3. Operator Manual (Deterministic)
L18: ```json
L19: {
L20:   "prereqs": [
L21:     "Node.js",
L22:     "Python"
L23:   ],
L24:   "install_steps": [
L25:     {
L26:       "step": "Install Node dependencies",
L27:       "command": "npm ci",
L28:       "evidence": {
L29:         "kind": "file_exists",
L30:         "path": "package-lock.json",
L31:         "snippet_hash": "053150b640a7",
L32:         "display": "package-lock.json (file exists)"
L33:       }
L34:     },
L35:     {
L36:       "step": "Install Python dependencies",
L37:       "command": "pip install .",
L38:       "evidence": {
L39:         "kind": "file_exists",
L40:         "path": "pyproject.toml",
L41:         "snippet_hash": "50c86b7ed8ac",
L42:         "display": "pyproject.toml (file exists)"
L43:       }
L44:     }
L45:   ],
L46:   "config": [
L47:     {
L48:       "name": "DATABASE_URL",
L49:       "purpose": "Secret referenced in code (see evidence)",
L50:       "evidence": {
L51:         "path": "drizzle.config.ts",
L52:         "line_start": 3,
L53:         "line_end": 3,
L54:         "snippet_hash": "a19790628fbe",
L55:         "display": "drizzle.config.ts:3"
L56:       }
L57:     },
L58:     {
L59:       "name": "AI_INTEGRATIONS_OPENAI_API_KEY",
L60:       "purpose": "Secret referenced in code (see evidence)",
L61:       "evidence": {
L62:         "path": "server/replit_integrations/audio/client.ts",
L63:         "line_start": 10,
L64:         "line_end": 10,
L65:         "snippet_hash": "05da5f1b1281",
L66:         "display": "server/replit_integrations/audio/client.ts:10"
L67:       }
L68:     },
L69:     {
L70:       "name": "AI_INTEGRATIONS_OPENAI_BASE_URL",
L71:       "purpose": "Secret referenced in code (see evidence)",
L72:       "evidence": {
L73:         "path": "server/replit_integrations/audio/client.ts",
L74:         "line_start": 11,
L75:         "line_end": 11,
L76:         "snippet_hash": "1f70e6a77d42",
L77:         "display": "server/replit_integrations/audio/client.ts:11"
L78:       }
L79:     }
L80:   ],
L81:   "run_dev": [
L82:     {
L83:       "step": "Start dev server",
L84:       "command": "npm run dev",
L85:       "evidence": {
L86:         "path": "package.json",
L87:         "line_start": 7,
L88:         "line_end": 7,
L89:         "snippet_hash": "fd240a9dc053",
L90:         "display": "package.json:7"
L91:       }
L92:     }
L93:   ],
L94:   "run_prod": [
L95:     {
L96:       "step": "Build for production",
L97:       "command": "npm run build",
L98:       "evidence": {
L99:         "path": "package.json",
L100:         "line_start": 8,
L101:         "line_end": 8,
L102:         "snippet_hash": "79d8bdf275d6",
L103:         "display": "package.json:8"
L104:       }
L105:     },
L106:     {
L107:       "step": "Start production",
L108:       "command": "npm start",
L109:       "evidence": {
L110:         "path": "package.json",
L111:         "line_start": 9,
L112:         "line_end": 9,
L113:         "snippet_hash": "020435ddf436",
L114:         "display": "package.json:9"
L115:       }
L116:     }
L117:   ],
L118:   "usage_examples": [],
L119:   "verification_steps": [],
L120:   "common_failures": [],
L121:   "unknowns": [
L122:     {
L123:       "what_is_missing": "Semantic analysis of code purpose and architecture",
L124:       "why_it_matters": "Cannot determine system intent, integration patterns, or risk factors without LLM analysis",
L125:       "what_evidence_needed": "Re-run without --no-llm flag for full analysis"
L126:     }
L127:   ],
L128:   "missing_evidence_requests": [],
L129:   "replit_execution_profile": {
L130:     "run_command": "npm run dev",
L131:     "language": "nodejs",
L132:     "port_binding": {
L133:       "port": null,
L134:       "binds_all_interfaces": true,
L135:       "uses_env_port": true,
L136:       "evidence": [
L137:         {
L138:           "path": "server/index.ts",
L139:           "line_start": 92,
L140:           "line_end": 92,
L141:           "snippet_hash": "75d345a78f84",
L142:           "display": "server/index.ts:92"
L143:         },
L144:         {
L145:           "path": "server/index.ts",
L146:           "line_start": 96,
L147:           "line_end": 96,
L148:           "snippet_hash": "9b7206f3d09a",
L149:           "display": "server/index.ts:96"
L150:         }
L151:       ]
L152:     },
L153:     "required_secrets": [
L154:       {
L155:         "name": "DATABASE_URL",
L156:         "referenced_in": [
L157:           {
L158:             "path": "drizzle.config.ts",
L159:             "line_start": 3,
L160:             "line_end": 3,
L161:             "snippet_hash": "a19790628fbe",
L162:             "display": "drizzle.config.ts:3"
L163:           },
L164:           {
L165:             "path": "drizzle.config.ts",
L166:             "line_start": 12,
L167:             "line_end": 12,
L168:             "snippet_hash": "1005be19f14a",
L169:             "display": "drizzle.config.ts:12"
L170:           },
L171:           {
L172:             "path": "server/db.ts",
L173:             "line_start": 7,
L174:             "line_end": 7,
L175:             "snippet_hash": "a19790628fbe",
L176:             "display": "server/db.ts:7"
L177:           },
L178:           {
L179:             "path": "server/db.ts",
L180:             "line_start": 13,
L181:             "line_end": 13,
L182:             "snippet_hash": "111f33de9945",
L183:             "display": "server/db.ts:13"
L184:           }
L185:         ]
L186:       },
L187:       {
L188:         "name": "AI_INTEGRATIONS_OPENAI_API_KEY",
L189:         "referenced_in": [
L190:           {
L191:             "path": "server/replit_integrations/audio/client.ts",
L192:             "line_start": 10,
L193:             "line_end": 10,
L194:             "snippet_hash": "05da5f1b1281",
L195:             "display": "server/replit_integrations/audio/client.ts:10"
L196:           },
L197:           {
L198:             "path": "server/replit_integrations/chat/routes.ts",
L199:             "line_start": 6,
L200:             "line_end": 6,
L201:             "snippet_hash": "05da5f1b1281",
L202:             "display": "server/replit_integrations/chat/routes.ts:6"
L203:           },
L204:           {
L205:             "path": "server/replit_integrations/image/client.ts",
L206:             "line_start": 6,
L207:             "line_end": 6,
L208:             "snippet_hash": "05da5f1b1281",
L209:             "display": "server/replit_integrations/image/client.ts:6"
L210:           }
L211:         ]
L212:       },
L213:       {
L214:         "name": "AI_INTEGRATIONS_OPENAI_BASE_URL",
L215:         "referenced_in": [
L216:           {
L217:             "path": "server/replit_integrations/audio/client.ts",
L218:             "line_start": 11,
L219:             "line_end": 11,
L220:             "snippet_hash": "1f70e6a77d42",
L221:             "display": "server/replit_integrations/audio/client.ts:11"
L222:           },
L223:           {
L224:             "path": "server/replit_integrations/chat/routes.ts",
L225:             "line_start": 7,
L226:             "line_end": 7,
L227:             "snippet_hash": "1f70e6a77d42",
L228:             "display": "server/replit_integrations/chat/routes.ts:7"
L229:           },
L230:           {
L231:             "path": "server/replit_integrations/image/client.ts",
L232:             "line_start": 7,
L233:             "line_end": 7,
L234:             "snippet_hash": "1f70e6a77d42",
L235:             "display": "server/replit_integrations/image/client.ts:7"
L236:           }
L237:         ]
L238:       }
L239:     ],
L240:     "external_apis": [
L241:       {
L242:         "api": "OpenAI",
L243:         "evidence_files": [
L244:           {
L245:             "path": "server/replit_integrations/audio/client.ts",
L246:             "line_start": 1,
L247:             "line_end": 1,
L248:             "snippet_hash": "1d3dd608c3bb",
L249:             "display": "server/replit_integrations/audio/client.ts:1"
L250:           },
L251:           {
L252:             "path": "server/replit_integrations/audio/routes.ts",
L253:             "line_start": 3,
L254:             "line_end": 3,
L255:             "snippet_hash": "2f87d29d3b03",
L256:             "display": "server/replit_integrations/audio/routes.ts:3"
L257:           },
L258:           {
L259:             "path": "server/replit_integrations/chat/routes.ts",
L260:             "line_start": 2,
L261:             "line_end": 2,
L262:             "snippet_hash": "4db7290b0afd",
L263:             "display": "server/replit_integrations/chat/routes.ts:2"
L264:           },
L265:           {
L266:             "path": "server/replit_integrations/image/routes.ts",
L267:             "line_start": 2,
L268:             "line_end": 2,
L269:             "snippet_hash": "7fd5b3abbeee",
L270:             "display": "server/replit_integrations/image/routes.ts:2"
L271:           },
L272:           {
L273:             "path": "server/replit_integrations/image/client.ts",
L274:             "line_start": 2,
L275:             "line_end": 2,
L276:             "snippet_hash": "1d3dd608c3bb",
L277:             "display": "server/replit_integrations/image/client.ts:2"
L278:           }
L279:         ]
L280:       }
L281:     ],
L282:     "deployment_assumptions": [
L283:       "Binds to 0.0.0.0 (all interfaces)",
L284:       "Requires 3 secret(s): DATABASE_URL, AI_INTEGRATIONS_OPENAI_API_KEY, AI_INTEGRATIONS_OPENAI_BASE_URL"
L285:     ],
L286:     "observability": {
L287:       "logging": true,
L288:       "health_endpoint": true,
L289:       "evidence": [
L290:         {
L291:           "path": "script/build.ts",
L292:           "line_start": 38,
L293:           "line_end": 38,
L294:           "snippet_hash": "2f74cc3fdab1",
L295:           "display": "script/build.ts:38"
L296:         },
L297:         {
L298:           "path": "server/routes.ts",
L299:           "line_start": 15,
L300:           "line_end": 15,
L301:           "snippet_hash": "f7ba68760271",
L302:           "display": "server/routes.ts:15"
L303:         },
L304:         {
L305:           "path": "shared/schema.ts",
L306:           "line_start": 10,
L307:           "line_end": 10,
L308:           "snippet_hash": "6ccc8e5d45a7",
L309:           "display": "shared/schema.ts:10"
L310:         }
L311:       ]
L312:     },
L313:     "limitations": [
L314:       "Deterministic mode (--no-llm): no semantic analysis performed"
L315:     ]
L316:   },
L317:   "completeness": {
L318:     "score": 62,
L319:     "max": 100,
L320:     "missing": [
L321:       "verification_steps: no step with both a runnable command and verified evidence",
L322:       "usage_examples: no examples with meaningful descriptions"
L323:     ],
L324:     "deductions": [
L325:       "-3 for 1 unknown(s)"
L326:     ],
L327:     "notes": "-3 for 1 unknown(s); 1 unknown(s) reported"
L328:   }
L329: }
L330: ```
L331: 
L332: ## 4. Limitations
L333: - This dossier was generated in `--no-llm` mode
L334: - No semantic analysis, claims extraction, or architecture inference was performed
L335: - For full analysis, re-run without `--no-llm`

--- FILE: output/REPORT_ENGINEER.md ---
L1: # Program Totality Report — Engineer View
L2: 
L3: **EvidencePack Version:** 1.0
L4: **Tool Version:** 0.1.0
L5: **Generated:** 2026-02-15T09:28:36.848729+00:00
L6: **Mode:** replit
L7: **Run ID:** 2e2b0d0dad53
L8: 
L9: ---
L10: 
L11: ## PTA Contract Audit — Run 2e2b0d0dad53
L12: 
L13: ### 1. System Snapshot
L14: 
L15: | Measure | Value |
L16: |---------|-------|
L17: | Files Analyzed | 169 |
L18: | Files Seen (incl. skipped) | 195 |
L19: | Files Skipped | 26 |
L20: | Claims Extracted | 17 |
L21: | Claims with Deterministic Evidence | 15 |
L22: | Unknown Governance Categories | 9 |
L23: | Verified Structural Categories | 0 |
L24: | Partial Coverage | Yes |
L25: 
L26: ### 2. Deterministic Coverage Index (DCI v1)
L27: 
L28: **Score:** 88.24%
L29: **Formula:** `verified_claims / total_claims`
L30: 
L31: 15 of 17 extracted claims contain hash-verified evidence.
L32: 
L33: This measures claim-to-evidence visibility only.
L34: It does not measure code quality, security posture, or structural surface coverage.
L35: 
L36: ### 3. Reporting Completeness Index (RCI)
L37: 
L38: **Score:** 50.08%
L39: **Formula:** `average(claims_coverage, unknowns_coverage, howto_completeness)`
L40: 
L41: | Component | Score |
L42: |-----------|-------|
L43: | claims_coverage | 88.24% |
L44: | unknowns_coverage | 0.00% |
L45: | howto_completeness | 62.00% |
L46: 
L47: RCI is a documentation completeness metric.
L48: It is not a security score and does not imply structural sufficiency.
L49: 
L50: ### 4. Structural Visibility (DCI v2)
L51: 
L52: **Status:** not_implemented
L53: **Formula (reserved):** `verified_structural_items / total_structural_surface`
L54: 
L55: Routes, dependencies, schemas, and enforcement extractors are not active.
L56: Structural surface visibility is intentionally reported as null rather than estimated.
L57: This prevents silent overstatement of governance posture.
L58: 
L59: ### 5. Epistemic Posture
L60: 
L61: PTA explicitly reports:
L62: - What is deterministically verified.
L63: - What is unknown.
L64: - What is not implemented.
L65: - What requires dedicated extractors.
L66: 
L67: There is no inference-based promotion from UNKNOWN to VERIFIED.
L68: 
L69: ---
L70: 
L71: ## Verified: Data & Security Posture
L72: 
L73: ### System requires 3 secret(s): DATABASE_URL, AI_INTEGRATIONS_OPENAI_API_KEY, AI_INTEGRATIONS_OPENAI_BASE_URL
L74: Confidence: 55%
L75: - Evidence: `drizzle.config.ts:3` (hash: `a19790628fbe`)
L76: - Evidence: `server/replit_integrations/audio/client.ts:10` (hash: `05da5f1b1281`)
L77: - Evidence: `server/replit_integrations/audio/client.ts:11` (hash: `1f70e6a77d42`)
L78: 
L79: ### Secret "DATABASE_URL" is referenced in 4 file(s)
L80: Confidence: 50%
L81: - Evidence: `drizzle.config.ts:3` (hash: `a19790628fbe`)
L82: - Evidence: `drizzle.config.ts:12` (hash: `1005be19f14a`)
L83: 
L84: ### Secret "AI_INTEGRATIONS_OPENAI_API_KEY" is referenced in 3 file(s)
L85: Confidence: 50%
L86: - Evidence: `server/replit_integrations/audio/client.ts:10` (hash: `05da5f1b1281`)
L87: - Evidence: `server/replit_integrations/chat/routes.ts:6` (hash: `05da5f1b1281`)
L88: 
L89: ### Secret "AI_INTEGRATIONS_OPENAI_BASE_URL" is referenced in 3 file(s)
L90: Confidence: 50%
L91: - Evidence: `server/replit_integrations/audio/client.ts:11` (hash: `1f70e6a77d42`)
L92: - Evidence: `server/replit_integrations/chat/routes.ts:7` (hash: `1f70e6a77d42`)
L93: 
L94: ## Verified: How to Use the Target System
L95: 
L96: ### npm script "dev" runs: NODE_ENV=development tsx server/index.ts
L97: Confidence: 60%
L98: - Evidence: `package.json:7` (hash: `fd240a9dc053`)
L99: 
L100: ### npm script "build" runs: tsx script/build.ts
L101: Confidence: 60%
L102: - Evidence: `package.json:8` (hash: `79d8bdf275d6`)
L103: 
L104: ### npm script "start" runs: NODE_ENV=production node dist/index.cjs
L105: Confidence: 60%
L106: - Evidence: `package.json:9` (hash: `020435ddf436`)
L107: 
L108: ### Server binds to 0.0.0.0 (all interfaces)
L109: Confidence: 55%
L110: - Evidence: `server/index.ts:92` (hash: `75d345a78f84`)
L111: - Evidence: `server/index.ts:96` (hash: `9b7206f3d09a`)
L112: 
L113: ### Server port is configured via environment variable
L114: Confidence: 55%
L115: - Evidence: `server/index.ts:92` (hash: `75d345a78f84`)
L116: - Evidence: `server/index.ts:96` (hash: `9b7206f3d09a`)
L117: 
L118: ### Replit run command: npm run dev
L119: Confidence: 55%
L120: - Evidence: `.replit:2` (hash: `96fa2e5505e4`)
L121: - Evidence: `.replit:5` (hash: `550b970621c2`)
L122: 
L123: ## Verified: Integration Surface
L124: 
L125: ### Key dependencies: drizzle-orm, express, openai, react
L126: Confidence: 50%
L127: - Evidence: `package.json:13` (hash: `f7eebadd079d`)
L128: 
L129: ### External API dependency: OpenAI
L130: Confidence: 45%
L131: - Evidence: `server/replit_integrations/audio/client.ts:1` (hash: `1d3dd608c3bb`)
L132: - Evidence: `server/replit_integrations/audio/routes.ts:3` (hash: `2f87d29d3b03`)
L133: 
L134: ### Database schema/migration files detected: drizzle.config.ts, server/db.ts, shared/schema.ts
L135: Confidence: 40%
L136: - Evidence: `drizzle.config.ts:1` (hash: `1f5c93c3d974`)
L137: - Evidence: `server/db.ts:1` (hash: `3d66d6ea5af3`)
L138: 
L139: ## Verified: What the Target System Is
L140: 
L141: ### The project is named "rest-express" (from package.json)
L142: Confidence: 60%
L143: - Evidence: `package.json:2` (hash: `a1f1a980b4b8`)
L144: 
L145: ### Python project named "program-totality-analyzer" (from pyproject.toml)
L146: Confidence: 50%
L147: - Evidence: `pyproject.toml:2` (hash: `f0d4a96fe7d6`)
L148: 
L149: ## Verified Structural (deterministic extractors only)
L150: 
L151: - **dependencies**: not_implemented: requires lockfile parser (package-lock.json, requirements.txt, etc.)
L152: - **enforcement**: not_implemented: requires auth/middleware pattern detector over source files
L153: - **routes**: not_implemented: requires AST/regex route extractor over source files
L154: - **schemas**: not_implemented: requires migration/model file parser
L155: 
L156: ## Known Unknown Surface
L157: 
L158: | Category | Status | Notes |
L159: |----------|--------|-------|
L160: | tls_termination | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L161: | encryption_at_rest | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L162: | secret_management | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L163: | deployment_topology | UNKNOWN | Candidate artifact files found (Dockerfile) but artifact detector not yet implemented — cannot read/hash/verify file content |
L164: | runtime_iam | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L165: | logging_sink | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L166: | monitoring_alerting | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L167: | backup_retention | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L168: | data_residency | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L169: 
L170: ## Snippet Hashes (20 total)
L171: 
L172: - `020435ddf436`
L173: - `053150b640a7`
L174: - `05da5f1b1281`
L175: - `1005be19f14a`
L176: - `1d3dd608c3bb`
L177: - `1f5c93c3d974`
L178: - `1f70e6a77d42`
L179: - `2f87d29d3b03`
L180: - `3d66d6ea5af3`
L181: - `50c86b7ed8ac`
L182: - `550b970621c2`
L183: - `75d345a78f84`
L184: - `79d8bdf275d6`
L185: - `96fa2e5505e4`
L186: - `9b7206f3d09a`
L187: - `a19790628fbe`
L188: - `a1f1a980b4b8`
L189: - `f0d4a96fe7d6`
L190: - `f7eebadd079d`
L191: - `fd240a9dc053`

--- FILE: output/packs/docs_pack.txt ---
L1: 
L2: --- FILE: replit.md ---
L3: L1: # Overview
L4: L2: 
L5: L3: **Program Totality Analyzer** — a full-stack web application that ingests software projects (via GitHub URL, local path, or live Replit workspace) and produces evidence-cited technical dossiers. The dossier covers what a target system is, how it works, how to use it, and what risks/unknowns exist. It combines a React frontend for submitting analysis requests and viewing results with an Express backend that manages projects/analyses in PostgreSQL and spawns a Python-based analyzer CLI for the actual code analysis.
L6: L4: 
L7: L5: ## User Preferences
L8: L6: 
L9: L7: Preferred communication style: Simple, everyday language.
L10: L8: 
L11: L9: ## System Architecture
L12: L10: 
L13: L11: ### Monorepo Structure
L14: L12: 
L15: L13: The project follows a three-zone monorepo pattern:
L16: L14: 
L17: L15: - **`client/`** — React SPA (frontend)
L18: L16: - **`server/`** — Express API (backend)
L19: L17: - **`shared/`** — Shared types, schemas, and route definitions used by both client and server
L20: L18: 
L21: L19: This avoids type drift between frontend and backend by sharing Zod schemas and TypeScript types from a single source of truth.
L22: L20: 
L23: L21: ### Frontend (`client/src/`)
L24: L22: 
L25: L23: - **Framework**: React 18 with TypeScript
L26: L24: - **Routing**: Wouter (lightweight client-side router)
L27: L25: - **State/Data Fetching**: TanStack React Query with polling for analysis status updates
L28: L26: - **UI Components**: shadcn/ui (new-york style) built on Radix UI primitives
L29: L27: - **Styling**: Tailwind CSS with CSS variables for theming (dark mode, cyan/neon aesthetic)
L30: L28: - **Animations**: Framer Motion for page transitions and loading states
L31: L29: - **Markdown Rendering**: react-markdown for displaying analysis dossiers
L32: L30: - **Build Tool**: Vite with React plugin
L33: L31: 
L34: L32: Key pages:
L35: L33: - `/` — Home page with URL input form and "Analyze Replit" button
L36: L34: - `/projects` — List of previous analyses
L37: L35: - `/projects/:id` — Detailed view of a specific analysis with tabs for dossier, claims, operator dashboard (operate.json), coverage, and unknowns
L38: L36: 
L39: L37: Path aliases: `@/` maps to `client/src/`, `@shared/` maps to `shared/`, `@assets/` maps to `attached_assets/`.
L40: L38: 
L41: L39: ### Backend (`server/`)
L42: L40: 
L43: L41: - **Framework**: Express 5 on Node.js
L44: L42: - **Language**: TypeScript, run via `tsx` in dev
L45: L43: - **API Pattern**: REST API under `/api/` prefix, route definitions shared via `shared/routes.ts`
L46: L44: - **Dev Server**: Vite middleware in development (HMR via `server/vite.ts`), static file serving in production (`server/static.ts`)
L47: L45: - **Build**: esbuild bundles server to `dist/index.cjs`; Vite builds client to `dist/public/`
L48: L46: 
L49: L47: Key API routes (defined in `server/routes.ts`):
L50: L48: - `GET /api/projects` — List all projects
L51: L49: - `POST /api/projects` — Create a new project (with mode: github/local/replit)
L52: L50: - `GET /api/projects/:id` — Get project details
L53: L51: - `GET /api/projects/:id/analysis` — Get analysis results
L54: L52: - `POST /api/projects/:id/analyze` — Trigger analysis (spawns Python CLI)
L55: L53: 
L56: L54: ### Python Analyzer (`server/analyzer/`)
L57: L55: 
L58: L56: - **CLI**: `analyzer_cli.py` using Typer, supports three input modes:
L59: L57:   - GitHub URL (`analyze <url>`)
L60: L58:   - Local path (`analyze <path>`)
L61: L59:   - Replit workspace (`analyze --replit`)
L62: L60: - **Core**: `server/analyzer/src/analyzer.py` — orchestrates file acquisition, indexing, and LLM-powered analysis
L63: L61: - **Operate Module**: `server/analyzer/src/core/operate.py` — deterministic (no LLM) extraction of operational data into `operate.json`
L64: L62:   - Extracts boot commands, ports, integration points (endpoints, env vars, auth), deployment config, and runbook steps
L65: L63:   - Uses three evidence tiers: EVIDENCED (file:line + SHA-256 snippet hash), INFERRED, UNKNOWN (with unknown_reason)
L66: L64:   - Computes readiness scores (0-100) for boot, integrate, deploy categories
L67: L65:   - Identifies operational gaps with severity ratings
L68: L66: - **LLM Integration**: OpenAI API (via Replit AI Integrations env vars: `AI_INTEGRATIONS_OPENAI_API_KEY`, `AI_INTEGRATIONS_OPENAI_BASE_URL`)
L69: L67: - The Express server spawns the Python analyzer as a child process
L70: L68: 
L71: L69: ### Database
L72: L70: 
L73: L71: - **Engine**: PostgreSQL (required, referenced via `DATABASE_URL` env var)
L74: L72: - **ORM**: Drizzle ORM with `drizzle-zod` for schema-to-Zod validation
L75: L73: - **Schema** (`shared/schema.ts`):
L76: L74:   - `projects` — id, url, name, mode (github/local/replit), status (pending/analyzing/completed/failed), createdAt
L77: L75:   - `analyses` — id, projectId, dossier (markdown text), claims (jsonb), howto (jsonb), coverage (jsonb), unknowns (jsonb), operate (jsonb), createdAt
L78: L76: - **Chat models** (`shared/models/chat.ts`):
L79: L77:   - `conversations` — id, title, createdAt
L80: L78:   - `messages` — id, conversationId, role, content, createdAt
L81: L79: - **Migrations**: Drizzle Kit with `drizzle-kit push` for schema sync
L82: L80: - **Storage Layer**: `server/storage.ts` implements `IStorage` interface with `DatabaseStorage` class
L83: L81: 
L84: L82: ### Replit Integrations (`server/replit_integrations/` and `client/replit_integrations/`)
L85: L83: 
L86: L84: Pre-built integration modules for AI features:
L87: L85: - **Chat** — Text-based conversation routes and storage using OpenAI
L88: L86: - **Audio** — Voice recording, playback, speech-to-text, text-to-speech with AudioWorklet
L89: L87: - **Image** — Image generation and editing via `gpt-image-1`
L90: L88: - **Batch** — Rate-limited batch processing with retries for LLM calls
L91: L89: 
L92: L90: These are utility modules that can be registered on the Express app as needed.
L93: L91: 
L94: L92: ### Key Design Decisions
L95: L93: 
L96: L94: 1. **Shared route definitions** — `shared/routes.ts` defines API contracts (paths, input schemas, response schemas) used by both frontend hooks and backend handlers. This ensures type safety across the stack.
L97: L95: 
L98: L96: 2. **Python + Node hybrid** — The analyzer logic lives in Python (better ecosystem for code analysis, rich CLI output) while the web layer is Node/Express. The server spawns Python as a child process rather than using a microservice architecture, keeping deployment simple.
L99: L97: 
L100: L98: 3. **Evidence-first analysis** — The analyzer is designed to cite file paths and line ranges for every claim. When evidence is missing, it must label findings as inference/unknown rather than hallucinate.
L101: L99: 
L102: L100: 4. **Polling for status** — The frontend polls project status every 2 seconds while analysis is in progress, switching to static once completed/failed.
L103: L101: 
L104: L102: ## External Dependencies
L105: L103: 
L106: L104: ### Required Services
L107: L105: - **PostgreSQL** — Primary database, must be provisioned with `DATABASE_URL` environment variable
L108: L106: - **OpenAI API** (via Replit AI Integrations) — Powers the code analysis LLM calls
L109: L107:   - `AI_INTEGRATIONS_OPENAI_API_KEY` — API key
L110: L108:   - `AI_INTEGRATIONS_OPENAI_BASE_URL` — Base URL for API
L111: L109: 
L112: L110: ### Key NPM Packages
L113: L111: - `express` v5 — HTTP server
L114: L112: - `drizzle-orm` + `drizzle-kit` — Database ORM and migrations
L115: L113: - `@tanstack/react-query` — Client-side data fetching and caching
L116: L114: - `wouter` — Client-side routing
L117: L115: - `react-markdown` — Markdown rendering for dossiers
L118: L116: - `framer-motion` — Animations
L119: L117: - `zod` + `drizzle-zod` — Runtime validation
L120: L118: - `vite` — Frontend build and dev server
L121: L119: - `esbuild` — Server build
L122: L120: 
L123: L121: ### Key Python Packages
L124: L122: - `typer` — CLI framework
L125: L123: - `openai` — LLM API client
L126: L124: - `rich` — Console output formatting
L127: L125: - `python-dotenv` — Environment variable loading
L128: L126: 
L129: L127: ### Dev/Build Tools
L130: L128: - `tsx` — TypeScript execution for development
L131: L129: - `tailwindcss` + `postcss` + `autoprefixer` — CSS toolchain
L132: L130: - `@replit/vite-plugin-runtime-error-modal` — Dev error overlay
L133: 
L134: --- FILE: README.md ---
L135: L1: # Program Totality Analyzer
L136: L2: 
L137: L3: A static-artifact-anchored analysis tool that generates technical dossiers for software projects. It extracts what a system is, how to run it, what it needs, and what it cannot determine — with every claim citing `file:line` evidence backed by SHA-256 snippet hashes.
L138: L4: 
L139: L5: **Scope limitation:** PTA analyzes static artifacts only (source files, config, lockfiles). It does not observe runtime behavior, prove correctness, or guarantee security. Claims labeled VERIFIED mean "anchored to a hash-verified source snippet," not "proven true at runtime."
L140: L6: 
L141: L7: ## What It Does
L142: L8: 
L143: L9: Given a software project (GitHub repo, local folder, or Replit workspace), the analyzer produces:
L144: L10: 
L145: L11: | File | Contents |
L146: L12: |------|----------|
L147: L13: | `operate.json` | Operator dashboard: boot commands, integration points, deployment config, readiness scores, gaps with severity. Deterministic, evidence-bound. Every item is EVIDENCED, INFERRED, or UNKNOWN. |
L148: L14: | `target_howto.json` | Legacy: evidence-scoped run steps. Prefer `operate.json` for operator workflows. |
L149: L15: | `claims.json` | Verifiable claims about the system, each with file:line evidence and confidence scores |
L150: L16: | `coverage.json` | Scan metadata: files scanned, files skipped, Replit detection evidence |
L151: L17: | `replit_profile.json` | Replit-specific: port binding, secrets, external APIs, observability (only in Replit mode) |
L152: L18: | `DOSSIER.md` | Human-readable markdown dossier summarizing all findings |
L153: L19: | `index.json` | Full file index of scanned files |
L154: L20: | `packs/` | Evidence packs (docs, config, code, ops) used during analysis |
L155: L21: 
L156: L22: ## Install
L157: L23: 
L158: L24: ```bash
L159: L25: pip install -e .
L160: L26: ```
L161: L27: 
L162: L28: This registers the `pta` command. Alternatively, run as a module or directly:
L163: L29: 
L164: L30: ```bash
L165: L31: python -m server.analyzer.src --help
L166: L32: python server/analyzer/analyzer_cli.py --help
L167: L33: ```
L168: L34: 
L169: L35: ### Dependencies
L170: L36: 
L171: L37: - Python 3.11+
L172: L38: - Required packages: `typer`, `rich`, `openai`, `gitpython`, `jsonschema`, `python-dotenv`, `pydantic`
L173: L39: 
L174: L40: ## Usage
L175: L41: 
L176: L42: ### Three Modes
L177: L43: 
L178: L44: **GitHub repository:**
L179: L45: ```bash
L180: L46: pta analyze https://github.com/user/repo -o ./output
L181: L47: ```
L182: L48: 
L183: L49: **Local folder:**
L184: L50: ```bash
L185: L51: pta analyze ./path/to/project -o ./output
L186: L52: ```
L187: L53: 
L188: L54: **Replit workspace (run from inside the workspace):**
L189: L55: ```bash
L190: L56: pta analyze --replit -o ./output
L191: L57: ```
L192: L58: 
L193: L59: ### Deterministic Mode (`--no-llm`)
L194: L60: 
L195: L61: Skip all LLM calls and produce only deterministic, structurally-extracted outputs:
L196: L62: 
L197: L63: ```bash
L198: L64: pta analyze --replit --no-llm -o ./output
L199: L65: ```
L200: L66: 
L201: L67: This mode requires no API keys and produces reproducible results. It generates `operate.json` and readiness scoring without any LLM involvement. It extracts:
L202: L68: - Package scripts (dev, build, start)
L203: L69: - Lockfile-based install commands
L204: L70: - Environment variable references (names only, never values)
L205: L71: - Port binding configuration
L206: L72: - External API usage
L207: L73: - Replit platform detection
L208: L74: - Operational gaps with severity ratings
L209: L75: - Readiness scores (boot, integrate, deploy)
L210: L76: 
L211: L77: ### With LLM Analysis
L212: L78: 
L213: L79: For semantic analysis (architecture understanding, risk assessment, integration patterns):
L214: L80: 
L215: L81: ```bash
L216: L82: pta analyze --replit -o ./output
L217: L83: ```
L218: L84: 
L219: L85: Requires `AI_INTEGRATIONS_OPENAI_API_KEY` and `AI_INTEGRATIONS_OPENAI_BASE_URL` environment variables.
L220: L86: 
L221: L87: ### Scoping a Subdirectory
L222: L88: 
L223: L89: ```bash
L224: L90: pta analyze https://github.com/user/monorepo --root packages/api -o ./output
L225: L91: ```
L226: L92: 
L227: L93: ## Evidence Model
L228: L94: 
L229: L95: Every claim in the output cites structured evidence:
L230: L96: 
L231: L97: ```json
L232: L98: {
L233: L99:   "path": "server/index.ts",
L234: L100:   "line_start": 92,
L235: L101:   "line_end": 92,
L236: L102:   "snippet_hash": "75d345a78f84",
L237: L103:   "display": "server/index.ts:92"
L238: L104: }
L239: L105: ```
L240: L106: 
L241: L107: - `path` -- file path relative to project root
L242: L108: - `line_start` / `line_end` -- 1-indexed line range (never 0)
L243: L109: - `snippet_hash` -- first 12 hex chars of SHA-256 of the stripped line(s)
L244: L110: - `display` -- human-readable location string
L245: L111: 
L246: L112: For file-existence evidence (e.g., lockfile detection):
L247: L113: 
L248: L114: ```json
L249: L115: {
L250: L116:   "kind": "file_exists",
L251: L117:   "path": "package-lock.json",
L252: L118:   "snippet_hash": "053150b640a7",
L253: L119:   "display": "package-lock.json (file exists)"
L254: L120: }
L255: L121: ```
L256: L122: 
L257: L123: ### Gap Severity
L258: L124: 
L259: L125: Operational gaps in `operate.json` include a severity rating:
L260: L126: - **high** — blocks boot or core execution
L261: L127: - **medium** — impacts deployment maturity
L262: L128: - **low** — best-practice or observability improvements
L263: L129: 
L264: L130: ### Verification
L265: L131: 
L266: L132: Snippet hashes are re-checked against source files: the analyzer re-reads the cited line range, strips whitespace, hashes the result, and confirms it matches the claimed hash. Claims that fail hash verification are capped at confidence 0.20 and marked `"status": "unverified"`.
L267: L133: 
L268: L134: **Important:** Hash verification confirms that a snippet exists at the cited location. It does not prove that the code behaves as described, is secure, or is free of bugs. PTA is not a security scanner, compliance certification tool, or correctness prover.
L269: L135: 
L270: L136: ### Whitespace Policy
L271: L137: 
L272: L138: Lines are stripped (trimmed) before hashing. This normalizes indentation differences across editors and formatters. Both evidence creation and verification use the same canonicalization.
L273: L139: 
L274: L140: ## Security
L275: L141: 
L276: L142: - **Symlink protection**: Every path component is checked. If any component in the path tree is a symlink, the file is rejected.
L277: L143: - **Path containment**: All resolved paths must remain within the project root (`relative_to()` check after `resolve()`).
L278: L144: - **Binary detection**: Null bytes in the first 4KB trigger rejection before text decoding.
L279: L145: - **Traversal prevention**: `..` segments and absolute paths are rejected.
L280: L146: - **Secret safety**: Only environment variable names are extracted, never their values.
L281: L147: - **Self-skip**: The analyzer excludes its own source files from analysis to prevent false-positive pattern matches.
L282: L148: 
L283: L149: ## Output Files
L284: L150: 
L285: L151: ### `operate.json`
L286: L152: 
L287: L153: Operator dashboard model with:
L288: L154: - `boot` -- install, dev, prod commands and port bindings, each with evidence tier
L289: L155: - `integrate` -- endpoints, env vars, auth mechanisms with evidence
L290: L156: - `deploy` -- Docker, platform hints, CI/CD, build commands with evidence
L291: L157: - `readiness` -- scores (0-100) for boot, integrate, deploy categories with reasons
L292: L158: - `gaps` -- operational gaps with severity (high/medium/low), rank, and action
L293: L159: - `runbooks` -- numbered step sequences for local_dev, production, and integration
L294: L160: - `snapshot` -- observability, migrations, and architectural metadata
L295: L161: 
L296: L162: All items carry a status of EVIDENCED, INFERRED, or UNKNOWN. EVIDENCED items include file:line references and SHA-256 snippet hashes. UNKNOWN items include an `unknown_reason`.
L297: L163: 
L298: L164: ### `target_howto.json` (legacy)
L299: L165: 
L300: L166: Legacy operator manual. Prefer `operate.json` for operator workflows. Contains:
L301: L167: - `prereqs` -- required runtimes
L302: L168: - `install_steps` -- with commands and evidence
L303: L169: - `config` -- environment variables with file:line references
L304: L170: - `run_dev` / `run_prod` -- start commands with evidence
L305: L171: - `replit_execution_profile` -- port binding, secrets, external APIs (Replit mode)
L306: L172: - `unknowns` -- things the analyzer could not determine
L307: L173: - `completeness` -- scoring with missing items
L308: L174: 
L309: L175: ### `claims.json`
L310: L176: 
L311: L177: Array of verifiable claims:
L312: L178: - `statement` -- what is claimed
L313: L179: - `confidence` -- 0.0 to 1.0
L314: L180: - `evidence` -- array of evidence objects with `snippet_hash_verified: true/false`
L315: L181: - `status` -- `"evidenced"` or `"unverified"`
L316: L182: 
L317: L183: ### `coverage.json`
L318: L184: 
L319: L185: - `mode_requested` / `mode` -- analysis mode
L320: L186: - `scanned` / `skipped` -- file counts
L321: L187: - `replit_detected` -- boolean
L322: L188: - `replit_detection_evidence` -- evidence for Replit detection
L323: L189: - `self_skip` -- analyzer self-exclusion details
L324: L190: 
L325: L191: ## Troubleshooting
L326: L192: 
L327: L193: ### "No module named 'core'"
L328: L194: 
L329: L195: Run from the repo root, or install with `pip install -e .`
L330: L196: 
L331: L197: ### Missing `DATABASE_URL`
L332: L198: 
L333: L199: The analyzer itself does not need a database. `DATABASE_URL` appears in outputs because it detects the target project's database configuration. No action needed for the analyzer.
L334: L200: 
L335: L201: ### Missing OpenAI environment variables
L336: L202: 
L337: L203: Only required when running without `--no-llm`. Set:
L338: L204: ```bash
L339: L205: export AI_INTEGRATIONS_OPENAI_API_KEY=your-key
L340: L206: export AI_INTEGRATIONS_OPENAI_BASE_URL=https://api.openai.com/v1
L341: L207: ```
L342: L208: 
L343: L209: ### "Port already in use"
L344: L210: 
L345: L211: The analyzer does not bind any ports. If you see port errors, they come from the target project's web server, not the analyzer.
L346: L212: 
L347: L213: ## Architecture
L348: L214: 
L349: L215: Two strictly separated layers:
L350: L216: 
L351: L217: 1. **Structural layer** (deterministic) — file indexing, pattern matching, evidence extraction. Outputs are reproducible and hash-verified against source artifacts.
L352: L218: 2. **Semantic layer** (LLM-powered, optional) — architecture interpretation, risk assessment, integration analysis. Outputs are labeled as LLM-generated and carry confidence scores, not deterministic guarantees.
L353: L219: 
L354: L220: The `--no-llm` flag gives you only the structural layer. The semantic layer adds interpretation but is namespaced separately and never contaminates structural evidence.
L355: L221: 
L356: L222: ## Running Tests
L357: L223: 
L358: L224: ```bash
L359: L225: bash scripts/smoke_test.sh
L360: L226: ```
L361: L227: 
L362: L228: ## License
L363: L229: 
L364: L230: MIT
L365: 
L366: --- FILE: Dockerfile ---
L367: L1: FROM node:20-slim AS base
L368: L2: WORKDIR /app
L369: L3: 
L370: L4: FROM base AS deps
L371: L5: COPY package.json package-lock.json ./
L372: L6: RUN npm ci --ignore-scripts
L373: L7: 
L374: L8: FROM base AS build
L375: L9: COPY --from=deps /app/node_modules ./node_modules
L376: L10: COPY . .
L377: L11: RUN npm run build
L378: L12: 
L379: L13: FROM base AS runtime
L380: L14: COPY --from=deps /app/node_modules ./node_modules
L381: L15: COPY --from=build /app/dist ./dist
L382: L16: COPY package.json ./
L383: L17: ENV NODE_ENV=production
L384: L18: EXPOSE 5000
L385: L19: HEALTHCHECK --interval=30s --timeout=5s --start-period=10s \
L386: L20:   CMD curl -f http://localhost:5000/health || exit 1
L387: L21: CMD ["npm", "start"]
L388: 
L389: --- FILE: client/requirements.md ---
L390: L1: ## Packages
L391: L2: react-markdown | For rendering the analysis dossier
L392: L3: framer-motion | For smooth page transitions and loading effects
L393: L4: clsx | Utility for constructing className strings conditionally
L394: L5: tailwind-merge | Utility for merging Tailwind classes efficiently
L395: L6: 
L396: L7: ## Notes
L397: L8: - Theme: Technical, dark mode, "Program Totality Analyzer" aesthetic.
L398: L9: - Fonts: Space Grotesk (headers), JetBrains Mono (code/data), Inter (UI).
L399: L10: - Polling: Project status needs polling to show "Analyzing..." vs "Completed".
L400: 
L401: --- FILE: examples/README.md ---
L402: L1: # Example Outputs
L403: L2: 
L404: L3: These are sample outputs from the Program Totality Analyzer running in `--no-llm` (deterministic) mode against its own workspace.
L405: L4: 
L406: L5: ## Files
L407: L6: 
L408: L7: - `out/operate.sample.json` — Operator dashboard: boot commands, integration points, deployment config, readiness scores, gaps with severity. Deterministic, evidence-bound.
L409: L8: - `out/target_howto.sample.json` — Legacy operator manual. Prefer `operate.json` for operator workflows.
L410: L9: - `out/coverage.sample.json` — Scan metadata: mode requested, files scanned/skipped, Replit detection evidence, self-skip configuration.
L411: L10: 
L412: L11: ## Generating Your Own
L413: L12: 
L414: L13: ```bash
L415: L14: pta analyze --replit --no-llm -o ./my_output
L416: L15: ```
L417: L16: 
L418: L17: Or for a GitHub repo:
L419: L18: 
L420: L19: ```bash
L421: L20: pta analyze https://github.com/user/repo -o ./my_output
L422: L21: ```
L423: L22: 
L424: L23: Or for a local folder:
L425: L24: 
L426: L25: ```bash
L427: L26: pta analyze ./path/to/project -o ./my_output
L428: L27: ```
L429: 
L430: --- FILE: docs/dossiers/lantern_program_totality_dossier.md ---
L431: L1: ---
L432: L2: title: Lantern Program Totality Dossier
L433: L3: generated_by: PTA / Lantern
L434: L4: mode: deterministic+curated
L435: L5: date: 2026-02-14
L436: L6: ---
L437: L7: 
L438: L8: # HALO-RECEIPTS: Program Totality Analyzer Dossier
L439: L9: 
L440: L10: ---
L441: L11: 
L442: L12: ## 1. **Identity of Target System**
L443: L13: 
L444: L14: **What it IS:**  
L445: L15: HALO-RECEIPTS (AI Receipts) is a forensic verification system specifically designed for AI conversation transcripts. It provides cryptographic verification (SHA-256, Ed25519), immutable receipt storage, tamper-evident audit trails, forensic export capabilities, and extensive auditing for post-hoc analysis. This system is both the backend API server (Node.js/Express/PostgreSQL/Drizzle ORM) and a modern React UI, bundling forensic guarantees directly into receipt management and export (README.md:3,9,60–61; replit.md:4,11–12,14–24).
L446: L16: 
L447: L17: **What it is NOT:**  
L448: L18: - **Not a real-time monitoring, content moderation, or multi-operator platform:** It is not designed for live chat moderation, direct truth judgment, or multi-user concurrency at the enforcement level (replit.md:55).
L449: L19: - **Not a database engine:** Instead, it relies on PostgreSQL for durable state.
L450: L20: - **Not a data lake or generic file archival platform.**
L451: L21: - **Not a replacement for WORM-compliant log systems, but can integrate via checkpoint anchoring (see below).**
L452: L22: - **Not a deployment framework:** The system expects to be deployed behind a reverse proxy or PaaS (README.md:24; SECURITY.md:54).
L453: L23: 
L454: L24: ---
L455: L25: 
L456: L26: ## 2. **Purpose & Jobs-to-be-done**
L457: L27: 
L458: L28: - **Forensic Conversation Integrity:** Operators can guarantee, via public proofs, that a transcript or "receipt" has not been tampered with since its recording date (README.md:9; replit.md:4).
L459: L29: - **Immutable Logging & Audit Trails:** Ensures all receipt actions (append, lock, kill switch, export, audit actions) are tracked in an append-only, hash-linked audit log, detecting insertions, deletions, reordering, and version tampering (SECURITY.md:8–10; STATE.md:165–169).
L460: L30: - **Cryptographic Verification:** Verifies that every receipt chain and audit log entry is both hash-linked (SHA-256) and checkpoint signed (Ed25519), with optional external anchoring (replit.md:36–46; STATE.md:116–121).
L461: L31: - **Forensic Export and Proof Packs:** Allows export of forensic packs (JSON) that can be offline verified and admitted as tamper-evident evidence (STATE.md:122–125; scripts/ci-forensic-gate.sh).
L462: L32: - **Regulatory Alignment:** System features mapped to compliance goals (21 CFR, HIPAA, SOC2, etc.) (replit.md:50, STATE.md:137).
L463: L33: - **Operator/Evidence Reliability:** Designed to provide demonstrable evidence for courts, regulators, or internal review.
L464: L34: 
L465: L35: ---
L466: L36: 
L467: L37: ## 3. **Capability Map**
L468: L38: 
L469: L39: | Capability             | Mechanism / Implementation | Evidence                                    |
L470: L40: |------------------------|---------------------------|---------------------------------------------|
L471: L41: | SHA-256 Hash Verification   | Canonicalized (c14n-v1) JSON hash | README.md:73–74; STATE.md:17,20             |
L472: L42: | Ed25519 Signatures     | Checkpoint signing, chain   | STATE.md:18,116–121; replit.md:36           |
L473: L43: | Immutable Storage      | Receipt lock, no mutation  | README.md:75; STATE.md:21,158–159           |
L474: L44: | Kill Switch            | Irreversible flag, disables outputs | README.md:76; STATE.md:22,157                |
L475: L45: | Audit Logs             | Append-only, hash-chained table | STATE.md:25–29,162                           |
L476: L46: | Forensic Export/Import | export_forensic_pack, verify_forensic_pack scripts | STATE.md:123–126                              |
L477: L47: | Forensic Sensors       | Interpreter, summarizer, claim extractor | README.md:78; STATE.md:80,85                  |
L478: L48: | Policy Enforcement     | Zod schemas, request shape limits | SECURITY.md:20–23                            |
L479: L49: | API Rate Limiting      | Per-IP, in-memory          | SECURITY.md:26–29; STATE.md:94; package.json:61 |
L480: L50: | Key Rotation & Anchoring | Multi-key support, anchor backends | replit.md:43–47                            |
L481: L51: | Secure API Structure   | API_KEY in x-api-key header; private/public endpoints | SECURITY.md:15–16,72                           |
L482: L52: | Client Auth Isolation  | LLMs see only transcript content | SECURITY.md:39–40; STATE.md:160,20           |
L483: L53: | UI Export & Compare    | Side-by-side comparison, JSONL/CSV export | README.md:143; replit.md:25,23                |
L484: L54: | Structured Logging     | JSON logs, in-memory counters | STATE.md:106–107,184                          |
L485: L55: | Health Checks          | /api/health, /api/ready    | STATE.md:37,43,100–101; docs/API_CONTRACTS.md:11,24 |
L486: L56: 
L487: L57: ---
L488: L58: 
L489: L59: ## 4. **Architecture Snapshot**
L490: L60: 
L491: L61: - **Frontend:** React (wouter router), Tailwind, shadcn/ui (README.md:59, client/src/App.tsx)
L492: L62: - **Backend:** Node.js 20, Express, Drizzle ORM (README.md:60–61; package.json)
L493: L63: - **Database:** PostgreSQL 14+ (README.md:24, .env.example:5)
L494: L64: - **Cryptography:** Node.js crypto (SHA-256), Ed25519 (STATE.md:18,116)
L495: L65: - **Session:** express-session, connect-pg-simple (package.json:51,57)
L496: L66: - **Audit Trail:** Hash-linking via prev_hash and payload_v, audit_head singleton row (STATE.md:9,25–29,48,52; drizzle.config.ts:9)
L497: L67: - **Forensic Export:** TypeScript scripts in `/scripts`, results in JSON proof packs (STATE.md:123–126)
L498: L68: - **CI/CD:** GitHub Actions, drift guard scripts, reproducible verifier zips (replit.md:39–42,57)
L499: L69: 
L500: L70: ---

--- FILE: .pytest_cache/README.md ---
L1: # pytest cache directory #
L2: 
L3: This directory contains data from the pytest's cache plugin,
L4: which provides the `--lf` and `--ff` options, as well as the `cache` fixture.
L5: 
L6: **Do not** commit this to version control.
L7: 
L8: See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.
