
--- FILE: AUDIT_REPORT.md ---
L1: # Lantern Audit Report (Ground Truth)
L2: 
L3: **Date**: January 21, 2026
L4: **Status**: PROCEED (Baseline Functionality Restored)
L5: 
L6: ## 1. Repository Map & Entrypoints
L7: 
L8: ### File Structure
L9: ```
L10: .
L11: ├── client
L12: │   ├── index.html          # Web Entrypoint
L13: │   └── src
L14: │       ├── main.tsx        # React Root
L15: │       ├── App.tsx         # Routing
L16: │       ├── pages
L17: │       │   └── lantern-extract.tsx # Main Application Logic
L18: │       └── lib
L19: │           └── lanternExtract.ts   # Core Extraction Engine (Client-Side)
L20: ├── server
L21: │   ├── index.ts            # Express Server Entrypoint
L22: │   ├── routes.ts           # API Routes (Currently Empty)
L23: │   └── storage.ts          # Backend Storage Adapter (MemStorage - Unused)
L24: ├── shared
L25: │   └── schema.ts           # Database Schema (Users - Unused)
L26: └── package.json
L27: ```
L28: 
L29: ### Entrypoints
L30: *   **Client (Dev)**: `vite dev --port 5000` (via `npm run dev:client`)
L31: *   **Server (Prod)**: `node dist/index.cjs` (via `npm start`)
L32: *   **Full Stack (Dev)**: `tsx server/index.ts` (via `npm run dev`)
L33: 
L34: ## 2. Runtime Architecture (As Built)
L35: 
L36: **Verdict**: **Hybrid (Client-Heavy)**.
L37: While the repository contains a full-stack scaffold (`server/`, `drizzle`), the **active** application logic is **100% Client-Side**.
L38: 
L39: *   **Data Flow**: User Input (UI) → `extract()` (Browser JS) → `localStorage` (Browser).
L40: *   **Server Role**: Currently acts only as a static asset host and API placeholder. `routes.ts` contains no active endpoints.
L41: *   **Storage**:
L42:     *   **Backend**: `MemStorage` (In-Memory Map) is implemented but **not connected** to the frontend.
L43:     *   **Frontend**: `localStorage` is the *actual* system of record for Lantern Packs.
L44: 
L45: ## 3. Network, Secrets & Telemetry
L46: 
L47: *   **Network Calls**: Zero application-level network calls found. No `fetch`, `axios`, or `XMLHttpRequest` calls inside `client/src` related to data transmission.
L48: *   **Secrets**: No secrets are loaded or accessed. The app operates as an isolated tool.
L49: *   **Telemetry**: No analytics or tracking scripts identified.
L50: 
L51: ## 4. Storage & Persistence Reality
L52: 
L53: *   **Declared Schema**: `shared/schema.ts` defines a Postgres `users` table (Drizzle ORM).
L54: *   **Actual Usage**:
L55:     *   The server uses `MemStorage` (ephemeral RAM) for the `users` interface.
L56:     *   The extraction app uses **Browser LocalStorage** (`key: lantern_packs`) for persisting extraction results.
L57:     *   **Risk**: Data is lost if browser cache is cleared.
L58: 
L59: ## 5. Extraction Engine Audit
L60: 
L61: **Module**: `client/src/lib/lanternExtract.ts`
L62: 
L63: *   **Methodology**: Deterministic Regex Heuristics (v0.1.5).
L64: *   **Determinism**: Uses a custom `mockHash` function to generate stable IDs based on content + offsets.
L65: *   **Segmentation**: Regex-based sentence approximation.
L66: *   **Quality**: Implements a strict "Quality Contract" with F1 scoring against golden fixtures (`client/src/fixtures/`).
L67: 
L68: ## 6. Known Failure Modes / Risks
L69: 
L70: 1.  **Data Loss**: `localStorage` is volatile. Users *will* lose work if cache is cleared.
L71: 2.  **Performance**: Large texts (>1MB) processed synchronously in the main thread will freeze the UI.
L72: 3.  **Heuristic Fragility**: Regex extraction is brittle for complex nested entities or non-standard quote formats.
L73: 4.  **Backend Disconnect**: The server exists but does nothing; upgrading to real persistence requires wiring up the `server/routes.ts`.
L74: 
L75: ## 7. Purchase-Grade Verdict
L76: 
L77: **PROCEED**.
L78: The core engine integrity is high (good rigorous code), even if the storage layer is currently prototyping-grade (`localStorage`). The architecture is clean and ready for Phase 3 upgrades (Backend Persistence).
L79: 
L80: ---
L81: 
L82: ## Current Breakage & Root Cause (Fixed)
L83: 
L84: *   **Issue**: Application failed to load ("Does not work at all").
L85: *   **Root Cause**:
L86:     1.  **Syntax Error**: Missing closing `</div>` tag in `client/src/pages/lantern-extract.tsx` (Line ~866) caused React parser failure.
L87:     2.  **Missing Logic**: Core `lanternExtract.ts` file was previously overwritten with placeholders, breaking the extraction function.
L88: *   **Fix**:
L89:     1.  Restored valid JSX structure.
L90:     2.  Restored full heuristic extraction engine logic.
L91:     3.  Verified via "Quality Dashboard" regression tests.

--- FILE: BOOK_OF_FIXES.md ---
L1: # Lantern Book of Fixes
L2: 
L3: Permanent record of high-risk incidents and remediation for audit purposes.
L4: 
L5: ---
L6: 
L7: ## Incident 001: lantern-extract.tsx Corruption
L8: 
L9: **Date:** 2026-01-22
L10: 
L11: ### Symptom
L12: - Build failure: `'import' and 'export' may only appear at the top level`
L13: - LSP reporting 50+ diagnostics in single file
L14: - Workflow unable to start
L15: 
L16: ### Root Cause
L17: Duplicate import block and partial function redefinition pasted inside component body at line 118. Structure:
L18: ```
L19: Lines 1-117: Valid component start
L20: Lines 118-247: Corrupted duplicate (imports + partial function + JSX fragments)
L21: Lines 248+: Rest of original component
L22: ```
L23: 
L24: ### Fix Approach
L25: 1. Added missing imports to top of file (createDossierFromExtract, Pack, useLocation)
L26: 2. Removed duplicate corrupted section (lines 118-247)
L27: 3. Restored missing function definitions (reset, toggleItem, handleLoadPack, handleCompare, downloadJSON, downloadPDF, runQualityTests)
L28: 4. Fixed type annotations (LanternPack[] → AnyPack[])
L29: 5. Fixed scoreExtraction call signature
L30: 
L31: ### Verification Gates Run
L32: - [x] `npm run build` - PASS
L33: - [x] Import count check: 22 top-level imports
L34: - [x] Export count check: 1 export default function
L35: - [x] Workflow running without console errors
L36: - [x] Manual: Extract → Promote → Editor flow
L37: 
L38: ### Files Changed
L39: - client/src/pages/lantern-extract.tsx
L40: - client/src/lib/lanternExtract.ts (added z import, LanternPack type export)
L41: 
L42: ---
L43: 
L44: ## Incident 002: Type Guard Upgrade
L45: 
L46: **Date:** 2026-01-22
L47: 
L48: ### Issue
L49: Structural type discrimination using `"pack_id" in p` could accidentally overlap if schemas evolve.
L50: 
L51: ### Old Pattern
L52: ```typescript
L53: const existing = savedPacks.find(p => "pack_id" in p ? p.pack_id : p.packId);
L54: ```
L55: 
L56: ### New Pattern
L57: ```typescript
L58: // storage.ts
L59: export function isExtractPack(p: AnyPack): p is LanternPack {
L60:   return "schema" in p && p.schema === "lantern.extract.pack.v1";
L61: }
L62: 
L63: export function isDossierPack(p: AnyPack): p is Pack {
L64:   return "schemaVersion" in p && (p as Pack).schemaVersion === 2;
L65: }
L66: 
L67: // Usage
L68: const existing = savedPacks.find(p => isExtractPack(p) ? p.pack_id : p.packId);
L69: ```
L70: 
L71: ### Verification
L72: - [x] `npm run build` - PASS
L73: - [x] Type guards exported and used consistently
L74: 
L75: ### Files Changed
L76: - client/src/lib/storage.ts (added type guards)
L77: - client/src/pages/lantern-extract.tsx (updated to use guards)
L78: 
L79: ---
L80: 
L81: ## Incident 003: Type Guard Regression (v1 Pack Orphaning)
L82: 
L83: **Date:** 2026-01-22
L84: 
L85: ### Symptom
L86: Architect review flagged that `isDossierPack` returning true only for schemaVersion === 2 would orphan legacy v1 packs (they would fail both guards and not be migrated or displayed).
L87: 
L88: ### Root Cause
L89: Type guard was too strict - checked for exact schemaVersion match instead of detecting "is a dossier pack" semantically.
L90: 
L91: ### Fix Approach
L92: Changed `isDossierPack` to detect dossier packs by presence of `packId` AND absence of extract schema:
L93: ```typescript
L94: export function isDossierPack(p: AnyPack): p is Pack {
L95:   return "packId" in p && !("schema" in p && (p as any).schema === "lantern.extract.pack.v1");
L96: }
L97: ```
L98: 
L99: This correctly identifies both v1 and v2 dossier packs for migration.
L100: 
L101: ### Verification
L102: - [x] `npm run build` - PASS
L103: - [x] 46/46 tests pass
L104: - [x] v1 packs will be detected and migrated by storage.loadLibrary()
L105: 
L106: ### Files Changed
L107: - client/src/lib/storage.ts
L108: 
L109: ---
L110: 
L111: ## Incident 004: Migration Field Transformation (Complete v1→v2)
L112: 
L113: **Date:** 2026-01-22
L114: 
L115: ### Symptom
L116: V1 pack acceptance tests failing with Zod validation errors: missing `packType`, `subjectName`, `timestamps` object, and wrong edge field names (`sourceId/targetId` vs `fromEntityId/toEntityId`).
L117: 
L118: ### Root Cause
L119: Migration logic only handled edge type remapping but not the structural field differences between v1 and v2 schemas.
L120: 
L121: ### Fix Approach
L122: Enhanced `migratePack()` to perform full field transformations:
L123: - `createdAt/updatedAt` → `timestamps: { created, updated }`
L124: - `title` → `subjectName`
L125: - Added default `packType: "public_figure"`
L126: - `sourceId/targetId` → `fromEntityId/toEntityId` on edges
L127: 
L128: ### Verification
L129: - [x] `npm run build` - PASS
L130: - [x] 57/57 tests pass (11 new v1 pack acceptance tests)
L131: - [x] v1 packs properly migrate and validate against PackSchema
L132: 
L133: ### Files Changed
L134: - client/src/lib/migrations.ts (enhanced field transformations)
L135: - client/src/lib/tests/unit/v1PackMigration.test.ts (new acceptance tests)
L136: 
L137: ---
L138: 
L139: ## Incident 005: Wrong Landing Route / Product Identity
L140: 
L141: **Date:** 2026-01-22
L142: 
L143: ### Symptom
L144: Screenshots showed "Sovereignty Navigation System" finance dashboard (home buying, savings curves) at root route instead of investigative Lantern.
L145: 
L146: ### Root Cause
L147: `/` routed to `Dashboard` component (sovereignty finance app) instead of the investigative Lantern.
L148: 
L149: ### Fix Approach
L150: 1. Created `client/src/pages/library.tsx` as investigative Lantern landing page
L151: 2. Updated `App.tsx` routing:
L152:    - `/` → Library (extracts + dossiers listing)
L153:    - `/legacy` → Dashboard (preserves old finance app)
L154: 3. Updated quick nav to show Library, Extract, Compare
L155: 
L156: ### Verification
L157: - [x] `npm run build` - PASS
L158: - [x] 57/57 tests pass
L159: - [x] `/` loads Library with investigative workflow actions
L160: - [x] No finance language on landing
L161: 
L162: ### Files Changed
L163: - client/src/pages/library.tsx (new)
L164: - client/src/App.tsx (routing update)
L165: 
L166: ---
L167: 
L168: ## Bookkeeping Standards
L169: 
L170: For any future incident:
L171: 1. Record symptom, root cause, fix approach
L172: 2. List verification gates run
L173: 3. List files changed
L174: 4. Note any behavior changes visible to users

--- FILE: CHANGELOG.md ---
L1: # Lantern Changelog
L2: 
L3: All notable changes to the Lantern investigative intelligence platform.
L4: 
L5: ## [1.0.0] - 2026-01-22
L6: 
L7: ### Complete Feature Set (M1-M12)
L8: 
L9: #### M1: Pack Schema & Storage
L10: - Implemented Pack v2 schema with entities, edges, claims, evidence
L11: - IndexedDB persistence layer with debounced saves
L12: - WHY: Foundation for all data operations
L13: 
L14: #### M2: Dossier Editor
L15: - Full CRUD for entities, edges, claims, evidence
L16: - Entity combobox with search
L17: - Relationship graph visualization
L18: - WHY: Core data curation interface
L19: 
L20: #### M3: Heuristic Analysis Engine
L21: - Influence Hubs (degree centrality)
L22: - Funding Gravity (monetary flow analysis)
L23: - Enforcement Map (coercive edge detection)
L24: - WHY: Shadow-Caste pattern detection
L25: 
L26: #### M4: Evidence Density Thresholds
L27: - Minimum evidence count gating per heuristic
L28: - "Insufficient Data" status propagation
L29: - WHY: Epistemic safety - prevent analysis on sparse data
L30: 
L31: #### M5: Report Generation
L32: - Structured report view with all findings
L33: - Section layout: title, explanation, content, receipt
L34: - WHY: Publication-ready artifact generation
L35: 
L36: #### M6: Markdown Export
L37: - Full report export with YAML frontmatter
L38: - Table escaping and formatting
L39: - WHY: Portable, version-controllable output
L40: 
L41: #### M7: Interpretation Limits & Disclaimers
L42: - Callout blocks explaining heuristic assumptions
L43: - "What This Does NOT Prove" sections
L44: - WHY: Epistemic safety - prevent misinterpretation
L45: 
L46: #### M8: Migration Transparency
L47: - Schema version tracking
L48: - Migration notes in reports
L49: - WHY: Audit trail for data transformations
L50: 
L51: #### M9: Print Layout
L52: - Print-optimized CSS
L53: - Hidden navigation in print mode
L54: - WHY: Physical distribution capability
L55: 
L56: #### M10: Claim Scope Tracking
L57: - claimType and claimScope fields
L58: - utterance vs content attribution
L59: - WHY: Precision in attribution claims
L60: 
L61: #### M11: Robustness & Sensitivity Checks
L62: - Single-point failure testing
L63: - Stability classification per finding
L64: - WHY: Confidence in finding durability
L65: 
L66: #### M12: Comparison Integrity
L67: - SHA-256 fingerprints for reports
L68: - Cross-dossier comparison binding
L69: - Tamper-evidence for comparisons
L70: - WHY: Cryptographic audit trail
L71: 
L72: ### Bug Fixes
L73: 
L74: #### lantern-extract.tsx Corruption (2026-01-22)
L75: - SYMPTOM: Imports inside function body, duplicate code blocks
L76: - CAUSE: Accidental paste/merge corruption
L77: - FIX: Normalized imports to top-level, removed duplicates, restored handlers
L78: - FILES: client/src/pages/lantern-extract.tsx
L79: - VERIFICATION: Build pass, manual testing
L80: 
L81: #### Type Guard Upgrade (2026-01-22)
L82: - OLD: `"pack_id" in p` structural check
L83: - NEW: `isExtractPack(p)` / `isDossierPack(p)` with schema literals
L84: - FILES: client/src/lib/storage.ts, client/src/pages/lantern-extract.tsx
L85: - WHY: Future-proof discriminated union handling
L86: 
L87: ---
L88: 
L89: ## Post-v1 Hardening - Completed 2026-01-22
L90: 
L91: ### Technical Hardening
L92: - [x] Type guards (isExtractPack, isDossierPack) - with v1/v2 compatibility
L93: - [x] Migration logic enhanced for v1→v2 field transformations
L94: - [x] Fingerprint determinism verified
L95: 
L96: ### UX Polish
L97: - [x] Report view: print CSS, interpretation limits callout
L98: - [x] Comparison view: stats cards, match badges
L99: - [x] Editor view: claimScope selector with helper text
L100: 
L101: ### Verification
L102: - [x] 57/57 tests passing (including v1 pack acceptance tests)
L103: - [x] Build successful
L104: - [x] No LSP errors
L105: 
L106: ---
L107: 
L108: ## Route Identity Fix - 2026-01-22
L109: 
L110: ### Changed
L111: - Root route (`/`) now shows **Library** page (investigative Lantern entrypoint)
L112: - Legacy "Sovereignty" dashboard moved to `/legacy`
L113: - Quick nav updated: Library, Extract, Compare
L114: 
L115: ### Added
L116: - `client/src/pages/library.tsx` - New landing page with:
L117:   - Extract packs section (schema: lantern.extract.pack.v1)
L118:   - Dossier packs section (schemaVersion: 2)
L119:   - Quick actions: New Extract, Compare Dossiers
L120:   - Stats and timestamps for each pack
L121: 
L122: ### Why
L123: Screenshots showed wrong product identity ("Sovereignty Navigation System" instead of investigative Lantern).
L124: User must see the correct investigative workflow on landing.

--- FILE: LANTERN_CORE_BOUNDARY.md ---
L1: # Lantern Core Boundary
L2: 
L3: **Definition:**
L4: "Lantern Core" is the portable, rule-based extraction engine within the application. It MUST be separable from the UI, storage layer, and React framework. It operates purely on string inputs and returns JSON outputs.
L5: 
L6: ## Core File Manifest
L7: 
L8: The following files constitute the "Lantern Core" and must remain dependency-free (except for internal helpers):
L9: 
L10: ### 1. Heuristics (The Brain)
L11: *   `client/src/lib/heuristics/segmenters/sentenceSegmenter.ts` (Segmentation Rules)
L12: *   `client/src/lib/heuristics/entities/entityExtractor.ts` (Extraction Logic)
L13: *   `client/src/lib/heuristics/entities/entityCanonicalizer.ts` (Normalization)
L14: *   `client/src/lib/heuristics/entities/entityTierer.ts` (Classification)
L15: *   `client/src/lib/heuristics/metrics/metricNormalizer.ts` (Schema & Types)
L16: 
L17: ### 2. Validation (The Guard)
L18: *   `client/src/lib/lanternExtract.ts` (Orchestrator & Provenance Validation)
L19:     *   *Note: Currently contains some orchestration logic that should ideally be split in M3, but `validateProvenance` and `extract` are the core functions.*
L20: 
L21: ### 3. Helpers
L22: *   `client/src/lib/lanternExtract.ts` (Exported helper: `mockHash`)
L23: 
L24: ## Core I/O Contract
L25: 
L26: ### Input
L27: ```typescript
L28: type ExtractionOptions = {
L29:   mode: "conservative" | "balanced" | "broad";
L30: };
L31: 
L32: // Function Signature
L33: function extract(text: string, options: ExtractionOptions): LanternPackResult;
L34: ```
L35: 
L36: ### Output
L37: ```typescript
L38: type LanternPackResult = {
L39:   items: {
L40:     entities: EntityItem[];
L41:     quotes: QuoteItem[];
L42:     metrics: MetricItem[];
L43:     timeline: TimelineItem[];
L44:   };
L45:   stats: EngineStats;
L46:   stable_source_hash: string;
L47: };
L48: ```
L49: 
L50: ### Artifact Shapes
L51: 
L52: **1. Entity (Mandatory)**
L53: ```typescript
L54: type EntityItem = {
L55:   id: string; // Stable Hash
L56:   provenance: {
L57:     start: number;
L58:     end: number;
L59:     sentence: string; // Legacy
L60:     sentence_text: string;
L61:     sentence_start: number;
L62:     sentence_end: number;
L63:   };
L64:   text: string; // Raw substring
L65:   canonical_family_id: string; // Hash of canonical text
L66:   confidence: number;
L67:   included: boolean;
L68:   type: "Organization" | "Person" | "Location" | "Event" | "Product";
L69:   // M3 Target: canonical_text (string), tier (enum) explicitly exposed
L70: };
L71: ```
L72: 
L73: **2. Metric (Mandatory)**
L74: ```typescript
L75: type MetricItem = {
L76:   id: string;
L77:   provenance: { /* same as above */ };
L78:   value: string; // Raw value string
L79:   unit: string;  // Raw unit string
L80:   metric_kind: "scalar" | "range" | "ratio" | "rate";
L81:   normalized_value?: number; // Best-effort scalar
L82:   // M3 Target: Full NormalizedMetric schema with min/max/currency
L83: };
L84: ```
L85: 
L86: **3. Segment (M3 Target / Internal Only Today)**
L87: ```typescript
L88: type Segment = {
L89:   start: number;
L90:   end: number;
L91:   text: string;
L92: };
L93: ```
L94: 
L95: ## Prohibited Dependencies in Core
L96: 
L97: The Core module **MUST NOT** import or use:
L98: *   `react` / `jsx` / `tsx` (UI components)
L99: *   `@/components/ui/*` (Shadcn/UI)
L100: *   `window` / `document` / `localStorage` (Browser APIs)
L101:     *   *Exception: `lanternExtract.ts` currently runs in browser, but logic should be isomorphic.*
L102: *   `replit` specific environment variables (unless injected via config)
L103: *   Network calls (`fetch`, `axios`) inside extraction logic (Extraction is offline/local).
L104: 
L105: ## Future Architectural Goal
L106: Isolate `client/src/lib/lanternExtract.ts` and `client/src/lib/heuristics/` into a separate package or strict module to enforce this boundary physically.

--- FILE: LANTERN_SYSTEM_SNAPSHOT.md ---
L1: # LANTERN SYSTEM SNAPSHOT
L2: 
L3: Portable reference document for external review.
L4: Generated: 2026-01-22
L5: 
L6: ---
L7: 
L8: ## A. What This System Is
L9: 
L10: Lantern is an evidentiary record system for investigative analysis. It allows analysts to extract structured data from unstructured text, curate dossiers of entities and relationships, apply bounded heuristics to detect structural patterns, and generate publication-ready reports with cryptographic integrity verification. All analysis requires human-in-the-loop curation. The system does not issue verdicts. Outputs are designed to withstand legal and journalistic scrutiny by emphasizing traceability over persuasion.
L11: 
L12: ---
L13: 
L14: ## B. What This System Is NOT
L15: 
L16: - **Not generative truth**: Lantern does not generate facts or conclusions.
L17: - **Not predictive**: Lantern does not forecast outcomes or behaviors.
L18: - **Not a recommender**: Lantern does not suggest actions or next steps.
L19: - **Not verdict-issuing**: Lantern does not determine guilt, innocence, intent, or truth.
L20: - **Not autonomous**: All dossier content is human-curated. Heuristics produce conditional findings, not conclusions.
L21: 
L22: ---
L23: 
L24: ## C. Core Objects & Schemas
L25: 
L26: ### Extract Pack
L27: - **Schema discriminator**: `schema: "lantern.extract.pack.v1"`
L28: - **Contents**: Entities, quotes, metrics, timeline events extracted from source text
L29: - **Provenance**: All items include character offsets to original source
L30: 
L31: ### Dossier Pack (v2)
L32: - **Schema discriminator**: `schemaVersion: 2`, presence of `packId`, absence of extract schema
L33: - **Contents**: Curated entities, edges (relationships), claims, evidence links
L34: - **Migration**: v1 packs auto-migrate on load; transformations logged
L35: 
L36: ### Report Snapshot
L37: - **Contents**: Frozen dossier state plus heuristic outputs
L38: - **Integrity**: SHA-256 fingerprint of canonical data
L39: - **Export format**: Markdown with YAML frontmatter
L40: 
L41: ### Comparison Report
L42: - **Contents**: Structural alignment of two dossiers
L43: - **Integrity**: Fingerprint A + Fingerprint B + comparison timestamp → comparison fingerprint
L44: - **Sufficiency**: Both packs must meet evidence thresholds independently
L45: 
L46: ---
L47: 
L48: ## D. System Flow (Stepwise)
L49: 
L50: 1. **Upload**: User provides source text (article, document, transcript)
L51: 2. **Extract**: System extracts entities, quotes, metrics, timeline events with character offsets
L52: 3. **Promote**: User promotes Extract Pack to Dossier Pack for curation
L53: 4. **Curate**: User adds/edits entities, edges, claims, and evidence
L54: 5. **Analyze**: Report view applies heuristics to dossier data
L55: 6. **Report**: Structured report generated with findings, limits, interpretation disclaimers
L56: 7. **Export**: Markdown export with integrity fingerprint for external use
L57: 
L58: ---
L59: 
L60: ## E. Heuristic Lenses
L61: 
L62: ### Influence Hubs
L63: - **Measures**: Degree centrality — which entities have the most relationship edges
L64: - **Minimum threshold**: 3 edges
L65: - **Insufficient data behavior**: Returns "Insufficient Data" status; no findings produced
L66: 
L67: ### Funding Gravity
L68: - **Measures**: Concentration and flow of monetary edges (funded_by, donated_to, grant_from, etc.)
L69: - **Minimum threshold**: 2 funding edges
L70: - **Insufficient data behavior**: Returns "Insufficient Data" status; no findings produced
L71: 
L72: ### Enforcement Map
L73: - **Measures**: Presence of coercive edges (censored_by, banned_by, sued_by, fired_by, etc.)
L74: - **Minimum threshold**: 1 enforcement edge
L75: - **Insufficient data behavior**: Returns "No Enforcement Edges Detected"
L76: 
L77: ### Sensitivity / Robustness
L78: - **Measures**: Whether findings survive removal of any single entity or edge
L79: - **Minimum threshold**: 2 data points for meaningful simulation
L80: - **Output classifications**: ROBUST, FRAGILE, SINGLE_POINT
L81: - **Insufficient data behavior**: Section omitted from report
L82: 
L83: ---
L84: 
L85: ## F. Safety & Epistemic Controls
L86: 
L87: ### Claim Scope
L88: - **Utterance**: "X said Y" — records that X made this statement
L89: - **Content**: "Y is true" — records an assertion about Y itself
L90: - User must select scope when recording claims to prevent conflation
L91: 
L92: ### Migration Logs
L93: - All schema transformations recorded in `pack.migrationLog[]`
L94: - Includes ISO timestamp and description of each change
L95: - Displayed in reports for audit
L96: 
L97: ### Interpretation Limits
L98: - Every report includes explicit disclaimers about what findings do NOT imply
L99: - Heuristic outputs are conditional on recorded data only
L100: 
L101: ### Evidence Density Thresholds
L102: - Each heuristic has a minimum edge count before analysis proceeds
L103: - System refuses to produce findings below threshold
L104: - Refusal is correct behavior, not failure
L105: 
L106: ---
L107: 
L108: ## G. Integrity & Auditability
L109: 
L110: ### Report Fingerprinting
L111: - **Algorithm**: SHA-256
L112: - **Input**: Canonical JSON of dossier data (sorted keys, sorted arrays)
L113: - **Purpose**: Tamper-evidence for the exact state analyzed
L114: 
L115: ### Comparison Fingerprinting
L116: - **Method**: Hash(Fingerprint A + Fingerprint B + timestamp)
L117: - **Purpose**: Cryptographically binds two dossiers at comparison time
L118: 
L119: ### Non-Auto-Updating Snapshots
L120: - Reports and comparisons are frozen at generation time
L121: - Changes to underlying dossier do not update existing reports
L122: - Each report is a point-in-time audit record
L123: 
L124: ### Legal and Journalistic Relevance
L125: - Fingerprints allow independent verification of analyzed data
L126: - Chain of custody can be established via export + fingerprint
L127: - Supports FOIA responses, legal discovery, editorial review
L128: 
L129: ---
L130: 
L131: ## H. Intended Use Cases
L132: 
L133: - **Investigative journalism**: Mapping relationships between actors across source materials
L134: - **Legal review**: Structuring evidence and claims with traceable provenance
L135: - **Historical analysis**: Recording documented relationships and events without inference
L136: - **Adversarial inquiry**: Testing claims against evidence with explicit sufficiency checks
L137: 
L138: ---
L139: 
L140: ## Visual Surfaces (Referenced)
L141: 
L142: | Surface | Purpose |
L143: |---------|---------|
L144: | Library View | Landing page. Lists all Extract Packs and Dossier Packs. Entry point. |
L145: | Extract View | Text input. Produces Extract Packs from source text. |
L146: | Dossier Editor | CRUD interface for entities, edges, claims, evidence. |
L147: | Report View | Read-only analysis output with heuristics, limits, fingerprint. |
L148: | Comparison View | Cross-dossier structural alignment with dual fingerprint binding. |
L149: | Reference View | "How Lantern Works" documentation (method, limits, safeguards). |
L150: 
L151: ---
L152: 
L153: *End of snapshot.*

--- FILE: M2_SUMMARY.md ---
L1: # M2 Summary: Heuristic Reliability (Consolidation)
L2: 
L3: **Scope:** M2 (Heuristic Reliability)
L4: **Date:** January 21, 2026
L5: **Status:** COMPLETE (Ready for Productization)
L6: 
L7: ## Certified Components
L8: 
L9: The following heuristic engines are now certified as **deterministic, rule-based, and provenance-enforced**:
L10: 
L11: 1.  **Sentence Segmentation (`M2.1`)**
L12:     *   Rule-based splitting with abbreviation whitelist (titles, commercial, geography).
L13:     *   Protected constructs: Decimals, currency, parentheses, quotes.
L14:     *   Zero-mutation offsets.
L15: 
L16: 2.  **Entity Extraction & Tiering (`M2.2`)**
L17:     *   Segment-based extraction (prevents cross-sentence merging).
L18:     *   Canonicalization (whitespace, punctuation trimming, suffix normalization).
L19:     *   Tiering (PRIMARY/SECONDARY/NOISE) based on patterns, suffixes, and repetition.
L20:     *   Stable ID generation (hash of canonical text + offsets).
L21: 
L22: 3.  **Metric Normalization (`M2.3`)**
L23:     *   **Status:** Certified (Schema + Rules), Integration Pending (M3).
L24:     *   Schema defined for scalar and range values.
L25:     *   Unresolved status handling defined for ambiguous units.
L26:     *   *Note: Full runtime wiring into the extraction loop is an M3 task.*
L27: 
L28: 4.  **Provenance Tightening (`M2.4`)**
L29:     *   Strict enforcement of `NO OFFSET, NO ITEM`.
L30:     *   Validation of finite bounds, logical order, and content integrity.
L31:     *   Expanded sentence context in provenance object.
L32: 
L33: ## Hard Invariants
L34: 
L35: These invariants are strictly enforced in the codebase. Violations result in item discard or transaction rejection.
L36: 
L37: *   **A) Determinism:** Same input text + same options → Exact same Output JSON (Artifacts, IDs, Offsets). Verified via 5-run stability tests.
L38: *   **B) No Offset, No Item:** Any extracted item with missing, invalid, or negative offsets is silently discarded during validation.
L39: *   **C) Offsets are Document-Absolute:** All offsets reference the original source string indices (0 to N).
L40: *   **D) Stable IDs:** IDs are deterministic hashes derived strictly from `content + start_offset + end_offset`. UUIDs/Randomness are PROHIBITED.
L41: *   **E) Binary Import Policy:** On `pack_id` collision during import, the policy is **SKIP** (preserve existing). No field-level merging.
L42: *   **F) No Guessing:** Ambiguous metrics or entities are marked `UNRESOLVED` or `NOISE`, not guessed. (Enforced by Tiering logic for Entities; Defined by Schema rules for Metrics).
L43: 
L44: ## Discard Taxonomy
L45: 
L46: | Reason | Enforced By | Example | Action |
L47: | :--- | :--- | :--- | :--- |
L48: | **Invalid Offsets** | `validateProvenance` | `start: -1`, `end: 5` | **DISCARD** (Counted in stats) |
L49: | **Content Mismatch** | `validateProvenance` | Text at offset `"Pear"` != Item `"Apple"` | **DISCARD** |
L50: | **Cross-Boundary** | `validateProvenance` | `start > end` | **DISCARD** |
L51: | **Noise Entity** | `tierEntities` | Single occurrence of "However" (Sentence Initial) | **EXCLUDE** (Tier=NOISE, included=false) |
L52: | **Ambiguous Unit** | `MetricNormalizer` (Schema) | `5m` (Minutes vs Million) | **MARK UNRESOLVED** (Keep item, status=unresolved) |
L53: 
L54: ## Operational Proof
L55: 
L56: ### Verification Commands
L57: ```bash
L58: # 1. Segmenter Regression
L59: npx tsx client/src/scripts/test-segmenter.ts
L60: 
L61: # 2. Unit Test Suite (Dedupe, Entities, Provenance)
L62: npx vitest run client/src/lib/tests/unit/
L63: 
L64: # 3. Provenance Stats
L65: npx tsx client/src/scripts/test-provenance.ts
L66: ```
L67: 
L68: ### Verification Output (Snapshot)
L69: ```
L70: === RUNNING SENTENCE SEGMENTER TESTS ===
L71: Unit Tests: 10/10 passed.
L72: === FIXTURE REGRESSION METRICS ===
L73: Total Fixtures: 5, Total Segments: 18, Short Segments: 0
L74: 
L75:  RUN  v4.0.17 /home/runner/workspace/client
L76:  ✓ src/lib/tests/unit/importDedupe.test.ts (2 tests)
L77:  ✓ src/lib/tests/unit/provenance.test.ts (4 tests)
L78:  ✓ src/lib/tests/unit/entityExtractor.test.ts (10 tests)
L79:  Test Files  3 passed (3)
L80:  Tests  16 passed (16)
L81: 
L82: === PROVENANCE REGRESSION METRICS ===
L83: Total Items Extracted: 24
L84: Discarded (Invalid/No Offset): 0
L85: Items with Invalid Offsets in Output: 0
L86: ```
L87: 
L88: ## Known Limitations
L89: 
L90: *   **Metric Normalization Wiring:** The `MetricNormalizer` schema is defined (`client/src/lib/heuristics/metrics/metricNormalizer.ts`), but the regex-based extractor in `lanternExtract.ts` is still used for the prototype. Full integration is a Productization task.
L91: *   **Complex Cross-Sentence Entities:** Entities spanning sentence boundaries (extremely rare in well-formed text) are split by the segmenter enforcement.
L92: *   **Internationalization:** Heuristics are currently tuned for English (suffixes, abbreviations, months).
L93: 
L94: ## Next Milestone Boundary
L95: 
L96: **M2 is COMPLETE.**
L97: All heuristic foundations are laid. No further heuristic research is required.
L98: The project is ready for **Productization (M3)**, focusing on UI refinement, bulk processing, and export workflows.

--- FILE: PRODUCT_PLAN.md ---
L1: # Lantern Product Plan
L2: 
L3: **Phase**: 2 (Productization) → **v1 Complete**
L4: **Status**: v1 Feature Complete
L5: **Date**: January 22, 2026
L6: 
L7: ---
L8: 
L9: ## 1. Product Definition
L10: 
L11: Lantern is an investigative journalism intelligence platform that enables analysts to curate dossiers, apply Shadow-Caste heuristics to detect structural patterns, and generate publication-ready reports with rigorous epistemic safety controls and cryptographic integrity verification.
L12: 
L13: **Target User**: Investigative Journalists, Financial Analysts, and Researchers who need rigorous, auditable analysis.
L14: 
L15: ### Core Features (v1 Complete)
L16: 1.  **Text Extraction**: Structured extraction from unstructured text (Entities, Quotes, Metrics, Timeline)
L17: 2.  **Dossier Curation**: Full CRUD for entities, edges, claims, evidence
L18: 3.  **Shadow-Caste Heuristics**: Influence Hubs, Funding Gravity, Enforcement Map
L19: 4.  **Epistemic Safety**: Evidence density thresholds, interpretation limits, disclaimers
L20: 5.  **Report Generation**: Publication-ready reports with cryptographic fingerprints
L21: 6.  **Cross-Dossier Comparison**: Entity overlap, structural alignment, comparison integrity
L22: 
L23: ---
L24: 
L25: ## 2. Module Status (M1-M12)
L26: 
L27: ### Completed Modules
L28: 
L29: | Module | Name | Status | Description |
L30: |--------|------|--------|-------------|
L31: | M1 | Pack Schema & Storage | ✅ DONE | IndexedDB persistence, Pack v2 schema |
L32: | M2 | Dossier Editor | ✅ DONE | Entity, edge, claim, evidence CRUD |
L33: | M3 | Heuristic Analysis | ✅ DONE | Influence, Funding, Enforcement heuristics |
L34: | M4 | Evidence Density | ✅ DONE | Minimum thresholds, insufficient data gating |
L35: | M5 | Report Generation | ✅ DONE | Structured report view |
L36: | M6 | Markdown Export | ✅ DONE | Full report export with YAML frontmatter |
L37: | M7 | Interpretation Limits | ✅ DONE | Disclaimers, "what this doesn't prove" |
L38: | M8 | Migration Transparency | ✅ DONE | Schema version tracking in reports |
L39: | M9 | Print Layout | ✅ DONE | Print-optimized CSS |
L40: | M10 | Claim Scope | ✅ DONE | utterance vs content attribution |
L41: | M11 | Robustness Checks | ✅ DONE | Sensitivity analysis, stability classification |
L42: | M12 | Comparison Integrity | ✅ DONE | SHA-256 fingerprints, tamper-evidence |
L43: 
L44: ---
L45: 
L46: ## 3. Post-v1 Hardening (COMPLETE)
L47: 
L48: ### Technical Hardening
L49: - [x] Type guards cleanup (isExtractPack, isDossierPack) - with v1/v2 compatibility
L50: - [x] Migration logic enhanced: v1→v2 field transformations
L51: - [x] Determinism verification for fingerprints
L52: - [x] V1 pack acceptance tests (57/57 passing)
L53: 
L54: ### UX Polish
L55: - [x] Report view: print CSS, interpretation limits callout
L56: - [x] Comparison view: stats cards, match badges, unavailable blocks
L57: - [x] Editor view: claim scope helpers with explanatory text
L58: 
L59: ### Documentation
L60: - [x] CHANGELOG.md created
L61: - [x] BOOK_OF_FIXES.md created (incident tracking)
L62: - [x] PRODUCT_PLAN.md updated
L63: 
L64: ### IA/Entry Route
L65: - [x] Root route `/` → Library (investigative Lantern landing)
L66: - [x] Legacy dashboard moved to `/legacy`
L67: - [x] Quick nav: Library, Extract, Compare
L68: 
L69: ---
L70: 
L71: ## 4. Architecture Decision Record
L72: 
L73: ### Client/Server Split
L74: - **Decision**: Client-Heavy (Thick Client)
L75: - **Rationale**: All analysis runs in-browser for zero latency, offline capability, and privacy
L76: 
L77: ### Persistence Strategy
L78: - **Current**: IndexedDB (Browser)
L79: - **Future**: Optional Postgres backend
L80: 
L81: ### Type Discrimination
L82: - **Pattern**: Explicit schema literals via type guards
L83: - **Extract Pack**: `schema === "lantern.extract.pack.v1"`
L84: - **Dossier Pack**: `schemaVersion === 2`
L85: 
L86: ---
L87: 
L88: ## 5. Known Limitations (v1)
L89: 
L90: 1. **No cloud sync**: Local-first only
L91: 2. **Text-only input**: No PDF/URL parsing
L92: 3. **Heuristic-only**: No LLM-based extraction
L93: 4. **Browser storage**: Data loss risk if cache cleared (mitigated by JSON export)
L94: 
L95: ---
L96: 
L97: **Next Step**: Complete Post-v1 Hardening (UX Polish + Verification Gates)

--- FILE: README.md ---
L1: # Lantern
L2: 
L3: Lantern is an interpretive inspection framework for reasoning about fixed evidence artifacts.
L4: 
L5: It integrates with cryptographic receipt systems to support tamper-evident audit trails. Currently demonstrated using HALO-RECEIPTS (private system; available by inquiry).
L6: 
L7: Lantern does not assert truth, intent, or legitimacy. It demonstrates how conclusions change under explicit interpretive lenses.
L8: 
L9: > **Status**: Early research. Open to technical collaboration on cryptographic verification and interpretive frameworks.
L10: 
L11: ## Key Features
L12: 
L13: - **Interpretive Lenses**: Analyze evidence under multiple explicit interpretive frameworks
L14: - **Tamper-Evident Trails**: Integration with cryptographic receipt systems (HALO-RECEIPTS)
L15: - **Local-First Design**: All data stored in browser localStorage for privacy
L16: - **Epistemic Discipline**: Clear separation between facts, claims, and interpretations
L17: - **Reference Implementation**: Demonstrates cryptographic verification workflows
L18: 
L19: ## Evidence Walkthrough
L20: 
L21: See `/demos/evidence-walkthrough` for a complete example of Lantern's interpretive discipline applied to a fixed exhibit.
L22: 
L23: ## Installation
L24: 
L25: **Prerequisites:**
L26: - Node.js 16+ 
L27: - npm or yarn
L28: 
L29: **Clone and install:**
L30: ```bash
L31: git clone https://github.com/Swixixle/Lantern.git
L32: cd Lantern
L33: npm install
L34: ```
L35: 
L36: ## Quick Start
L37: 
L38: ```bash
L39: npm install
L40: npm run dev
L41: ```
L42: 
L43: The app will be available at `http://localhost:5000`.
L44: 
L45: ## Usage Example
L46: 
L47: 1. Upload a fixed evidence artifact (PDF, document, etc.)
L48: 2. Select an interpretive lens (legal, technical, operational)
L49: 3. View how conclusions change under different frameworks
L50: 4. Export tamper-evident audit trail
L51: 
L52: See `/demos/evidence-walkthrough` for a complete walkthrough.
L53: 
L54: ## Production Build
L55: 
L56: ```bash
L57: npm run build
L58: npm start
L59: ```
L60: 
L61: ## How to Open the External App URL (Replit)
L62: 
L63: 1. In the Replit workspace, look for the **Webview** panel on the right side
L64: 2. Click the **"Open in new tab"** button (square with arrow icon) in the Webview header
L65: 3. Alternatively, copy the URL from the Webview address bar and paste it into a new browser tab
L66: 
L67: If you don't see a Webview panel:
L68: - Ensure the workflow is running (green status)
L69: - Try refreshing the page
L70: - Check the Console for error messages
L71: 
L72: ## Technical Stack
L73: 
L74: - **Frontend**: React 18, TypeScript, Vite
L75: - **UI Components**: Custom components with accessibility
L76: - **Storage**: Browser localStorage (local-first)
L77: - **Server**: Express (development hot-reload, production static host)
L78: - **Cryptographic Integration**: HALO-RECEIPTS adapter (private)
L79: 
L80: ## Troubleshooting
L81: 
L82: ### White Screen / App Won't Load
L83: 
L84: 1. Check the Console for errors
L85: 2. Visit `/__boot` (dev only) to verify the server is responding
L86: 3. If `/__boot` loads but the app doesn't, the issue is in the React layer
L87: 
L88: ### EADDRINUSE Error
L89: 
L90: This error means another process is already using port 5000.
L91: 
L92: **Fix:**
L93: 1. Stop all running workflows
L94: 2. Wait 5 seconds for the port to release
L95: 3. Restart the workflow
L96: 
L97: The server will now display a clear error message with fix instructions if this occurs.
L98: 
L99: ### Stale Process
L100: 
L101: If the app behaves unexpectedly:
L102: 1. Stop the workflow completely
L103: 2. Wait for the Console to show no activity
L104: 3. Start the workflow again
L105: 
L106: ## Architecture
L107: 
L108: - **Frontend**: React + Vite + TypeScript
L109: - **Storage**: Browser localStorage (local-first design)
L110: - **Server**: Express (static asset host in production)
L111: 
L112: ## Development Routes (Dev Only)
L113: 
L114: These routes are disabled in production:
L115: - `/__boot` - Plain HTML boot test
L116: - `/__health` - JSON health check with PID
L117: 
L118: ## Scripts
L119: 
L120: | Command | Description |
L121: |---------|-------------|
L122: | `npm run dev` | Start development server |
L123: | `npm run build` | Build for production |
L124: | `npm start` | Run production server |
L125: | `npm run check` | TypeScript type check |
L126: 
L127: ## Collaboration
L128: 
L129: Open to technical collaboration on:
L130: - Cryptographic verification protocols
L131: - Interpretive framework design
L132: - Audit trail architectures
L133: 
L134: **Contact**: Available via GitHub Issues or inquiry for HALO-RECEIPTS integration.

--- FILE: SECURITY.md ---
L1: # Security Policy
L2: 
L3: ## Supported Versions
L4: 
L5: | Version | Supported |
L6: | ------- | --------- |
L7: | main    | ✅        |
L8: 
L9: ## Reporting a Vulnerability
L10: 
L11: **Do not file public issues for security vulnerabilities.**
L12: 
L13: To report a security issue, email: **albearpig@gmail.com**
L14: 
L15: Include:
L16: - Description of the vulnerability
L17: - Steps to reproduce
L18: - Potential impact
L19: - Suggested fix (if any)
L20: 
L21: You should receive a response within 48 hours.
L22: 
L23: ## Scope
L24: 
L25: Lantern is early-stage research software. It is provided "as is" without warranties of any kind. No production use is recommended without independent security review.
L26: 
L27: ## Known Limitations
L28: 
L29: - Local-first storage means browser localStorage is the trust boundary
L30: - No server-side validation or authentication
L31: - Cryptographic verification depends on HALO-RECEIPTS integration (private system)

--- FILE: SYSTEM_MAP.md ---
L1: # LANTERN SYSTEM MAP
L2: 
L3: Complete architectural documentation of the Lantern Evidentiary Record System.
L4: Suitable for internal audit, legal review, and future maintainers.
L5: 
L6: ---
L7: 
L8: ## A. High-Level Architecture
L9: 
L10: ### Pages and Routes
L11: 
L12: | Route | Component | Purpose |
L13: |-------|-----------|---------|
L14: | `/` | Library | Landing page. Lists extract packs and dossier packs. Entry point for workflow. |
L15: | `/extract` | LanternExtract | Text extraction interface. Produces Extract Packs from source text. |
L16: | `/dossier/:id` | DossierEditor | CRUD interface for dossier curation (entities, edges, claims, evidence). |
L17: | `/dossier/:id/report` | DossierReport | Read-only report view with heuristic analysis and integrity fingerprints. |
L18: | `/compare` | DossierComparison | Cross-dossier comparison with structural alignment and fingerprint binding. |
L19: | `/reference` | HowItWorks | Reference documentation. Method, limits, safeguards. |
L20: | `/legacy` | Dashboard | Archived finance dashboard (not part of Lantern v1 workflow). |
L21: 
L22: ### Major Modules
L23: 
L24: | Module | Location | Responsibility |
L25: |--------|----------|----------------|
L26: | `lanternExtract.ts` | `client/src/lib/` | Deterministic text extraction engine. Produces entities, quotes, metrics, timeline. |
L27: | `storage.ts` | `client/src/lib/` | IndexedDB persistence layer. Pack storage, retrieval, type guards. |
L28: | `migrations.ts` | `client/src/lib/` | Schema migration (v1 → v2). Field transformations, migration logging. |
L29: | `integrity.ts` | `client/src/lib/` | SHA-256 fingerprint generation for reports and comparisons. |
L30: | `comparison.ts` | `client/src/lib/` | Cross-dossier structural comparison. Jaccard index, entity matching. |
L31: | `heuristics/` | `client/src/lib/heuristics/` | `influenceHubs.ts`, `fundingGravity.ts`, `enforcementMap.ts`, `sensitivity.ts` |
L32: | `pack_v1.ts` | `client/src/lib/schema/` | Zod schema definitions for Pack v2, entities, edges, claims, evidence. |
L33: 
L34: ### Data Flow
L35: 
L36: ```
L37: Source Text → Extract → Extract Pack → Promote → Dossier Pack → Edit → Analysis → Report → Export
L38:                                                     ↓
L39:                                                Compare (2 packs)
L40: ```
L41: 
L42: ---
L43: 
L44: ## B. Data Lifecycle
L45: 
L46: ### Extract → Dossier → Analysis → Report → Export
L47: 
L48: 1. **Extract**: User pastes text → `lanternExtract.ts` extracts structured items → Extract Pack created
L49: 2. **Promote**: User promotes Extract Pack → Dossier Pack scaffolded from extracted entities
L50: 3. **Edit**: User curates dossier (add/remove entities, edges, claims, evidence)
L51: 4. **Analysis**: Report page runs heuristics on dossier data
L52: 5. **Report**: Structured report generated with findings, limits, fingerprint
L53: 6. **Export**: Markdown export with YAML frontmatter, suitable for version control
L54: 
L55: ### Where Migrations Occur
L56: 
L57: - **On Load**: `storage.loadLibrary()` runs `migratePack()` on all dossier packs
L58: - **Trigger**: Any pack with `schemaVersion < 2` or missing v2 fields
L59: - **Log**: All transformations recorded in `pack.migrationLog[]`
L60: 
L61: ### Where Integrity Checks Occur
L62: 
L63: - **Report Generation**: SHA-256 fingerprint computed from canonical pack data
L64: - **Comparison**: Both pack fingerprints plus comparison fingerprint generated
L65: - **Export**: Fingerprints embedded in YAML frontmatter
L66: 
L67: ### Where Refusal / Gating Occurs
L68: 
L69: - **Heuristics**: If edge count < threshold, returns `status: "INSUFFICIENT_DATA"`
L70: - **Comparison**: If either pack lacks sufficient data, shows "Analysis Unavailable"
L71: - **Sensitivity**: If removal simulation cannot run, section omitted
L72: 
L73: ---
L74: 
L75: ## C. Schema & Versioning
L76: 
L77: ### Pack Types
L78: 
L79: | Type | Discriminator | Schema |
L80: |------|---------------|--------|
L81: | Extract Pack | `schema: "lantern.extract.pack.v1"` | `LanternPack` type |
L82: | Dossier Pack | `packId` present, no extract schema | `Pack` type (v2) |
L83: 
L84: ### Type Guards
L85: 
L86: ```typescript
L87: // storage.ts
L88: export function isExtractPack(p: AnyPack): p is LanternPack {
L89:   return "schema" in p && p.schema === "lantern.extract.pack.v1";
L90: }
L91: 
L92: export function isDossierPack(p: AnyPack): p is Pack {
L93:   return "packId" in p && !("schema" in p && (p as any).schema === "lantern.extract.pack.v1");
L94: }
L95: ```
L96: 
L97: ### Migration Strategy
L98: 
L99: | v1 Field | v2 Field | Transformation |
L100: |----------|----------|----------------|
L101: | `createdAt`, `updatedAt` | `timestamps: { created, updated }` | Nested object |
L102: | `title` | `subjectName` | Rename |
L103: | (missing) | `packType` | Default to `"public_figure"` |
L104: | `sourceId`, `targetId` (edges) | `fromEntityId`, `toEntityId` | Rename |
L105: | Unknown edge types | `"affiliated_with"` | Remap with notes |
L106: 
L107: ### Backward Compatibility
L108: 
L109: - `isDossierPack` detects both v1 and v2 packs by checking for `packId` presence
L110: - Migration runs automatically on load, transparent to user
L111: - Original values preserved in notes/migrationLog for audit
L112: 
L113: ---
L114: 
L115: ## D. Heuristic Pipeline
L116: 
L117: ### Influence Hubs
L118: 
L119: | Property | Value |
L120: |----------|-------|
L121: | **Inputs** | All edges in dossier |
L122: | **Threshold** | Minimum 3 edges total |
L123: | **Computation** | Degree centrality (count of edges per entity) |
L124: | **Output** | Ranked list of entities by connection count |
L125: | **Failure Mode** | `INSUFFICIENT_DATA` if < 3 edges |
L126: 
L127: ### Funding Gravity
L128: 
L129: | Property | Value |
L130: |----------|-------|
L131: | **Inputs** | Edges of type: `funded_by`, `donated_to`, `donated_by`, `sponsored_by`, `grant_from`, `grant_to` |
L132: | **Threshold** | Minimum 2 funding edges |
L133: | **Computation** | Count inflows/outflows per entity |
L134: | **Output** | Funding concentration map |
L135: | **Failure Mode** | `INSUFFICIENT_DATA` if < 2 funding edges |
L136: 
L137: ### Enforcement Map
L138: 
L139: | Property | Value |
L140: |----------|-------|
L141: | **Inputs** | Edges of type: `censored_by`, `banned_by`, `sued_by`, `threatened_by`, `fired_by`, `investigated_by`, `sanctioned_by` |
L142: | **Threshold** | Minimum 1 enforcement edge |
L143: | **Computation** | List of coercive relationships |
L144: | **Output** | Enforcement edge inventory |
L145: | **Failure Mode** | `INSUFFICIENT_DATA` if no enforcement edges |
L146: 
L147: ### Sensitivity / Robustness
L148: 
L149: | Property | Value |
L150: |----------|-------|
L151: | **Inputs** | All heuristic outputs |
L152: | **Threshold** | At least 2 data points for meaningful test |
L153: | **Computation** | Simulate removal of each entity/edge, check if findings persist |
L154: | **Output** | Stability classification: ROBUST, FRAGILE, SINGLE_POINT |
L155: | **Failure Mode** | Skipped if insufficient base data |
L156: 
L157: ---
L158: 
L159: ## E. Integrity & Safety Layers
L160: 
L161: ### Report Fingerprinting
L162: 
L163: - **Algorithm**: SHA-256
L164: - **Input**: Canonical JSON of pack data (sorted keys, sorted arrays)
L165: - **Output**: 64-character hex string
L166: - **Location**: Report header, YAML frontmatter in export
L167: 
L168: ### Comparison Fingerprinting
L169: 
L170: - **Fingerprint A**: SHA-256 of Pack A
L171: - **Fingerprint B**: SHA-256 of Pack B
L172: - **Comparison Fingerprint**: SHA-256 of (Fingerprint A + Fingerprint B + comparison timestamp)
L173: - **Purpose**: Tamper-evidence binding two packs at comparison time
L174: 
L175: ### Migration Logs
L176: 
L177: - **Storage**: `pack.migrationLog: string[]`
L178: - **Content**: ISO timestamp + description of transformation
L179: - **Display**: Shown in report under "Migration Notes" section
L180: - **Purpose**: Audit trail for schema changes
L181: 
L182: ### Sensitivity Analysis
L183: 
L184: - **Method**: Remove each entity/edge, rerun heuristics
L185: - **Output**: Which findings survive single-point removal
L186: - **Classification**: 
L187:   - `ROBUST`: Finding survives all removals
L188:   - `FRAGILE`: Finding disappears on some removals
L189:   - `SINGLE_POINT`: Finding depends on exactly one data point
L190: 
L191: ---
L192: 
L193: ## F. UX Boundaries
L194: 
L195: ### What Users Can Do
L196: 
L197: - Create extract packs from source text
L198: - Promote extracts to dossiers
L199: - Add/edit/delete entities, edges, claims, evidence
L200: - Run heuristic analysis (automatic on report view)
L201: - Export reports as Markdown
L202: - Compare two dossiers
L203: - View reference documentation
L204: 
L205: ### What Users Cannot Do
L206: 
L207: - Override insufficiency gating
L208: - Force analysis without meeting thresholds
L209: - Edit fingerprints or migration logs
L210: - Modify heuristic thresholds
L211: - Access production database (local-first only)
L212: 
L213: ### Where Lantern Refuses Action
L214: 
L215: | Scenario | Behavior |
L216: |----------|----------|
L217: | < 3 edges for Influence Hubs | "Insufficient Data" displayed |
L218: | < 2 funding edges for Funding Gravity | "Insufficient Data" displayed |
L219: | No enforcement edges | "No Enforcement Edges Detected" |
L220: | Comparison with insufficient pack | "Analysis Unavailable" for that section |
L221: | Invalid pack schema on import | Pack skipped (binary dedupe policy) |
L222: | Migration validation failure | Error thrown, pack not loaded |
L223: 
L224: ---
L225: 
L226: ## G. File Structure
L227: 
L228: ```
L229: client/src/
L230: ├── App.tsx                    # Router, hamburger menu
L231: ├── pages/
L232: │   ├── library.tsx            # Landing page (/)
L233: │   ├── lantern-extract.tsx    # Text extraction (/extract)
L234: │   ├── dossier-editor.tsx     # Dossier CRUD (/dossier/:id)
L235: │   ├── dossier-report.tsx     # Report view (/dossier/:id/report)
L236: │   ├── dossier-comparison.tsx # Comparison (/compare)
L237: │   ├── how-it-works.tsx       # Reference (/reference)
L238: │   ├── dashboard.tsx          # Legacy (/legacy)
L239: │   └── not-found.tsx          # 404
L240: ├── lib/
L241: │   ├── lanternExtract.ts      # Extraction engine
L242: │   ├── storage.ts             # Persistence + type guards
L243: │   ├── migrations.ts          # v1 → v2 migration
L244: │   ├── integrity.ts           # SHA-256 fingerprinting
L245: │   ├── comparison.ts          # Cross-dossier analysis
L246: │   ├── guardrails.ts          # Validation rules
L247: │   ├── schema/pack_v1.ts      # Zod schemas
L248: │   └── heuristics/
L249: │       ├── influenceHubs.ts   # Influence Hubs
L250: │       ├── fundingGravity.ts  # Funding Gravity
L251: │       ├── enforcementMap.ts  # Enforcement Map
L252: │       ├── sensitivity.ts     # Robustness analysis
L253: │       └── types.ts           # Shared heuristic types
L254: └── components/                # shadcn/ui components
L255: ```
L256: 
L257: ---
L258: 
L259: ## H. Version History
L260: 
L261: | Version | Date | Changes |
L262: |---------|------|---------|
L263: | 1.0.0 | 2026-01-22 | Initial release. M1-M12 complete. |
L264: | Post-v1 | 2026-01-22 | Migration hardening, type guards, route identity fix, reference panel |
L265: 
L266: ---
L267: 
L268: *This map reflects the implementation as of the latest checkpoint.*

--- FILE: UX_GOVERNANCE.md ---
L1: # LANTERN UX GOVERNANCE
L2: 
L3: > **Lantern is an Evidentiary Record System, not an insight engine.**
L4: >
L5: > All UI language must:
L6: > - Describe process, not conclusions
L7: > - Surface constraints, not confidence
L8: > - Preserve traceability over persuasion
L9: >
L10: > The system must never:
L11: > - Assert truth
L12: > - Hide insufficiency
L13: > - Collapse distinction between utterance and fact
L14: > - Produce outputs that cannot survive adversarial review
L15: >
L16: > If a feature cannot be explained as:
L17: > *"What was recorded, under what constraints, with what limits"*
L18: > it does not belong in Lantern.
L19: 
L20: ---
L21: 
L22: **Role Definition**: Lantern is an evidentiary record system, not a productivity app, dashboard, planner, or assistant.
L23: 
L24: Lantern's purpose is **epistemic containment**:
L25: - To record claims
L26: - To link evidence
L27: - To apply constrained heuristics
L28: - To show where evidentiary limits are exceeded
L29: 
L30: Lantern does not determine truth, intent, or guilt.
L31: 
L32: ---
L33: 
L34: ## Core Principles (Non-Negotiable)
L35: 
L36: ### 1. NO NARRATIVE UX
L37: 
L38: Do not introduce:
L39: - Progress metaphors
L40: - "Journeys," "phases," or "paths"
L41: - Success indicators
L42: - Gamification
L43: - Optimization framing
L44: 
L45: Lantern must feel procedural, cold, and forensic, not motivational.
L46: 
L47: ### 2. NO IMPLIED CONCLUSIONS
L48: 
L49: The interface must never imply:
L50: - That a finding is "good," "bad," or "important"
L51: - That higher scores mean correctness
L52: - That absence of data implies innocence or guilt
L53: 
L54: All analysis must be framed as conditional and bounded.
L55: 
L56: Every heuristic output must clearly show:
L57: - Data sufficiency status
L58: - Threshold requirements
L59: - Processed counts
L60: 
L61: ### 3. ABSENCE IS A FIRST-CLASS STATE
L62: 
L63: Empty states are correct behavior, not errors.
L64: 
L65: Prefer:
L66: - "No extract packs yet"
L67: - "Insufficient data for analysis"
L68: - "Comparison unavailable due to density threshold"
L69: 
L70: Do not auto-generate placeholders, charts, or insights.
L71: 
L72: Silence is intentional.
L73: 
L74: ### 4. LANGUAGE MUST BE PROCEDURAL
L75: 
L76: **Approved verbs**: Record, Link, Inspect, Compare, Verify, Export
L77: 
L78: **Disallowed verbs**: Discover, Reveal, Expose, Uncover, Prove, Optimize
L79: 
L80: Lantern documents boundaries, it does not persuade.
L81: 
L82: ### 5. INTEGRITY METADATA IS ALWAYS VISIBLE
L83: 
L84: Never hide or downplay:
L85: - Schema version
L86: - Report fingerprint (SHA-256)
L87: - Migration notes
L88: - Claim scope (utterance vs content)
L89: - Sufficiency gates
L90: 
L91: These are core UI elements, not advanced settings.
L92: 
L93: ### 6. COMPARISON IS FORENSIC, NOT COMPETITIVE
L94: 
L95: The comparison view must feel like:
L96: - A diff
L97: - A cross-examination
L98: - A lab comparison
L99: 
L100: Avoid:
L101: - Scores framed as performance
L102: - Leaderboard language
L103: - "Alignment strength" metaphors
L104: 
L105: If either side lacks sufficient data, show Analysis Unavailable.
L106: 
L107: ### 7. VISUAL DESIGN CONSTRAINTS
L108: 
L109: Design should communicate:
L110: - Restraint
L111: - Auditability
L112: - Reluctance to conclude
L113: 
L114: Avoid:
L115: - Bright success colors
L116: - Emotional color coding
L117: - Trend-implying animations
L118: 
L119: Warning/insufficient states should be neutral, amber, or gray, not alarming.
L120: 
L121: ### 8. FIRST-TIME USER GUIDANCE MUST BE MECHANICAL
L122: 
L123: **Allowed**: "Extract packs contain structured claims derived from source text."
L124: 
L125: **Disallowed**: "Start your investigation by uncovering key insights."
L126: 
L127: No motivational copy. No calls to action that imply urgency or outcome.
L128: 
L129: ### 9. DO NOT MERGE PRODUCT DOMAINS
L130: 
L131: Lantern must not visually or semantically resemble:
L132: - Financial dashboards
L133: - Planning tools
L134: - AI assistants
L135: - Intelligence "engines"
L136: 
L137: If any UI element resembles optimization, refactor or remove it.
L138: 
L139: ---
L140: 
L141: ## Success Criteria
L142: 
L143: A correct Lantern UI:
L144: - Is boring in the way court filings are boring
L145: - Makes misuse difficult
L146: - Makes verification easy
L147: - Refuses to speak where evidence is thin
L148: - Survives hostile scrutiny
L149: 
L150: If a feature increases persuasion more than auditability, it is wrong.
L151: 
L152: ---
L153: 
L154: ## Final Check Before Any Change
L155: 
L156: Before committing any UX change, ask:
L157: 
L158: > "Does this help someone measure overreach — or does it help them tell a story?"
L159: 
L160: If it tells a story, do not ship it.
L161: 
L162: ---
L163: 
L164: ## DO NOT ADD Blacklist
L165: 
L166: - Progress bars / completion percentages
L167: - "Insights" sections
L168: - AI-generated summaries or conclusions
L169: - Success/failure indicators
L170: - Trend arrows or performance graphs
L171: - Motivational copy or calls-to-action
L172: - "Smart" suggestions or recommendations
L173: - Gamification elements (badges, streaks, scores)
L174: - "Journey" or "path" metaphors
L175: - Competitive comparison language

--- FILE: replit.md ---
L1: # Lantern - Investigative Intelligence Platform
L2: 
L3: ## Overview
L4: 
L5: Lantern is a client-side investigative journalism intelligence platform that enables analysts to:
L6: 1. Extract structured data from unstructured text (entities, quotes, metrics, timeline events)
L7: 2. Curate dossiers with entities, edges, claims, and evidence
L8: 3. Apply "Shadow-Caste" heuristics to detect structural patterns (influence hubs, funding flows, enforcement actions)
L9: 4. Generate publication-ready reports with cryptographic integrity fingerprints
L10: 
L11: The application is a **client-heavy hybrid** architecture with server-side durability for large document extraction. Data persists to IndexedDB locally, with server-side PostgreSQL job queue for documents exceeding 75K characters.
L12: 
L13: ### Server-Side Job Queue (Institutional-Grade Durability)
L14: - **Threshold-Based Routing**: Documents <75K chars use browser Web Worker; ≥75K use server job queue
L15: - **PostgreSQL Persistence**: Jobs survive page refresh and server restarts
L16: - **Automatic Recovery**: localStorage stores job_id for reconnection on page load with 2-second polling
L17: - **Stall Detection**: 30-second client-side watchdog with amber warning UI; server has 5-minute timeout
L18: - **Job States**: `pending`, `processing`, `completed`, `failed`, `cancelled`
L19: 
L20: ### Verified Record (Canonical Output Artifact)
L21: The **Verified Record** is the single deterministic, printable output artifact for every corpus run:
L22: - **Schema**: `lantern.verified_record.v1`
L23: - **Contents**:
L24:   - All input sources with SHA-256 hashes
L25:   - All supported claims (DEFENSIBLE) with exact source anchors
L26:   - All restricted claims (REFUSED) with refusal reasons
L27:   - All ambiguous claims
L28:   - Conflicts, missing evidence, and time mismatches
L29:   - Integrity metadata (SHA-256 content hash, timestamp, schema version)
L30: - **Serialization**: JSON (canonical) and text-based PDF export
L31: - **Determinism**: All arrays sorted by ID, canonical JSON serialization with sorted keys
L32: - **API Endpoints**:
L33:   - `GET /api/corpus/:corpusId/verified-record` - JSON export
L34:   - `GET /api/corpus/:corpusId/verified-record.pdf` - Text-based printable export
L35: - **Purpose**: Courts, regulators, executives, and audits
L36: 
L37: ## User Preferences
L38: 
L39: Preferred communication style: Simple, everyday language.
L40: 
L41: ## System Architecture
L42: 
L43: ### Runtime Architecture
L44: - **Frontend**: React + Vite + TypeScript single-page application
L45: - **Storage**: IndexedDB via `idb` library (local-first architecture)
L46: - **Server**: Express server with extraction job queue API endpoints
L47: - **Database**: PostgreSQL for durable job queue persistence (extraction_jobs table)
L48: - **Web Worker**: Browser-side extraction for documents <75K characters
L49: 
L50: ### Core Data Model
L51: Two discriminated pack types coexist in the library:
L52: - **Extract Packs** (`schema: "lantern.extract.pack.v1"`): Machine-extracted, provenance-heavy input artifacts
L53: - **Dossier Packs** (`schemaVersion: 2`): Curated, claim-bearing output artifacts with entities, edges, evidence, and claims
L54: 
L55: ### Key Modules
L56: 
L57: **Extraction Engine** (`client/src/lib/lanternExtract.ts`)
L58: - Deterministic, rule-based text extraction
L59: - Produces entities, quotes, metrics, and timeline events
L60: - Enforces provenance validation: "No offset, no item"
L61: - Stable IDs via SHA-256 hashing of content + offsets
L62: - **Sanitation Pass** (v0.1.6): Entity denylist, type classification, confidence scoring
L63: 
L64: **Entity Sanitizer** (`client/src/lib/heuristics/entities/entitySanitizer.ts`)
L65: - 120+ word denylist blocking stopwords, temporal phrases, citation scaffolding
L66: - Entity type classification: Person, Organization, Location, Event, Product
L67: - Confidence scoring per entity: span_length + ontology_match + canonical_frequency
L68: - Canonical collapse: deduplicates exact position matches
L69: - Reclassification: Temporal phrases → Timeline events
L70: 
L71: **Trust Contract** (Pack Metadata)
L72: - `schema_version`: Pack schema identifier
L73: - `confidence_model`: Scoring formula used
L74: - `sanitation_pass`: Boolean indicating sanitation applied
L75: - `pack_confidence`: Aggregate confidence score (0-1)
L76: - `confidence_threshold`: Minimum confidence for inclusion (default 0.5)
L77: 
L78: **Heuristics System** (`client/src/lib/heuristics/`)
L79: - Influence Hubs: Degree centrality analysis
L80: - Funding Gravity: Monetary flow and concentration detection
L81: - Enforcement Map: Coercive edge detection (censorship, bans, lawsuits)
L82: - All heuristics include evidence density thresholds and sufficiency gating
L83: 
L84: **Persistence Layer** (`client/src/lib/storage.ts`)
L85: - Debounced saves to prevent write storms
L86: - Schema versioning with migration support
L87: - Export/Import with lossless round-trip guarantee
L88: - Binary dedupe policy: on pack_id collision, SKIP (no field-level merging)
L89: 
L90: **Report Generation** (`client/src/pages/dossier-report.tsx`)
L91: - Structured reports with cryptographic fingerprints (SHA-256)
L92: - Markdown export with YAML frontmatter
L93: - Interpretation limits and disclaimers for epistemic safety
L94: - Print-optimized CSS
L95: 
L96: ### Hard Invariants (Non-Negotiable)
L97: 1. **Determinism**: Same input + options = identical output JSON
L98: 2. **No Offset, No Item**: Missing/invalid offsets cause silent discard
L99: 3. **Document-Absolute Offsets**: All offsets reference original source indices
L100: 4. **Stable IDs**: Hash-based, never random UUIDs
L101: 5. **Binary Import Policy**: Pack collision = SKIP, no partial merging
L102: 6. **No Guessing**: Ambiguous data marked UNRESOLVED, not inferred
L103: 
L104: ### File Structure
L105: ```
L106: client/src/
L107: ├── pages/
L108: │   ├── lantern-extract.tsx    # Main extraction UI with pagination (100 items/page)
L109: │   ├── dossier-editor.tsx     # CRUD for dossier curation
L110: │   ├── dossier-report.tsx     # Publication-ready reports
L111: │   └── dossier-comparison.tsx # Cross-dossier analysis
L112: ├── lib/
L113: │   ├── lanternExtract.ts      # Core extraction engine
L114: │   ├── storage.ts             # IndexedDB persistence layer
L115: │   ├── heuristics/            # Analysis algorithms
L116: │   └── schema/pack_v1.ts      # Dossier schema (v2)
L117: ├── workers/
L118: │   └── extraction.worker.ts   # Web Worker for browser-side extraction
L119: server/
L120: ├── index.ts                   # Express entrypoint
L121: ├── routes.ts                  # API endpoints for job queue and file upload
L122: ├── extractionProcessor.ts     # Server-side extraction job processor
L123: └── storage.ts                 # MemStorage adapter for job persistence
L124: shared/
L125: └── schema.ts                  # Drizzle schema for extraction_jobs table
L126: ```
L127: 
L128: ## External Dependencies
L129: 
L130: ### UI Framework
L131: - **React 18** with React Router for SPA routing
L132: - **Radix UI** primitives for accessible components
L133: - **Tailwind CSS** via @tailwindcss/vite plugin
L134: - **shadcn/ui** component library (New York style)
L135: 
L136: ### Build & Development
L137: - **Vite** for development server and production builds
L138: - **TypeScript** with strict mode enabled
L139: - **tsx** for server-side TypeScript execution
L140: 
L141: ### Data Validation
L142: - **Zod** for runtime schema validation
L143: - **drizzle-zod** for database schema integration (scaffold only)
L144: 
L145: ### Database (Active - Extraction Job Queue)
L146: - **Drizzle ORM** configured for PostgreSQL
L147: - **extraction_jobs** table: Durable job queue with states (pending, processing, completed, failed, cancelled)
L148: - **connect-pg-simple** for session storage
L149: 
L150: ### Charting & Visualization
L151: - **Recharts** for data visualization (referenced in attached assets)
L152: - **Embla Carousel** for UI carousels
L153: 
L154: ### Crypto
L155: - **Web Crypto API** (`crypto.subtle`) for SHA-256 fingerprinting
L156: - Used for pack identity, report integrity, and comparison fingerprints

--- FILE: client/src/lib/EXTRACTION_ENGINE.md ---
L1: # Lantern Extraction Engine (v0.1.5)
L2: 
L3: ## Architecture Summary
L4: The Lantern Extraction Engine is a deterministic, heuristic-based NLP pipeline designed for high-integrity knowledge extraction from raw text. It operates purely on the frontend (client-side) to ensure privacy and speed.
L5: 
L6: ### Core Components
L7: 1.  **Sentence Segmenter**: Splits text into addressable spans with offset tracking.
L8: 2.  **Entity Extractor**: Regex-based recognition of Organizations, Persons, and Locations using capitalization and suffix heuristics.
L9: 3.  **Quote Extractor**: Pattern matching for direct speech with "Forward Scan" attribution logic (bounded to 1 sentence).
L10: 4.  **Metric Extractor**: Context-aware parsing of Scalars, Ranges, Ratios, and Rates, normalizing units and values.
L11: 5.  **Timeline Extractor**: Explicit date parsing and relative date resolution (partial).
L12: 6.  **Canonicalizer**: Deterministic identity generation for deduplication and persistence.
L13: 
L14: ## Invariants
L15: 1.  **Provenance Integrity**: Every extracted item MUST map to a character span in the source text (`text.slice(start, end) === item.text`). Items failing this are dropped.
L16: 2.  **Determinism**: Given the same input text and mode, the engine MUST produce bit-for-bit identical output JSON.
L17: 3.  **Stable Identity**:
L18:     *   `stable_source_hash`: SHA256 of raw source text.
L19:     *   `pack_id`: SHA256 of the canonicalized artifact (Source + Engine + Items + Curation).
L20: 
L21: ## Extraction Modes
L22: *   **Conservative**: High precision, lower recall. Requires strict validation (e.g., attribution verbs, explicit metric units).
L23: *   **Balanced**: Default mode. Standard heuristics.
L24: *   **Broad**: High recall, lower precision. Loosens constraints (e.g., allows capitalized phrases without known suffixes as entities).
L25: 
L26: ## Testing & Quality
L27: Run the **Quality Dashboard** in the UI to verify:
L28: *   Precision/Recall/F1 against golden fixtures.
L29: *   Cross-mode monotonicity (Conservative should not be noisier than Broad).
L30: *   Determinism checks.

--- FILE: client/src/lib/LIBRARY_STORAGE.md ---
L1: # Lantern Library Storage Schema
L2: 
L3: ## Storage Mechanism
L4: Lantern uses `localStorage` for the v0.1.5 prototype.
L5: 
L6: **Key**: `lantern_packs`
L7: **Value**: JSON Array of `LanternPack` objects.
L8: 
L9: ## Key Schema
L10: Each `LanternPack` contains:
L11: *   `pack_id`: Unique identifier (SHA256 of canonical content).
L12: *   `hashes`:
L13:     *   `source_text_sha256`: ID of the input text.
L14:     *   `pack_sha256`: Redundant ID check.
L15: *   `source`: Metadata (Title, Author, URL, Retrieved At).
L16: *   `engine`: Version and Name.
L17: *   `items`: Arrays of Entities, Quotes, Metrics, Timeline.
L18: *   `stats`: Execution statistics.
L19: 
L20: ## Snapshot Semantics
L21: *   **Immutable Snapshots**: A pack is defined by its content. If you change curation (toggle an item include/exclude), the content changes, resulting in a **new `pack_id`**.
L22: *   **No Overwrites**: Saving a pack with a new ID appends it to the library. Saving a pack with an existing ID is a no-op (idempotent).
L23: *   **Source Grouping**: Packs are grouped in the UI by `stable_source_hash` to show the history of extractions for a single document.
L24: 
L25: ## Canonicalization Exclusions
L26: To ensure stable hashing, the following are **excluded** from the `pack_id` calculation:
L27: *   UI-only state (e.g., `showDetails` flags).
L28: *   Runtime timestamps (except extraction timestamp if part of metadata).
L29: *   Order of keys in JSON objects (strictly sorted before hashing).
L30: *   Order of items in arrays (strictly sorted by Item ID before hashing).

--- FILE: client/src/lib/PHASE_3_BACKLOG.md ---
L1: # Phase 3 Backlog (Lantern)
L2: 
L3: The following features are explicitly **out of scope** for the v0.1.5 Baseline Freeze. They are parked here for future implementation.
L4: 
L5: ## Persistence & Backend
L6: *   [ ] **SQLite/Postgres Integration**: Migrate from `localStorage` to a real backend database.
L7: *   [ ] **API Layer**: REST endpoints for saving/loading packs.
L8: *   [ ] **Multi-User Support**: User accounts and shared libraries.
L9: 
L10: ## Advanced Extraction
L11: *   [ ] **Shadow NLP Engine**: Integrate LLM (Gemini/GPT) as a shadow extractor to propose items missed by heuristics.
L12: *   [ ] **Advanced Disambiguation**: Cross-document entity linking and resolution.
L13: *   [ ] **Graph Mapping**: Relationship extraction between entities (Subject-Verb-Object).
L14: 
L15: ## Workflow Tools
L16: *   [ ] **Multi-Source Corpora**: Upload and process multiple documents at once.
L17: *   [ ] **Pack Merging**: Tools to merge two different extractions of the same source.
L18: *   [ ] **Batch Exports**: Export multiple packs to CSV/JSON-L.
L19: *   [ ] **Citation Generator**: Auto-generate citations from extraction provenance.

--- FILE: client/src/lib/QUALITY_CONTRACT.md ---
L1: # Lantern Extract: Quality Contract (v1.0)
L2: 
L3: This document defines the strict rules for scoring extraction quality and calculating diffs. Any changes to these rules must be documented here to prevent metric drift.
L4: 
L5: ## 1. Item Identity (Canonical Keys)
L6: 
L7: All diffs and quality scores must match items based on these structural keys, **never** on display strings or array indices.
L8: 
L9: | Stream    | Canonical Key Structure                                      | Notes                                                                 |
L10: |-----------|--------------------------------------------------------------|-----------------------------------------------------------------------|
L11: | **Entity**| `entity|<type>|<normalized_text>`                            | Normalized text ignores case and minor spacing.                       |
L12: | **Quote** | `quote|<normalized_text>|<speaker_or_null>`                 | Speaker is part of identity. If speaker changes, it is a Diff Change. |
L13: | **Metric**| `metric|<kind>|<unit>|<normalized_value>`                    | Unit and Kind are strict. Value allows constrained fuzziness.         |
L14: | **Timeline**| `time|<date_type>|<normalized_date>`                       | Date type (explicit/relative) is strict.                              |
L15: 
L16: ## 2. Constrained Fuzziness
L17: 
L18: Fuzzy matching is **only** permitted for Metric values and Date strings.
L19: 
L20: ### Permitted Normalizations
L21: - **Whitespace**: Multiple spaces collapse to single space (`5  M` -> `5 M`).
L22: - **Dashes**: Unicode dashes normalize to hyphen (`–` -> `-`).
L23: - **Separators**: Commas in numbers are stripped (`1,000` -> `1000`).
L24: - **Casing**: Content matching is case-insensitive (but Casing features are preserved in extraction).
L25: 
L26: ### Forbidden Normalizations (Strict Fail)
L27: - **Units**: `USD` != `EUR`, `%` != `null`.
L28: - **Metric Kind**: `range` != `scalar`.
L29: - **Rate Denominators**: `per 100k` != `per 10k`.
L30: - **Rounding**: `5.1` != `5`.
L31: 
L32: ## 3. Diff Classification
L33: 
L34: When comparing two packs (Base vs Current):
L35: 
L36: - **Added**: Item Key exists in Current but not Base.
L37: - **Removed**: Item Key exists in Base but not Current.
L38: - **Changed**: Item Key exists in BOTH, but secondary attributes differ.
L39:   - *Secondary Attributes*: Provenance spans, confidence scores, original raw text (if normalized key is same).
L40: 
L41: ## 4. Cross-Mode Validation Rules
L42: 
L43: The engine must satisfy these monotonicity checks per stream. Violations are flagged as "Suspicious".
L44: 
L45: 1. **Recall Monotonicity**: `Recall(Conservative) <= Recall(Balanced) <= Recall(Broad)`
L46:    - Conservative mode should not find *more* items than Broad mode.
L47: 2. **Precision Monotonicity**: `Precision(Conservative) >= Precision(Balanced) >= Precision(Broad)`
L48:    - Conservative mode should be *more accurate* than Broad mode.
L49: 
L50: ## 5. Scoring Metrics
L51: 
L52: - **Precision**: `Valid_Actuals / Total_Actuals`
L53: - **Recall**: `Matches / Total_Expected`
L54: - **F1**: Harmonic mean of Precision and Recall.
L55: 
L56: ---
L57: *Last Updated: Lantern v0.1.5*

--- FILE: demos/evidence-walkthrough/README.md ---
L1: # Evidence Walkthrough Demo (Exhibit, Not Feature)
L2: 
L3: This demo is an **exhibit**: a fixed evidence artifact and a disciplined walkthrough of what can and cannot be concluded from it.
L4: 
L5: **This demo does not assert truth.**  
L6: It does not infer intent, authorship, legitimacy, or safety.  
L7: It shows how reasoning changes under different interpretive lenses *given a verified artifact*.
L8: 
L9: ---
L10: 
L11: ## What this is
L12: 
L13: - A minimal evidence object (`evidence.json`) shaped like a HALO receipt.
L14: - A structured interpretation (`interpretation.md`) that separates:
L15:   - **What is known**
L16:   - **What is not known**
L17:   - **What would require assumptions**
L18: - A "lens discipline" that prevents semantic drift.
L19: 
L20: ---
L21: 
L22: ## What this is NOT
L23: 
L24: This demo does **not**:
L25: - claim the content is true, accurate, ethical, or safe
L26: - identify a real-world person or organization
L27: - prove authorship or ownership
L28: - prove legitimacy of the signer's actions
L29: - provide a trusted timestamp
L30: - provide a confidence score or "AI judgment"
L31: 
L32: ---
L33: 
L34: ## How to use this demo
L35: 
L36: ### Step 1 — Treat `evidence.json` as an *exhibit*
L37: Read it as you would a logged artifact:
L38: - it is a structured claim that **some bytes existed**
L39: - and that (in a real pipeline) those bytes would be **tamper-evident once signed**
L40: 
L41: ### Step 2 — (Optional) Swap in a real HALO receipt
L42: If you want this exhibit to be **cryptographically verifiable**, replace `evidence.json` with an actual receipt produced by HALO-RECEIPTS, and verify it there.
L43: 
L44: This demo intentionally keeps Lantern out of cryptographic verification. Lantern is an interpretive framework.
L45: 
L46: Suggested workflow:
L47: 
L48: 1. In HALO-RECEIPTS, sign + verify a receipt:
L49:    - produces `*.payload` and `*.payload.sig`
L50: 2. Copy the receipt JSON into this folder as `evidence.json`
L51: 3. Keep verification outputs out of Lantern repo unless explicitly needed
L52: 
L53: ### Step 3 — Read `interpretation.md`
L54: That file is the "Lantern move":
L55: - it refuses meaning inflation
L56: - it documents inference boundaries
L57: - it compares lenses without changing the evidence
L58: 
L59: ---
L60: 
L61: ## Why this exists
L62: 
L63: Most "AI demos" try to impress.
L64: 
L65: Lantern demos try to **withstand cross-examination**.
L66: 
L67: The goal is not persuasion.  
L68: The goal is **stable reasoning under constraints**.

--- FILE: demos/evidence-walkthrough/interpretation.md ---
L1: # Interpretation Walkthrough (Disciplined Reasoning)
L2: 
L3: This document demonstrates Lantern's core stance:
L4: 
L5: **Evidence is fixed. Interpretation is external.**
L6: 
L7: The exhibit in `evidence.json` is treated as an artifact whose meaning must not be inflated.
L8: 
L9: ---
L10: 
L11: ## 0) Exhibit Snapshot (What we are allowed to look at)
L12: 
L13: From `evidence.json`, we can read these *declared* fields:
L14: 
L15: - Receipt schema + canonicalization version
L16: - A receipt identifier (`receipt_id`)
L17: - A creation time field (`created_at`) *(informational only)*
L18: - A subject summary:
L19:   - artifact type/name
L20:   - byte length
L21:   - sha256 digest
L22: - A signing header:
L23:   - namespace
L24:   - signer identifier (string label)
L25:   - public key hint (placeholder)
L26: 
L27: ---
L28: 
L29: ## 1) What is known (strict)
L30: 
L31: **K1 — A structured exhibit exists.**  
L32: We have a JSON object conforming to an intended receipt-shaped schema.
L33: 
L34: **K2 — The exhibit claims a subject digest and size.**  
L35: It states a `sha256` and `byte_length` for `demo_memo.txt`.
L36: 
L37: **K3 — The exhibit declares a signing namespace and signer label.**  
L38: It contains labels indicating *how it would be signed* in a real pipeline.
L39: 
L40: That is the end of the "known" list **unless** cryptographic verification is performed elsewhere.
L41: 
L42: ---
L43: 
L44: ## 2) What is not known (even if it feels "obvious")
L45: 
L46: **U1 — Whether the content exists.**  
L47: A receipt-shaped object can describe non-existent files.
L48: 
L49: **U2 — Whether the hash matches any real bytes.**  
L50: Without the actual bytes and verification workflow, the digest is just text.
L51: 
L52: **U3 — Whether any signature is valid.**  
L53: This demo does not include `payload` + `payload.sig` or a verifiable public key.
L54: 
L55: **U4 — Whether `created_at` is meaningful.**  
L56: It is not a trusted timestamp; it can be forged.
L57: 
L58: **U5 — Whether "demo-signer" corresponds to a real entity.**  
L59: Labels do not establish identity.
L60: 
L61: ---
L62: 
L63: ## 3) What would require assumptions (explicitly named)
L64: 
L65: If someone concludes more than the "known" list above, they are importing assumptions.
L66: 
L67: Common assumption injections:
L68: 
L69: **A1 — "The bytes existed."**  
L70: Requires: the underlying bytes are produced, hashed, and verified against the digest.
L71: 
L72: **A2 — "The signer is a real actor."**  
L73: Requires: identity binding beyond key possession (organizational identity, HR/role binding, etc.).
L74: 
L75: **A3 — "The act was legitimate."**  
L76: Requires: governance proof (authority, policy compliance, approvals).
L77: 
L78: **A4 — "The content is true."**  
L79: Requires: domain validation external to cryptography.
L80: 
L81: ---
L82: 
L83: ## 4) Lens Views (same evidence, different questions)
L84: 
L85: ### 4.1 Security Lens (cryptographic integrity)
L86: - Question: "Do these bytes match a signed payload under an allowed key?"
L87: - For this demo exhibit: **cannot be answered** until a real signature is verified.
L88: 
L89: ### 4.2 Governance Lens (authority & process)
L90: - Question: "Was signing authorized under policy, by the right role, for the right purpose?"
L91: - For this demo exhibit: **cannot be answered** without governance artifacts (approvals, policy, key custody rules).
L92: 
L93: ### 4.3 Evidentiary / Legal Lens (admissibility, chain-of-custody)
L94: - Question: "Can this be authenticated, preserved, and explained without gaps?"
L95: - For this demo exhibit: **weak** as-is (no signature chain), but demonstrates the *structure of reasoning*.
L96: 
L97: ### 4.4 Operations Lens (audit and incident workflow)
L98: - Question: "Can this support triage, audits, or post-mortems?"
L99: - For this demo exhibit: usable as a **template**, not as a verified artifact.
L100: 
L101: ---
L102: 
L103: ## 5) The Lantern Conclusion (what survives cross-examination)
L104: 
L105: This exhibit supports **structure**, not certainty.
L106: 
L107: It demonstrates:
L108: - disciplined separation of evidence vs interpretation
L109: - explicit naming of unknowns
L110: - explicit naming of assumption injection points
L111: 
L112: Lantern's credibility comes from the refusal to "complete the story."

--- FILE: docs/GOVERNANCE_AUDIT_2026_01_23.md ---
L1: # Lantern/Nikodemus Governance Audit
L2: 
L3: **Generated**: 2026-01-23  
L4: **Updated**: 2026-01-23  
L5: **Branch**: main
L6: 
L7: ---
L8: 
L9: ## Deliverable 0: Snapshot & Change Audit
L10: 
L11: ### Repo Snapshot Summary
L12: 
L13: | Component | Value |
L14: |-----------|-------|
L15: | **Framework** | React 19 + Vite + TypeScript (frontend), Express 5 (backend) |
L16: | **Runtime** | Node.js (tsx dev, node prod) |
L17: | **Database** | PostgreSQL (cases/uploads/chunks), IndexedDB (packs) |
L18: | **Storage** | `server/storage.ts` — Drizzle ORM, `client/src/lib/storage.ts` — IndexedDB |
L19: | **Server Entrypoint** | `server/index.ts` |
L20: | **Client Entrypoint** | `client/src/main.tsx` |
L21: 
L22: ### Key Directories
L23: 
L24: ```
L25: client/src/
L26: ├── pages/              # 10 routes (Library, Cases, Extract, Editor, Report, etc.)
L27: ├── lib/                # Core logic (extraction, heuristics, storage, schema, llm)
L28: │   ├── heuristics/     # Analysis algorithms (influence, funding, enforcement)
L29: │   ├── llm/            # LLM call contract with governance gating
L30: │   └── schema/         # Pack schema v2
L31: ├── components/         # UI primitives (shadcn/ui + custom)
L32: │   └── UploadDrawer.tsx  # Case-bound file/photo/scan upload
L33: server/
L34: ├── index.ts            # Express entrypoint
L35: ├── routes.ts           # Case and upload API endpoints
L36: ├── storage.ts          # Drizzle ORM database operations
L37: docs/
L38: ├── investor/           # Pitch materials (2 lanes)
L39: ├── snapshots/          # Safe snapshot system
L40: shared/
L41: ├── schema.ts           # PostgreSQL schema (cases, uploads, chunks, etc.)
L42: ```
L43: 
L44: ---
L45: 
L46: ## Deliverable 1: Current-State Architecture Map
L47: 
L48: ### Frontend Routes
L49: 
L50: | Route | Component | Purpose |
L51: |-------|-----------|---------|
L52: | `/` | Library | Pack management, import/export |
L53: | `/cases` | Cases | Case CRUD, upload management |
L54: | `/extract` | LanternExtract | Text extraction UI |
L55: | `/dossier/:id` | DossierEditor | Dossier CRUD |
L56: | `/dossier/:id/report` | DossierReport | Publication-ready reports |
L57: | `/compare` | DossierComparison | Cross-dossier analysis |
L58: | `/reference` | HowItWorks | Methodology docs |
L59: | `/legacy` | Dashboard | (Legacy, unused) |
L60: | `/legacy/core` | LanternCore | (Legacy, unused) |
L61: 
L62: ### Backend Endpoints
L63: 
L64: | Method | Path | Purpose |
L65: |--------|------|---------|
L66: | POST | `/api/cases` | Create new case |
L67: | GET | `/api/cases` | List all active cases |
L68: | GET | `/api/cases/:caseId` | Get case details |
L69: | PATCH | `/api/cases/:caseId` | Update case |
L70: | POST | `/api/cases/:caseId/uploads/init` | Initialize upload |
L71: | POST | `/api/cases/:caseId/uploads/complete` | Complete upload |
L72: | GET | `/api/cases/:caseId/uploads` | List uploads for case |
L73: | GET | `/api/cases/:caseId/uploads/:uploadId` | Get upload detail |
L74: 
L75: ### Database Schema (PostgreSQL via Drizzle)
L76: 
L77: ```typescript
L78: cases: { id, name, status, decisionTarget, decisionTime, createdAt, updatedAt, deletedAt }
L79: uploads: { id, caseId(FK), filename, mimeType, evidenceType, sha256, ingestionState, storagePath, fileSize, pageCount, createdAt, updatedAt, deletedAt }
L80: upload_pages: { id, uploadId(FK), pageNumber, storagePath, sha256, createdAt, deletedAt }
L81: chunks: { id, caseId(FK), uploadId(FK), pageNumber, chunkIndex, content, embedding, createdAt, deletedAt }
L82: users: { id, username, password }
L83: ```
L84: 
L85: ### Governance Constraints Implemented
L86: 
L87: | Constraint | Implementation |
L88: |------------|----------------|
L89: | **Soft Delete** | `deletedAt` column on cases, uploads, upload_pages, chunks |
L90: | **FK Cascade** | `onDelete: cascade` on uploads.caseId, chunks.caseId, chunks.uploadId, upload_pages.uploadId |
L91: | **Query Filtering** | All read queries filter `isNull(deletedAt)` |
L92: | **Sealed Protection** | Upload routes reject operations on sealed cases |
L93: | **Case Binding** | All uploads require valid caseId |
L94: 
L95: ### LLM Pipeline
L96: 
L97: **Status: IMPLEMENTED (Contract Only)**
L98: 
L99: The LLM call contract is implemented in `client/src/lib/llm/contract.ts`:
L100: 
L101: - **Context Validation**: `validateLLMContext()` enforces required caseId
L102: - **Fail-Closed Gating**: Returns `CONTEXT_REQUIRED` response when missing required fields
L103: - **System Prompt**: Governance-aware prompts with evidence citation requirements
L104: - **Response Types**: SUCCESS, CONTEXT_REQUIRED, REFUSAL, ERROR
L105: 
L106: ```typescript
L107: // LLM call requires case binding
L108: const response = await executeLLMCall({
L109:   caseId: "abc-123",
L110:   decisionTarget: "Determine regulatory violation",
L111:   decisionTime: "2026-01-23T12:00:00Z",
L112:   evidenceIds: ["ev-1", "ev-2"]
L113: }, prompt);
L114: 
L115: // Without caseId, returns:
L116: {
L117:   type: "CONTEXT_REQUIRED",
L118:   missing_fields: ["caseId"],
L119:   next_actions: ["Select or create a case before proceeding"]
L120: }
L121: ```
L122: 
L123: ### Extraction Pipeline (Local, Deterministic)
L124: 
L125: ```
L126: Source Text
L127:     ↓
L128: lanternExtract.ts (rule-based NLP)
L129:     ↓
L130: LanternPack { entities, quotes, metrics, timeline }
L131:     ↓
L132: extract_to_dossier.ts (converter)
L133:     ↓
L134: Pack (Dossier schema v2)
L135:     ↓
L136: storage.ts (IndexedDB persistence)
L137: ```
L138: 
L139: ---
L140: 
L141: ## Deliverable 2: Issues Resolved
L142: 
L143: | Issue | Resolution |
L144: |-------|------------|
L145: | ~~No case infrastructure~~ | **IMPLEMENTED**: `cases` table with full CRUD API |
L146: | ~~No upload API~~ | **IMPLEMENTED**: `/api/cases/:caseId/uploads/*` endpoints |
L147: | ~~Missing decision fields~~ | **IMPLEMENTED**: `decisionTarget`, `decisionTime` in cases table |
L148: | ~~No ingestion states~~ | **IMPLEMENTED**: State machine (uploaded → stored → extracted → chunked → indexed → ready) |
L149: | ~~No LLM gating~~ | **IMPLEMENTED**: LLM contract with CONTEXT_REQUIRED responses |
L150: 
L151: ### Remaining Issues
L152: 
L153: | Severity | Issue | Status |
L154: |----------|-------|--------|
L155: | **MEDIUM** | Legacy routes visible | `/legacy` accessible but unused |
L156: | **LOW** | Boot probe logging | BOOT-PROBE console.log in main.tsx |
L157: 
L158: ---
L159: 
L160: ## Deliverable 3: Field Audit (Completed)
L161: 
L162: ### Case Build Completeness
L163: 
L164: | Field | Status | Location |
L165: |-------|--------|----------|
L166: | `caseId` | **IMPLEMENTED** | `cases.id` |
L167: | `decisionTarget` | **IMPLEMENTED** | `cases.decisionTarget` |
L168: | `decisionTime` | **IMPLEMENTED** | `cases.decisionTime` |
L169: | Case status | **IMPLEMENTED** | `cases.status` (active/sealed/archived) |
L170: | Evidence type | **IMPLEMENTED** | `uploads.evidenceType` |
L171: | Ingestion state | **IMPLEMENTED** | `uploads.ingestionState` |
L172: | Soft delete | **IMPLEMENTED** | `deletedAt` on all tables |
L173: 
L174: ---
L175: 
L176: ## Deliverable 4: Data Model Sanity & Governance Constraints
L177: 
L178: ### Current Schema
L179: 
L180: | Table | FK Constraints | Indexes | Cascade | Soft Delete |
L181: |-------|----------------|---------|---------|-------------|
L182: | cases | N/A | status, createdAt | N/A | **Yes** |
L183: | uploads | caseId → cases | caseId, caseId+createdAt, ingestionState | **Yes** | **Yes** |
L184: | upload_pages | uploadId → uploads | uploadId, uploadId+pageNumber | **Yes** | **Yes** |
L185: | chunks | caseId → cases, uploadId → uploads | caseId, uploadId, caseId+uploadId | **Yes** | **Yes** |
L186: | users | N/A | username (unique) | N/A | No |
L187: 
L188: ### Governance Constraints Verified
L189: 
L190: 1. **uploads.caseId** → NOT NULL, FK to cases with ON DELETE CASCADE ✓
L191: 2. **chunks.caseId** → NOT NULL, FK to cases with ON DELETE CASCADE ✓
L192: 3. **chunks.uploadId** → NOT NULL, FK to uploads with ON DELETE CASCADE ✓
L193: 4. **Indexes**: Appropriate indexes on FK columns and common queries ✓
L194: 5. **Soft delete**: `deletedAt` column on cases, uploads, upload_pages, chunks ✓
L195: 6. **Query filtering**: All read operations filter `isNull(deletedAt)` ✓
L196: 
L197: ---
L198: 
L199: ## Deliverable 5: Upload Feature Implementation (Completed)
L200: 
L201: ### Phase 1A: Database Schema ✓
L202: 
L203: **File**: `shared/schema.ts`
L204: 
L205: Tables implemented: `cases`, `uploads`, `upload_pages`, `chunks`
L206: 
L207: ### Phase 1B: API Endpoints ✓
L208: 
L209: **File**: `server/routes.ts`
L210: 
L211: All case-scoped upload routes implemented with sealed-case protection.
L212: 
L213: ### Phase 1C: Upload UI ✓
L214: 
L215: **File**: `client/src/components/UploadDrawer.tsx`
L216: 
L217: Tabs implemented:
L218: 1. **Files** — Drag/drop zone + file browser
L219: 2. **Photos** — Camera capture
L220: 3. **Scan** — Multi-page capture workflow
L221: 
L222: Header shows: "Attach to Case: {caseName}" (read-only)
L223: 
L224: ### Phase 1D: Ingestion State Machine ✓
L225: 
L226: **States**: `uploaded → stored → extracted → chunked → indexed → ready`
L227: 
L228: **Failure states**: `failed_storing`, `failed_extraction`, `failed_chunking`, `failed_indexing`
L229: 
L230: ---
L231: 
L232: ## Deliverable 6: LLM Call Contract (Completed)
L233: 
L234: **File**: `client/src/lib/llm/contract.ts`
L235: 
L236: ### Fail-Closed Gating
L237: 
L238: | Condition | Response |
L239: |-----------|----------|
L240: | Missing caseId | `CONTEXT_REQUIRED` with next actions |
L241: | Missing evidence | `REFUSAL` with refusal flags |
L242: | API error | `ERROR` with error message |
L243: | Valid context | `SUCCESS` with content and citations |
L244: 
L245: ### System Prompt Governance
L246: 
L247: All LLM calls include:
L248: - Case ID binding
L249: - Evidence citation requirements (`[CANON_REF:evidenceId]` format)
L250: - Confidence level requirements
L251: - Refusal on insufficient evidence
L252: 
L253: ---
L254: 
L255: ## Deliverable 7: Verification Checklist
L256: 
L257: ### Manual Demo Test Steps
L258: 
L259: - [x] **Create case**: POST /api/cases → returns caseId
L260: - [x] **Upload file**: POST /api/cases/:caseId/uploads/init → POST .../complete → see state=stored
L261: - [ ] **Ingestion transitions**: Watch state progress stored → extracted → chunked → ready (backend job pending)
L262: - [x] **Chunks belong to case**: Schema enforces case binding via FK
L263: - [x] **Photo capture**: Click Photos tab → capture → file bound to case
L264: - [x] **Scan multi-page**: Click Scan tab → capture pages → produces ordered pages
L265: - [x] **Upload without case BLOCKED**: Attempt upload with no caseId → UI shows case selection
L266: - [x] **LLM without context BLOCKED**: Call validateLLMContext without caseId → returns `CONTEXT_REQUIRED`
L267: 
L268: ---
L269: 
L270: ## Deliverable 8: Naming Sweep (ELI/CABINET)
L271: 
L272: ### Code Files Checked
L273: 
L274: | Path | Status |
L275: |------|--------|
L276: | `client/src/**/*.tsx` | **CLEAN** — No ELI/CABINET references |
L277: | `client/src/**/*.ts` | **CLEAN** — No ELI/CABINET references |
L278: | `server/**/*.ts` | **CLEAN** — No ELI/CABINET references |
L279: | `shared/**/*.ts` | **CLEAN** — No ELI/CABINET references |
L280: 
L281: ### Documentation
L282: 
L283: | Path | Status |
L284: |------|--------|
L285: | `docs/investor/*.md` | **CLEAN** — Lantern/Nikodemus only |
L286: | `README.md` | **CLEAN** |
L287: | `replit.md` | **CLEAN** |
L288: 
L289: ---
L290: 
L291: ## Summary
L292: 
L293: **Current State**: Lantern is a case-bound investigative intelligence platform with:
L294: - PostgreSQL backend with governance constraints (soft delete, FK cascades)
L295: - Case-scoped upload API with sealed-case protection
L296: - Upload Drawer UI with Files/Photos/Scan tabs
L297: - LLM call contract with fail-closed gating
L298: - Local-first extraction and dossier curation
L299: 
L300: **Governance Features Implemented**:
L301: 1. ✓ PostgreSQL with cases/uploads/chunks tables
L302: 2. ✓ Soft delete on all tables
L303: 3. ✓ FK cascades for case deletion
L304: 4. ✓ Case-scoped API endpoints
L305: 5. ✓ Upload UI with Files/Photos/Scan tabs
L306: 6. ✓ Ingestion state machine schema
L307: 7. ✓ LLM call contract with CONTEXT_REQUIRED gating
L308: 
L309: **Naming**: Clean. No ELI/CABINET product confusion.
L310: 
L311: **Next Actions**:
L312: 1. Implement backend ingestion job processing
L313: 2. Add LLM API endpoint integration
L314: 3. Remove legacy routes and boot probe logging

--- FILE: docs/investor/01_NARRATIVE.md ---
L1: # Lantern: Evidentiary Record System for Institutional Risk
L2: 
L3: ## The Problem
L4: 
L5: Organizations managing compliance, legal, and investigative workflows face a fundamental breakdown: **evidence is scattered, claims are untraced, and analysis is not auditable**.
L6: 
L7: Current tools force a choice between:
L8: - **Speed** (AI summaries, keyword search) — fast but legally indefensible
L9: - **Rigor** (manual review, paralegal labor) — defensible but unscalable
L10: 
L11: When regulators, auditors, or opposing counsel ask "how did you reach this conclusion?", most organizations cannot answer. They have findings without provenance, conclusions without evidence chains, and reports without integrity verification.
L12: 
L13: This gap costs real money:
L14: - Compliance failures from missed connections
L15: - Legal exposure from undocumented reasoning
L16: - Audit findings from untraceable assertions
L17: 
L18: ## Why Current Tools Fail
L19: 
L20: 1. **Document management systems** store files but don't extract structure
L21: 2. **AI summarization tools** produce conclusions but erase provenance
L22: 3. **Investigation platforms** optimize for speed, not defensibility
L23: 4. **Manual review** doesn't scale and can't be audited after the fact
L24: 
L25: The market has optimized for "insights" when what institutions need is **evidence records**.
L26: 
L27: ## Lantern's Differentiator
L28: 
L29: Lantern is an **evidentiary record system**, not an insight engine.
L30: 
L31: Core design principles:
L32: - **No offset, no item**: Every extracted entity traces to exact source positions
L33: - **Deterministic by design**: Configured to produce consistent, repeatable output
L34: - **Report fingerprinting**: SHA-256 hashes for post-hoc integrity verification
L35: - **Explicit limits**: The system states what it cannot conclude, not just what it can
L36: 
L37: Lantern produces:
L38: - Structured data extraction with full provenance
L39: - Curated dossiers with entities, claims, and evidence linkage
L40: - Heuristic analysis that gates on evidence sufficiency
L41: - Publication-ready reports with audit trails
L42: 
L43: ## Who Buys
L44: 
L45: **Primary buyers** (payor + legal/compliance lane):
L46: - Healthcare payors managing fraud investigation and audit defense
L47: - Legal/compliance teams requiring defensible evidence chains
L48: - Institutional risk functions needing auditable analysis
L49: - Government affairs teams tracking regulatory and enforcement actions
L50: 
L51: **Common pain points**:
L52: - "We can't show our work to auditors"
L53: - "Our AI tools produce conclusions we can't defend"
L54: - "We need to trace every claim to source documents"
L55: 
L56: ## Why Now
L57: 
L58: 1. **Regulatory pressure increasing**: SOX, HIPAA, emerging AI governance rules demand auditability
L59: 2. **AI backlash in legal contexts**: Courts and regulators rejecting AI-generated analysis without provenance
L60: 3. **Cost of ambiguity rising**: Settlement costs, audit penalties, and reputational damage from indefensible claims
L61: 4. **No incumbent solution**: Enterprise tools optimize for throughput, not defensibility
L62: 
L63: The market is moving from "what did you find?" to "how can you prove it?"
L64: 
L65: Lantern is purpose-built for that question.
