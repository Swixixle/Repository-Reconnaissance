
--- FILE: README.md ---
L1: # HALO-RECEIPTS
L2: 
L3: AI Receipts - Forensic Verification System for AI Conversation Transcripts
L4: 
L5: [![Run on Replit](https://replit.com/badge/github/Swixixle/HALO-RECEIPTS)](https://replit.com/github/Swixixle/HALO-RECEIPTS)
L6: 
L7: ## Overview
L8: 
L9: AI Receipts is a forensic verification system for AI conversation transcripts. It provides cryptographic verification of receipt capsules, immutable storage, and forensic analysis capabilities.
L10: 
L11: For detailed documentation, see [replit.md](./replit.md)
L12: 
L13: ## Quick Start
L14: 
L15: ### Run on Replit (Recommended for Quick Setup)
L16: 
L17: 1. Click the "Run on Replit" badge above
L18: 2. Follow the setup guide in [docs/REPLIT_SETUP.md](./docs/REPLIT_SETUP.md)
L19: 
L20: ### Local Development
L21: 
L22: 1. **Prerequisites**
L23:    - Node.js 20+
L24:    - PostgreSQL database
L25:    - npm or yarn
L26: 
L27: 2. **Installation**
L28:    ```bash
L29:    npm install
L30:    ```
L31: 
L32: 3. **Configuration**
L33:    - Copy `.env.example` to `.env`
L34:    - Update `DATABASE_URL` with your PostgreSQL connection string
L35:    - Set other environment variables as needed
L36: 
L37: 4. **Database Setup**
L38:    ```bash
L39:    npm run db:push
L40:    ```
L41: 
L42: 5. **Run Development Server**
L43:    ```bash
L44:    npm run dev
L45:    ```
L46: 
L47: The application will start on port 5000 (http://localhost:5000)
L48: 
L49: ## Available Scripts
L50: 
L51: - `npm run dev` - Start development server
L52: - `npm run build` - Build for production
L53: - `npm run start` - Start production server
L54: - `npm run check` - Type check with TypeScript
L55: - `npm run db:push` - Push database schema changes
L56: 
L57: ## Technology Stack
L58: 
L59: - **Frontend**: React + TypeScript + Tailwind CSS + shadcn/ui
L60: - **Backend**: Express.js + Node.js 20
L61: - **Database**: PostgreSQL with Drizzle ORM
L62: - **Validation**: Zod schemas
L63: - **Cryptography**: Node.js crypto (SHA-256)
L64: 
L65: ## Documentation
L66: 
L67: - [Full Documentation](./replit.md) - Complete API and feature documentation
L68: - [Replit Setup Guide](./docs/REPLIT_SETUP.md) - Step-by-step Replit deployment
L69: - [Receipt Capsule v2 Proposal](./docs/RECEIPT_CAPSULE_V2.md) - Future schema evolution
L70: 
L71: ## Core Features
L72: 
L73: - **Receipt Verification**: Validates AI conversation receipts using SHA-256 hash verification
L74: - **Canonicalization (c14n-v1)**: Deterministic JSON canonicalization for consistent hashing
L75: - **Immutable Storage**: Verified receipts are locked and cannot be modified
L76: - **Kill Switch**: Irreversible control to permanently disable interpretation for a receipt
L77: - **Interpretation System**: Categorized as FACT, INTERPRETATION, or UNCERTAINTY (append-only)
L78: - **Tri-Sensor Analysis**: Parallel analysis with interpreter, summarizer, and claim extractor
L79: 
L80: ## Contributing
L81: 
L82: Contributions are welcome! Please feel free to submit a Pull Request.
L83: 
L84: ## License
L85: 
L86: MIT
L87: 
L88: ---
L89: 
L90: ### User Information
L91: - **Current User's Login:** Swixixle

--- FILE: SECURITY.md ---
L1: # SECURITY.md — AI Receipts Security Posture
L2: 
L3: *Last updated: 2026-02-12*
L4: 
L5: ## What This System Protects
L6: 
L7: ### Data Integrity
L8: - **Receipt hashes**: SHA-256 verification ensures transcript content has not been modified after signing.
L9: - **Receipt chain**: Sequential receipts are cryptographically linked via prev_hash, detecting insertions, deletions, or reordering.
L10: - **Audit trail**: Append-only, hash-chained event log detects unauthorized modifications to operator actions.
L11: - **Immutable lock**: Verified receipts cannot be modified after locking.
L12: - **Kill switch**: Irreversible mechanism to permanently disable interpretation of any receipt.
L13: 
L14: ### Authentication & Authorization
L15: - Private API endpoints require API key via `x-api-key` header.
L16: - Public verification endpoints are read-only with no write capability.
L17: - API keys are stored as environment secrets, never logged or exposed.
L18: 
L19: ### Input Validation
L20: - All request bodies validated with Zod schemas before processing.
L21: - Content-Type enforcement: only `application/json` accepted for POST/PUT/PATCH.
L22: - UTF-8 validation: malformed sequences rejected.
L23: - Request body size limit: 1MB maximum.
L24: 
L25: ### Rate Limiting
L26: - Per-IP burst and sustained rate limits on all API endpoints.
L27: - Public endpoints: 100/min sustained, 10/sec burst.
L28: - Private endpoints: 50/min sustained, 5/sec burst.
L29: - Rate limit headers included in all responses (X-RateLimit-*).
L30: 
L31: ### Response Headers
L32: - `X-Content-Type-Options: nosniff` — prevents MIME sniffing.
L33: - `X-Frame-Options: DENY` — prevents clickjacking.
L34: - `Referrer-Policy: strict-origin-when-cross-origin` — limits referrer leakage.
L35: - `X-XSS-Protection: 0` — defers to CSP (modern approach).
L36: - `Permissions-Policy` — disables camera, microphone, geolocation.
L37: 
L38: ### LLM Sensor Isolation
L39: - LLMs see ONLY transcript content, never verification data or system state.
L40: - Policy enforcement filters LLM output for forbidden words, confidence boundaries, and hedging requirements.
L41: - `observation_type` field only exists at wire boundaries; internal code uses camelCase.
L42: 
L43: ## What This System Does NOT Protect Against
L44: 
L45: ### Fully-Privileged Database Administrator
L46: A DBA with unrestricted access to the PostgreSQL database can:
L47: - Rewrite all audit trail rows AND the head pointer simultaneously.
L48: - Modify receipt data directly, bypassing API validation.
L49: - Delete or alter any stored data.
L50: 
L51: **Mitigation**: Anchor the audit trail head hash externally (signed checkpoint, WORM log, or third-party attestation service). This is documented as a future enhancement.
L52: 
L53: ### Network-Level Attacks
L54: - No built-in TLS termination (expected to run behind a reverse proxy or platform like Replit Deployments).
L55: - No mutual TLS for API clients.
L56: 
L57: ### Client-Side Tampering
L58: - The frontend is a read/display layer. All security enforcement happens server-side.
L59: - A malicious client can attempt any API call, but validation and auth gates prevent unauthorized actions.
L60: 
L61: ### Denial of Service
L62: - Rate limiting provides basic protection but is in-memory only (resets on restart).
L63: - No distributed rate limiting or WAF integration.
L64: 
L65: ## Expected Deployment Configuration
L66: 
L67: ### Environment Variables (Required)
L68: | Variable | Purpose |
L69: |----------|---------|
L70: | `DATABASE_URL` | PostgreSQL connection string |
L71: | `SESSION_SECRET` | Session signing secret |
L72: | `API_KEY` | API key for private endpoint authentication |
L73: 
L74: ### Environment Variables (Optional)
L75: | Variable | Purpose | Default |
L76: |----------|---------|---------|
L77: | `TRANSCRIPT_MODE` | Display mode: `full`, `redacted`, `hidden` | `full` |
L78: | `PORT` | HTTP listen port | `5000` |
L79: 
L80: ### Deployment Requirements
L81: - Run behind TLS-terminating reverse proxy (Replit Deployments, nginx, Cloudflare).
L82: - PostgreSQL 14+ with connection pooling recommended for production.
L83: - Set `NODE_ENV=production` for production builds.
L84: - Ensure `SESSION_SECRET` and `API_KEY` are cryptographically random (32+ bytes).
L85: 
L86: ### Logging & Monitoring
L87: - All API requests logged with method, path, status, and duration.
L88: - Auth failures, rate limit hits, and payload rejections logged with IP.
L89: - Prompt injection attempts flagged and logged.
L90: - No secrets or API keys are ever included in logs.
L91: 
L92: ## Reporting Security Issues
L93: 
L94: If you discover a security vulnerability, please report it responsibly. Do not open a public issue.

--- FILE: STATE.md ---
L1: # STATE.md — AI Receipts System Truth
L2: 
L3: *Last updated: 2026-02-12*
L4: 
L5: ## Build Health
L6: 
L7: | Check | Status |
L8: |-------|--------|
L9: | `tsc --noEmit` | CLEAN (0 errors) |
L10: | `npm run build` | CLEAN (client + server) |
L11: | Tests (42 total) | ALL PASSING |
L12: | Server runtime | RUNNING on port 5000 |
L13: 
L14: ## Implemented (ship-ready)
L15: 
L16: ### Core Verification
L17: - SHA-256 hash verification of receipt capsules
L18: - Ed25519 signature verification
L19: - Receipt chain verification (prev_hash linking)
L20: - Canonicalization (c14n-v1) for deterministic hashing
L21: - Immutable lock (verified receipts cannot be modified)
L22: - Kill switch (irreversible, blocks all interpretations)
L23: 
L24: ### Audit Trail (v1.1)
L25: - SHA-256 hash-chained append-only event log
L26: - `stableStringifyStrict`: rejects undefined, BigInt, Date, Map, Set, RegExp, Buffer, functions, symbols, NaN/Infinity, circular refs, dangerous keys (__proto__, constructor, prototype), non-plain objects — all with path-based error messages
L27: - `auditPayloadV1` → `hashAuditPayload` pipeline (single-sourced, shared by append and verify)
L28: - `payload_v` column: optimization hint cross-checked against hash-protected `_v` (self-auditing invariant)
L29: - `payloadV` derived from builder output at write time (append cannot lie)
L30: - Strict mode: `?strict=true` fails if limit < totalEvents (prevents partial-coverage screenshots)
L31: - Transactional append with `FOR UPDATE` row locking
L32: - 12 action types covering full operator workflow
L33: - Partial verification: first-N window by seq, shows "seq 1–N of M"
L34: 
L35: ### Endpoint Contracts
L36: 
L37: #### `GET /api/health` (liveness — no auth)
L38: Fast, no DB hit. Always returns 200 with JSON:
L39: ```json
L40: { "status": "ok", "time": "...", "version": "..." }
L41: ```
L42: 
L43: #### `GET /api/ready` (readiness — no auth)
L44: Hits DB + audit head. Returns:
L45: ```json
L46: { "status": "ok"|"degraded", "ready": true|false, "db": { "ok": true }, "audit": { "ok": true }, "time": "...", "version": "..." }
L47: ```
L48: HTTP 200 when DB is reachable, 503 when DB is down. `status` degrades on either DB or audit head failure.
L49: 
L50: **Anti-flap design:**
L51: - DB check is `SELECT 1` (fast, deterministic)
L52: - Audit check reads only the `audit_head` singleton row (not full chain verify)
L53: - `ready: true` when DB is up, even if audit head is degraded — prevents load balancer flap
L54: - `ready: false` + 503 only on DB connection failure
L55: 
L56: #### `GET /api/audit/verify` (authenticated, rate-limited)
L57: Verifies audit chain integrity. Server-side cap: max 50,000 events per request. Rate-limited via `rateLimitVerify`. Returns:
L58: ```json
L59: {
L60:   "ok": true|false,
L61:   "status": "EMPTY"|"GENESIS"|"LINKED"|"BROKEN",
L62:   "checked": 42,
L63:   "checkedEvents": 42,
L64:   "totalEvents": 100,
L65:   "partial": true,
L66:   "head": { "seq": 42, "hash": "..." },
L67:   "firstBadSeq": null,
L68:   "break": null
L69: }
L70: ```
L71: **Query params**: `limit` (default 5000, max 50000), `strict` (boolean), `fromSeq` (cursor start), `toSeq` (cursor end).
L72: **Cursor-based verification**: `fromSeq`/`toSeq` enable targeted segment verification. When used, head consistency check is skipped and `partial` is always `true`.
L73: **Guarantees**: cryptographic integrity of the audit chain (hash linkage, payload version consistency, sequence continuity).
L74: **Does NOT guarantee**: semantic truth of logged actions, completeness of operator behavior, real-world event accuracy.
L75: 
L76: #### `GET /api/health/metrics` (authenticated)
L77: Returns in-memory instrumentation counters. Counters reset on restart.
L78: 
L79: ### Forensic Analysis
L80: - Tri-sensor analysis (interpreter, summarizer, claim extractor)
L81: - Forensic detectors (risk keywords, high-entropy patterns, PII heuristics)
L82: - Tamper-evident share packs with hash-chained event log
L83: - Public verification proof packs
L84: 
L85: ### LLM Sensor Integration
L86: - Adapter registry: 11 providers (openai, anthropic, google, xai, meta, mistral, cohere, perplexity, deepseek, qwen, mock)
L87: - Typed tuple registry (`Array<[ProviderName, AdapterFactory]>`)
L88: - Wire format (`observation_type`) isolated to adapter boundary; internal code uses `observationType`
L89: - Policy enforcement: forbidden words, hedging, confidence, limitations
L90: - Data isolation: LLMs see ONLY transcript content, never verification data
L91: 
L92: ### API & Security
L93: - API key authentication for private endpoints
L94: - Per-IP burst and sustained rate limiting (including `rateLimitVerify` on `/api/audit/verify`)
L95: - Input validation via Zod schemas
L96: - Backend pagination with server-side filtering/sorting
L97: - Security headers: X-Content-Type-Options, X-Frame-Options, Referrer-Policy, Permissions-Policy
L98: - Standardized error shape via `apiError()` helper
L99: - JSON 404 catch-all for unmatched `/api/*` routes
L100: - `GET /api/health` (no auth): liveness, no DB hit
L101: - `GET /api/ready` (no auth): readiness, DB + audit head, anti-flap design
L102: - `GET /api/health/metrics` (auth required): in-memory counters for audit, policy, adapter, rate-limit events
L103: - Canonical endpoint documentation: `docs/API_CONTRACTS.md`
L104: 
L105: ### Operational Instrumentation
L106: - Structured JSON logging for audit append/fail, verify results, policy violations, adapter errors
L107: - In-memory counters: audit.append.ok/fail, audit.verify.{ok,partial,broken,empty}, policy.violation.{type,total}, adapter.error.{provider,total}, ratelimit.{endpoint,total}
L108: - Wire/internal boundary formalization: `wireToInternalObservationType()` / `internalToWireObservationType()` / `buildAdapterOptions()` in `server/llm/wire-boundary.ts`
L109: 
L110: ### CI/CD
L111: - GitHub Actions workflow (`.github/workflows/ci.yml`): install, typecheck, db:push, test, boundary drift guard, canon drift guard, build on PR + main
L112: - Boundary drift guard: grep-based CI check that `observation_type` only appears in allowed boundary/type/test files
L113: - Canonicalization drift guard: `scripts/ci-canon-drift-guard.sh` enforces single source of truth in `server/audit-canon.ts`
L114: 
L115: ### Ed25519 Signed Checkpoints
L116: - `audit_checkpoints` table: stores signed checkpoint records anchoring audit chain state
L117: - `server/checkpoint-signer.ts`: Ed25519 key management (ephemeral auto-gen or env var keys)
L118: - Configurable interval via `CHECKPOINT_INTERVAL` env var (default 100)
L119: - Each checkpoint signs: seq, hash, timestamp, prev checkpoint link, event count
L120: - Verification endpoint and offline verifier support checkpoint signature verification
L121: 
L122: ### Forensic Export Pack (v1.1)
L123: - `scripts/export_forensic_pack.ts`: exports audit segment with checkpoints, version info, and verification manifest
L124: - `scripts/verify_forensic_pack.ts`: standalone offline verifier with `--public-key` flag for Ed25519 signature verification
L125: - Pack includes: events, checkpoints, system version (semver/commit/engineId), segment metadata, verification result, manifest, and self-integrity hash (`packHash`)
L126: - Documented in `docs/FORENSIC_EXPORT_PACK.md`
L127: 
L128: ### Version Stamping
L129: - `server/version.ts`: centralized semver + git commit tracking
L130: - All forensic packs embed system version info
L131: - Health endpoint uses centralized version string
L132: 
L133: ### Demo & Documentation
L134: - `docs/DEMO_SPINE.md`: scripted 6-8 minute operator walkthrough with exact curl commands and expected outputs
L135: - `scripts/demo.sh`: automated demo runner with formatted output
L136: - `docs/THREAT_MODEL.md` v2: assets/adversaries/mitigations table, operator misuse/misinterpretation section, residual risks
L137: - `docs/REGULATORY_ALIGNMENT.md`: compliance matrix for 21 CFR Part 11, HIPAA, SOC 2, ISO 27001, EU AI Act, NIST AI RMF
L138: - `docs/CRYPTO_AGILITY.md`: signature abstraction, PQC migration roadmap (Ed25519 → ML-DSA), hash chain transition plan
L139: 
L140: ### UI
L141: - Receipt viewer with row virtualization
L142: - Receipt comparison (side-by-side with field deltas)
L143: - Bulk export (JSONL, CSV) with snapshot boundaries
L144: - Saved views for quick filter access
L145: - Governance page with audit trail, verify button, threat model text
L146: - Halo UI control layer
L147: - Global audit integrity banner (top of all pages): verified/partial/broken/degraded states with auto-refresh
L148: 
L149: ## Explicit Invariants
L150: 
L151: 1. `stableStringifyStrict` rejects ambiguous types BEFORE hashing. No silent coercion.
L152: 2. `auditPayloadV1()` and `hashAuditPayload()` are the ONLY path to audit hashes. Both append and verify use this pipeline.
L153: 3. `payload_v` column MUST equal `_v` in the hashed payload. Verification cross-checks this.
L154: 4. `payloadV` is derived from builder output, never hardcoded.
L155: 5. Strict verify fails if limit < totalEvents. No partial coverage can display as "OK".
L156: 6. Unverified receipts cannot be interpreted.
L157: 7. Kill switch is irreversible and blocks all interpretations.
L158: 8. Interpretations are append-only.
L159: 9. Immutable lock prevents raw JSON modification.
L160: 10. LLM observations never affect verification_status (data isolation).
L161: 11. `AdapterObservation` does NOT contain `observation_type`. Only `AdapterRequest` and `LlmObservation` use it.
L162: 
L163: ## Threat Model (Audit Trail)
L164: 
L165: **What it detects:**
L166: - Payload modification (hash_mismatch)
L167: - Row deletion (seq_gap)
L168: - Row reordering (prevHash_mismatch)
L169: - Version column tampering (version_mismatch / unknown_payload_version)
L170: 
L171: **What it does NOT protect against:**
L172: - Fully-privileged DB admin who rewrites ALL rows + head simultaneously
L173: - For that level: anchor head hash externally (signed checkpoint, WORM log, third-party attestation)
L174: 
L175: **Self-auditing property:**
L176: - Even if a database administrator edits `payload_v`, verification fails unless the hash-protected `_v` matches
L177: 
L178: ## Known Risks / Technical Debt
L179: 
L180: 1. No request correlation ID threaded through adapter → policy → audit
L181: 2. 3 LLM adapter stubs (anthropic, google, etc.) return NOT_IMPLEMENTED — expected until API keys are configured
L182: 3. `routes.ts` is 2000+ lines — candidate for splitting by domain
L183: 4. Rate limiter is in-memory only (resets on restart)
L184: 5. Counters are in-memory only (reset on restart)
L185: 
L186: ## Explicitly Forbidden
L187: 
L188: - No mock/placeholder data in production paths
L189: - No `JSON.stringify` on intermediate objects for hash computation (manual string building only)
L190: - No `observation_type` field access on `AdapterObservation` type
L191: - No hardcoded `payloadV` — always derived from builder
L192: - No silent fallbacks for type errors — surface explicit errors
L193: 
L194: ## Test Inventory (42 tests)
L195: 
L196: ### Golden Tests (35 tests) — `server/__tests__/golden-audit-chain.test.ts`
L197: - `stableStringifyStrict`: 19 tests (determinism, sorting, type rejection, circular refs, dangerous keys)
L198: - `auditPayloadV1`: 4 tests (_v embedding, JSON parsing, determinism, hash determinism)
L199: - Audit chain integrity: 3 tests (3-event chain + tamper, prevHash alteration, _v in hash)
L200: - Adapter boundary: 2 tests (observation_type not on AdapterObservation, AdapterRequest shape)
L201: - Wire/internal boundary: 7 tests (valid/invalid wire values, identity mapping, adapter options wrapping, drift guard, round-trip stability, recursive snake_case absence)
L202: 
L203: ### E2E Integration Tests (7 tests) — `server/__tests__/audit-e2e.test.ts`
L204: - Health endpoint returns 200 with JSON `{ status: "ok" }`
L205: - Ready endpoint returns 200 with DB/audit status
L206: - Audit verify returns chain status with operator fields (`ok`, `checkedEvents`, `firstBadSeq`)
L207: - Metrics endpoint returns counters object
L208: - Audit append → verify lifecycle (POST events, verify OK)
L209: - Tamper detection (modify row, verify BROKEN with correct `firstBadSeq`)
L210: - Partial coverage indication when limit < total events
L211: 
L212: Run: `npx vitest run --config vitest.config.ts`
L213: 
L214: ## Top-10 Punchlist (next priority order)
L215: 
L216: 1. Add request correlation ID across adapter → policy → audit
L217: 2. ~~Signed checkpoint anchoring (Ed25519 signature every N events)~~ DONE
L218: 3. Split `routes.ts` by domain (receipts, audit, exports, sensors)
L219: 4. Rate limiter persistence (Redis or DB-backed)
L220: 5. Policy enforcement golden test (deterministic output for same input)
L221: 6. External anchor sink (WORM storage for checkpoint hashes)
L222: 7. Bulk export streaming for large datasets
L223: 8. Counter persistence (Redis or DB-backed)
L224: 9. Role-based access control (RBAC) for regulatory alignment
L225: 10. Constant-time comparison for auth key validation

--- FILE: replit.md ---
L1: # AI Receipts - Forensic Verification System
L2: 
L3: ## Overview
L4: AI Receipts is a forensic verification system designed to provide cryptographic verification, immutable storage, and forensic analysis for AI conversation transcripts. Its core purpose is to ensure the integrity and authenticity of AI interactions, enabling detailed forensic examination and preventing tampering. The project aims to establish a trusted framework for AI accountability and transparency.
L5: 
L6: ## User Preferences
L7: Not specified.
L8: 
L9: ## System Architecture
L10: 
L11: ### Core Capabilities
L12: - **Receipt Verification**: Validates AI conversation receipts using SHA-256 hash verification and Ed25519 signatures.
L13: - **Canonicalization (c14n-v1)**: Employs deterministic JSON canonicalization for consistent hashing across all receipts.
L14: - **Immutable Storage**: Verified receipts are permanently locked against modification.
L15: - **Kill Switch**: Provides an irreversible mechanism to disable interpretation for any given receipt.
L16: - **Interpretation System**: Supports append-only interpretations categorized as FACT, INTERPRETATION, or UNCERTAINTY.
L17: - **Tri-Sensor Analysis**: Facilitates parallel analysis of transcripts using an interpreter, summarizer, and claim extractor.
L18: - **Receipt Chaining**: Verifies cryptographic links between sequential receipts using SHA256 hashes of canonicalized core fields.
L19: - **Forensic Detectors**: Independently analyze transcripts for risk keywords, high-entropy patterns, and PII heuristics, generating integrity context based on verification status.
L20: - **LLM Sensor Integration**: Allows LLMs to observe and describe transcript content (paraphrase, ambiguity, tone, etc.) without making truth judgments, ensuring data isolation from verification outcomes.
L21: - **Research Dataset**: Generates anonymized, aggregatable research data for model behavior analysis with explicit opt-in consent and strict exclusion of sensitive information.
L22: - **Tamper-Evident Share Pack**: Provides a hash-chained forensic event log and a system for building verifiable share packs with sensitive data redaction.
L23: - **Bulk Export System**: Allows authenticated users to export receipts in various formats (JSONL, CSV) with guardrails for PII and kill-switched content.
L24: - **Saved Views**: Enables users to store and manage filtered receipt views.
L25: - **Receipt Comparison**: Side-by-side comparison of two receipts with field deltas, forensics comparison, and per-side actions (View Detail, Proof Pack, Export).
L26: - **Proof Spine v1**: Canonical `/api/proofpack/:receiptId` endpoint returns unified proof pack with integrity, signature, chain status, and audit summary. All downstream modules consume this single contract.
L27: - **Proof-Gated Lantern**: `POST /api/lantern/followup` only responds when receipt is VERIFIED. Stores durable threads with ProofPack snapshot. Thread CRUD: `GET /api/lantern/threads/:receiptId`, `GET /api/lantern/thread/:threadId/messages`.
L28: - **Durable Threads**: `threads` and `thread_messages` tables for conversation continuity with receipt binding and ProofPack snapshot at creation time.
L29: - **Append-Only Audit Trail**: Logs all operator actions (saved view create/delete/apply, bulk export lifecycle, receipt export, comparison views, lantern followup) with IP, user-agent, and JSON payload. Displayed on Governance page with action filter, receipt ID filter, pagination, and copy-payload button.
L30: - **Backend Pagination**: Implements server-side pagination, filtering, and sorting for efficient data retrieval.
L31: - **Health Endpoints**: `GET /api/health` (liveness, no DB), `GET /api/ready` (readiness, DB + audit head, anti-flap), `GET /api/health/metrics` (in-memory counters).
L32: - **Cursor-Based Audit Verify**: `GET /api/audit/verify` supports `fromSeq`/`toSeq` for targeted segment verification, rate-limited via `rateLimitVerify`.
L33: - **Security Headers**: X-Content-Type-Options, X-Frame-Options, Referrer-Policy, Permissions-Policy on all responses.
L34: - **Wire/Internal Boundary**: Formalized `wireToInternalObservationType()` / `internalToWireObservationType()` in `server/llm/wire-boundary.ts`.
L35: - **Instrumentation**: Structured JSON logging + in-memory counters for audit, policy, adapter, and rate-limit events.
L36: - **Ed25519 Signed Checkpoints**: Automatic checkpoint creation every N events (configurable via `CHECKPOINT_INTERVAL`), stored in `audit_checkpoints` table with signature verification. Signed payload includes `engine_id`, `audit_payload_version`, `checkpoint_seq`, `event_seq`, `event_hash` for ungameable binding.
L37: - **Checkpoint Chain Continuity**: Each checkpoint links to its predecessor via `prev_checkpoint_id` and `prev_checkpoint_hash`, verified by offline verifier.
L38: - **Version Stamping**: Centralized `server/version.ts` embeds semver + git commit in forensic packs and API responses.
L39: - **CI**: GitHub Actions workflow (`.github/workflows/ci.yml`) runs typecheck, test, canon drift guard, boundary drift guard, proof run, build on PR + main. Proof bundle uploaded as artifact with 90-day retention.
L40: - **Proof Run**: `scripts/proof_run.ts` orchestrates end-to-end proof generation: tests → event generation → checkpoint forcing → forensic pack export → clean verification → tamper detection → artifact output.
L41: - **Verifier Release**: `scripts/build_verifier_release.ts` packages standalone offline verifier into self-contained zip (compiled JS + public key + README, no external dependencies).
L42: - **Release Workflow**: `.github/workflows/release.yml` triggers on `v*` tags, runs full CI, builds proof bundle + verifier zip, computes SHA-256 checksums, publishes GitHub Release.
L43: - **Key Ring Support**: Verifier accepts `--key-ring <dir>` for multi-key verification; each checkpoint's `publicKeyId` matched to `<kid>.pem` file. Enables seamless key rotation.
L44: - **Key Custody Model**: Dev/staging/prod key environments with `CHECKPOINT_KEY_ENV` classification. Key ring rotation protocol documented in THREAT_MODEL.md.
L45: - **External Anchoring**: `server/checkpoint-anchor.ts` defines `CheckpointAnchor` interface with `anchor()`, `verify()`, `name()` methods. Four backends: LogOnly (default), S3WormAnchor (real S3 Object Lock with GOVERNANCE/COMPLIANCE modes), Rfc3161TsaAnchor (RFC3161 timestamp authority with messageImprint validation and trusted fingerprints), MultiAnchor (fan-out to multiple backends). Configured via `CHECKPOINT_ANCHOR_TYPE` env var. S3 supports cross-account IAM, configurable retention days, objectBody/objectHash for offline verification. RFC3161 supports pinned TSA cert allowlist.
L46: - **Anchor Modes**: `--anchors=required` in proof_run hard-fails when only log-only backend configured. `--anchors=optional` (default) allows log-only for dev/testing.
L47: - **Anchor Payload v1**: Constant-size canonical JSON payload with `_v`, `engine_id`, `audit_payload_version`, `checkpoint_id`, `checkpoint_seq`, `event_seq`, `event_hash`, `checkpoint_hash`, `kid`, `created_at`. SHA-256 hash of payload stored as `anchorHash`.
L48: - **Anchor Receipts in Forensic Packs**: Format 1.2 includes `anchorReceipts` array. Offline verifier validates anchor hash integrity, checkpoint binding, and reports anchor type coverage.
L49: - **Proof Bundle Spec**: `docs/PROOF_BUNDLE.md` documents bundle files, what each proves, expected outputs, and failure meanings.
L50: - **Regulatory Alignment**: `docs/REGULATORY_ALIGNMENT.md` maps capabilities to 21 CFR Part 11, HIPAA, SOC 2, ISO 27001, EU AI Act, NIST AI RMF.
L51: - **Regulatory Matrix Excerpt**: `docs/REGULATORY_MATRIX_EXCERPT.md` provides 10-row compliance officer quick reference across 6 frameworks.
L52: - **Executive Summary**: `docs/EXECUTIVE_SUMMARY.md` 2-page non-technical overview for pilots.
L53: - **Crypto Agility**: `docs/CRYPTO_AGILITY.md` documents signature abstraction and PQC migration roadmap (Ed25519 → ML-DSA).
L54: - **Documentation Index**: `docs/START_HERE.md` provides 90-second overview, pilot packet links, reading order, and release process description.
L55: - **Non-Goals Document**: `docs/NON_GOALS.md` defines explicit boundaries: no truth judgments, no content moderation, no real-time monitoring, single-operator model.
L56: - **Competitive Comparison**: `docs/COMPETITIVE_COMPARISON.md` positions system vs. AI observability, immutable databases, and governance platforms.
L57: - **Release Reproducibility**: CI rebuilds verifier zip and compares SHA-256 hashes for deterministic builds.
L58: - **Sigstore Cosign Signing**: All release artifacts signed with keyless OIDC; `.sig` and `.pem` files published.
L59: - **SBOM**: CycloneDX 1.5 format documenting zero external dependencies.
L60: - **Key Rotation Proof Tests**: 8 tests covering dual-key eras, chain continuity across rotation, missing/wrong key detection.
L61: - **Anchor Integration Tests**: 22 tests covering anchor_hash integrity, tamper detection, S3 objectBody/objectHash validation, RFC3161 messageImprint validation, MultiAnchor fan-out, anchor-required mode, payload binding.
L62: - **Failure-Mode Playbooks**: Key compromise, key loss, incorrect rotation, unknown kid resolution procedures in THREAT_MODEL.md.
L63: - **External Anchoring Guide**: `docs/EXTERNAL_ANCHORING.md` documents what anchoring prevents/doesn't, threat model delta (2-party → 4-party collusion), minimal IAM policy, deployment recommendations.
L64: - **Pilot Setup Guide**: `docs/PILOT_SETUP_AWS_ANCHOR_ACCOUNT.md` step-by-step AWS S3 Object Lock anchor account setup with IAM policy, retention config, cross-account setup, and verification checklist.
L65: - **TSA Providers Guide**: `docs/TSA_PROVIDERS.md` tested TSA provider configs (FreeTSA, DigiCert, Sectigo), fingerprint pinning instructions, environment recommendations.
L66: - **Objections Document**: `docs/OBJECTIONS_AND_PRECISE_ANSWERS.md` stakeholder Q&A one-pager covering truth claims, admin rewrite, key rotation, anchor downtime, offline verification, PII, verifier integrity, compliance, non-goals.
L67: - **Pilot Runbook**: `docs/PILOT_RUNBOOK.md` end-to-end pilot flow from clone to verified proof with artifact inventory.
L68: - **Anchor Smoke Test**: `scripts/anchor_smoke.ts` validates anchor backend (S3/TSA/log-only) write, verify, binding, and tamper detection.
L69: - **TSA Smoke Test**: `scripts/tsa_smoke.ts` validates RFC3161 TSA messageImprint, payload binding, and tamper detection.
L70: - **CI Split**: `.github/workflows/ci.yml` split into 4 jobs: test gate, determinism gate, proof bundle (log-only), proof bundle (anchored, conditional on ANCHORS_AVAILABLE var).
L71: 
L72: ### UI/UX Decisions
L73: - Frontend built with React, TypeScript, Tailwind CSS, and shadcn/ui.
L74: - Row virtualization is used for efficient rendering of large datasets.
L75: - UI components include debounced search, filter selectors, pagination controls, and confirmation dialogs for sensitive operations.
L76: - Global audit integrity banner at top of all pages showing verified/partial/broken/degraded status.
L77: 
L78: ### Technical Implementation
L79: - **Backend**: Express.js with Node.js 20.
L80: - **Database**: PostgreSQL with Drizzle ORM.
L81: - **Validation**: Zod schemas for robust data validation.
L82: - **Cryptography**: Node.js built-in crypto module for SHA-256 hashing and Ed25519 signing.
L83: - **Rate Limiting**: Per-IP burst and sustained rate limits on API endpoints.
L84: - **Authentication**: API key-based authentication for private endpoints.
L85: - **Testing**: 72 tests total — 35 golden tests + 7 E2E integration tests + 8 key rotation proof tests + 22 anchor integration tests (run via `npx vitest run --config vitest.config.ts`).
L86: - **Guards & Constraints**:
L87:     - Unverified receipts cannot be interpreted.
L88:     - Kill switch is irreversible and blocks all interpretations.
L89:     - Interpretations are append-only.
L90:     - Immutable lock prevents raw JSON modification.
L91:     - Private endpoints require valid API keys.
L92: 
L93: ## External Dependencies
L94: - **PostgreSQL**: Relational database for storing receipt data, interpretations, and system configurations.
L95: - **Drizzle ORM**: Object-Relational Mapper for interacting with PostgreSQL.
L96: - **React**: JavaScript library for building user interfaces.
L97: - **TypeScript**: Superset of JavaScript that adds static typing.
L98: - **Tailwind CSS**: Utility-first CSS framework for styling.
L99: - **shadcn/ui**: Component library built on Tailwind CSS and Radix UI.
L100: - **Express.js**: Web application framework for Node.js.
L101: - **Zod**: TypeScript-first schema declaration and validation library.
L102: - **Archiver**: Library for creating ZIP archives (used in bulk export).
L103: - **@tanstack/react-virtual**: Library for efficient virtualization of large lists in React.

--- FILE: docs/API_CONTRACTS.md ---
L1: # API Endpoint Contracts
L2: 
L3: Canonical documentation for all API endpoints. Each contract specifies the request/response shape, guarantees, and non-guarantees.
L4: 
L5: ---
L6: 
L7: ## Health & Readiness
L8: 
L9: ### `GET /api/health`
L10: 
L11: **Purpose**: Liveness probe. No DB hit.
L12: 
L13: | Field | Value |
L14: |-------|-------|
L15: | Auth | None |
L16: | Rate limit | None |
L17: | Response | `{ status: "ok", time: "<ISO 8601>", version: "<engine_id>" }` |
L18: | Status code | Always `200` |
L19: | Guarantee | Returns within 10ms, never touches DB |
L20: | Non-guarantee | Does not prove DB or audit chain are healthy |
L21: 
L22: ### `GET /api/ready`
L23: 
L24: **Purpose**: Readiness probe. Cheap DB + audit head check.
L25: 
L26: | Field | Value |
L27: |-------|-------|
L28: | Auth | None |
L29: | Rate limit | None |
L30: | Response | `{ status, ready, time: "<ISO 8601>", version: "<engine_id>", db: { ok }, audit: { ok } }` |
L31: | Status `200` | DB reachable (even if audit head degraded) |
L32: | Status `503` | DB connection failure |
L33: | Anti-flap | Returns 200 when DB is up regardless of audit state to prevent load balancer flapping |
L34: | Guarantee | Does NOT run full chain verification |
L35: 
L36: ### `GET /api/health/metrics`
L37: 
L38: **Purpose**: In-memory operational counters.
L39: 
L40: | Field | Value |
L41: |-------|-------|
L42: | Auth | `requireAuth` (API key) |
L43: | Rate limit | None |
L44: | Response | `{ counters: { <key>: <number>, ... } }` |
L45: | Guarantee | Counters are monotonically increasing within a process lifetime |
L46: | Non-guarantee | Counters reset on server restart |
L47: 
L48: ---
L49: 
L50: ## Audit Trail
L51: 
L52: ### `GET /api/audit`
L53: 
L54: **Purpose**: Paginated audit event list with filtering and sorting.
L55: 
L56: | Field | Value |
L57: |-------|-------|
L58: | Auth | `requireAuth` |
L59: | Rate limit | `rateLimitPublic` |
L60: | Query params | `page`, `limit`, `action`, `receiptId`, `sortBy`, `sortDir` |
L61: | Response | `{ events, total, page, limit, totalPages }` |
L62: | Guarantee | Server-side pagination, max 100 per page |
L63: 
L64: ### `GET /api/audit/verify`
L65: 
L66: **Purpose**: Full or cursor-based audit chain integrity verification.
L67: 
L68: | Field | Value |
L69: |-------|-------|
L70: | Auth | `requireAuth` |
L71: | Rate limit | `rateLimitVerify` |
L72: | Query params | `limit` (default 5000, max 50000), `strict`, `fromSeq`, `toSeq` |
L73: | Response | `{ ok, status, checked, checkedEvents, totalEvents, partial, head, expectedHead, firstBadSeq, break }` |
L74: | Status values | `EMPTY`, `GENESIS`, `LINKED`, `BROKEN` |
L75: | `ok` field | `true` unless status is `BROKEN` |
L76: | `fromSeq`/`toSeq` | Cursor-based segment verification; skips head consistency check |
L77: | `strict` mode | Fails if limit covers fewer events than total |
L78: | Guarantee | Server-side cap at 50,000 events per call |
L79: 
L80: ---
L81: 
L82: ## Receipts
L83: 
L84: ### `GET /api/receipts`
L85: 
L86: **Purpose**: Paginated receipt list with filtering and sorting.
L87: 
L88: | Field | Value |
L89: |-------|-------|
L90: | Auth | `requireAuth` |
L91: | Query params | `page`, `limit`, `search`, `verified`, `killSwitched`, `hasInterpretations`, `sortBy`, `sortDir` |
L92: | Response | `{ receipts, total, page, limit, totalPages }` |
L93: 
L94: ### `GET /api/receipts/:id`
L95: 
L96: **Purpose**: Single receipt detail.
L97: 
L98: | Field | Value |
L99: |-------|-------|
L100: | Auth | `requireAuth` |
L101: | Response | Full receipt object |
L102: | Status `404` | Receipt not found |
L103: 
L104: ### `POST /api/receipts`
L105: 
L106: **Purpose**: Create a new receipt.
L107: 
L108: | Field | Value |
L109: |-------|-------|
L110: | Auth | `requireAuth` |
L111: | Body | Receipt JSON (validated by Zod schema) |
L112: | Response | Created receipt object |
L113: | Side effect | Appends audit event |
L114: 
L115: ### `POST /api/receipts/:id/verify`
L116: 
L117: **Purpose**: Verify a receipt's SHA-256 hash.
L118: 
L119: | Field | Value |
L120: |-------|-------|
L121: | Auth | `requireAuth` |
L122: | Response | `{ verified, hash, expectedHash }` |
L123: | Side effect | Updates receipt verified status, appends audit event |
L124: 
L125: ### `POST /api/receipts/:id/kill-switch`
L126: 
L127: **Purpose**: Irreversibly disable interpretation for a receipt.
L128: 
L129: | Field | Value |
L130: |-------|-------|
L131: | Auth | `requireAuth` |
L132: | Response | Updated receipt |
L133: | Guarantee | Irreversible; blocks all future interpretations |
L134: | Side effect | Appends audit event |
L135: 
L136: ### `POST /api/receipts/:id/lock`
L137: 
L138: **Purpose**: Immutably lock a receipt against modification.
L139: 
L140: | Field | Value |
L141: |-------|-------|
L142: | Auth | `requireAuth` |
L143: | Precondition | Receipt must be verified |
L144: | Side effect | Appends audit event |
L145: 
L146: ---
L147: 
L148: ## Interpretations
L149: 
L150: ### `GET /api/receipts/:id/interpretations`
L151: 
L152: **Purpose**: List interpretations for a receipt.
L153: 
L154: | Field | Value |
L155: |-------|-------|
L156: | Auth | `requireAuth` |
L157: | Response | Array of interpretation objects |
L158: 
L159: ### `POST /api/receipts/:id/interpretations`
L160: 
L161: **Purpose**: Append an interpretation to a receipt.
L162: 
L163: | Field | Value |
L164: |-------|-------|
L165: | Auth | `requireAuth` |
L166: | Precondition | Receipt must be verified and not kill-switched |
L167: | Categories | `FACT`, `INTERPRETATION`, `UNCERTAINTY` |
L168: | Guarantee | Append-only; existing interpretations cannot be modified |
L169: 
L170: ---
L171: 
L172: ## Public Verification
L173: 
L174: ### `POST /api/public/verify`
L175: 
L176: **Purpose**: Public receipt verification (no auth required).
L177: 
L178: | Field | Value |
L179: |-------|-------|
L180: | Auth | None |
L181: | Rate limit | `rateLimitPublic` |
L182: | Body | Receipt JSON for hash verification |
L183: | Response | `{ verified, hash }` |
L184: | Security | Request size limited to 1MB |
L185: 
L186: ### `POST /api/public/verify/chain`
L187: 
L188: **Purpose**: Public receipt chain verification.
L189: 
L190: | Field | Value |
L191: |-------|-------|
L192: | Auth | None |
L193: | Rate limit | `rateLimitPublic` |
L194: | Body | Array of receipts to verify chain linkage |
L195: 
L196: ### `POST /api/public/proof-pack`
L197: 
L198: **Purpose**: Generate a cryptographic proof pack for a receipt.
L199: 
L200: | Field | Value |
L201: |-------|-------|
L202: | Auth | None |
L203: | Rate limit | `rateLimitPublic` |
L204: | Body | Receipt JSON |
L205: | Response | Proof pack with verification artifacts |
L206: 
L207: ---
L208: 
L209: ## Bulk Export
L210: 
L211: ### `POST /api/export/start`
L212: 
L213: **Purpose**: Start a bulk export job.
L214: 
L215: | Field | Value |
L216: |-------|-------|
L217: | Auth | `requireAuth` |
L218: | Body | Export parameters (format, filters) |
L219: | Response | `{ exportId }` |
L220: | Side effect | Appends audit event |
L221: 
L222: ### `GET /api/export/:id/status`
L223: 
L224: **Purpose**: Check export job status.
L225: 
L226: | Field | Value |
L227: |-------|-------|
L228: | Auth | `requireAuth` |
L229: | Response | `{ status, progress, downloadUrl }` |
L230: 
L231: ### `POST /api/export/:id/confirm`
L232: 
L233: **Purpose**: Confirm and finalize an export.
L234: 
L235: | Field | Value |
L236: |-------|-------|
L237: | Auth | `requireAuth` |
L238: | Side effect | Appends audit event |
L239: 
L240: ### `GET /api/export/:id/download`
L241: 
L242: **Purpose**: Download completed export file.
L243: 
L244: | Field | Value |
L245: |-------|-------|
L246: | Auth | `requireAuth` |
L247: | Response | File download (JSONL or CSV) |
L248: | Side effect | Appends audit event |
L249: 
L250: ---
L251: 
L252: ## Saved Views
L253: 
L254: ### `GET /api/saved-views`
L255: 
L256: **Purpose**: List saved filter views.
L257: 
L258: | Field | Value |
L259: |-------|-------|
L260: | Auth | `requireAuth` |
L261: | Response | Array of saved view objects |
L262: 
L263: ### `POST /api/saved-views`
L264: 
L265: **Purpose**: Create a saved view.
L266: 
L267: | Field | Value |
L268: |-------|-------|
L269: | Auth | `requireAuth` |
L270: | Side effect | Appends audit event |
L271: 
L272: ### `DELETE /api/saved-views/:id`
L273: 
L274: **Purpose**: Delete a saved view.
L275: 
L276: | Field | Value |
L277: |-------|-------|
L278: | Auth | `requireAuth` |
L279: | Side effect | Appends audit event |
L280: 
L281: ---
L282: 
L283: ## Comparison
L284: 
L285: ### `POST /api/compare/viewed`
L286: 
L287: **Purpose**: Log a receipt comparison view.
L288: 
L289: | Field | Value |
L290: |-------|-------|
L291: | Auth | `requireAuth` |
L292: | Body | `{ left, right }` receipt IDs |
L293: | Side effect | Appends audit event |
L294: 
L295: ---
L296: 
L297: ## Error Shape
L298: 
L299: All API errors follow a consistent shape:
L300: 
L301: ```json
L302: {
L303:   "error": {
L304:     "code": 404,
L305:     "message": "Not found",
L306:     "detail": "No route matches GET /api/nonexistent"
L307:   }
L308: }
L309: ```
L310: 
L311: Public endpoint errors use a structured format:
L312: 
L313: ```json
L314: {
L315:   "error": {
L316:     "code": "RATE_LIMIT_EXCEEDED",
L317:     "message": "Rate limit exceeded",
L318:     "meta": { "retry_after_seconds": 60 }
L319:   }
L320: }
L321: ```
L322: 
L323: ---
L324: 
L325: ## Rate Limit Headers
L326: 
L327: All rate-limited endpoints return:
L328: 
L329: | Header | Description |
L330: |--------|-------------|
L331: | `X-RateLimit-Limit` | Max requests in window |
L332: | `X-RateLimit-Remaining` | Remaining requests |
L333: | `X-RateLimit-Reset` | Window reset timestamp (Unix seconds) |
L334: | `Retry-After` | Seconds until retry (only on 429) |
L335: 
L336: ---
L337: 
L338: ## Security Headers
L339: 
L340: All responses include:
L341: 
L342: | Header | Value |
L343: |--------|-------|
L344: | `X-Content-Type-Options` | `nosniff` |
L345: | `X-Frame-Options` | `DENY` |
L346: | `Referrer-Policy` | `strict-origin-when-cross-origin` |
L347: | `Permissions-Policy` | `camera=(), microphone=(), geolocation=()` |

--- FILE: docs/CLIENT_INTEGRATION.md ---
L1: # Client Integration: Proxy Pattern (Do Not Expose API Keys)
L2: 
L3: AI Receipts uses an authenticated verifier endpoint for receipt submission:
L4: 
L5: - **Privileged:** `POST /api/verify` (requires `x-api-key`)
L6: - **Public, read-only (no auth required):**
L7:   - `GET /api/public/receipts/:id/verify`
L8:   - `GET /api/public/receipts/:id/proof`
L9: 
L10: ---
L11: 
L12: ## Core Rule
L13: 
L14: **Client applications must never call `POST /api/verify` directly from a browser.**
L15: 
L16: Browser environments cannot keep secrets. If you ship an API key to the frontend, you should assume it is compromised.
L17: 
L18: ### Correct Architecture
L19: 
L20: ```
L21: Browser → Client Backend (proxy) → AI Receipts verifier
L22: ```
L23: 
L24: The client backend:
L25: - stores the verifier API key in server secrets
L26: - injects the key via the `x-api-key` header
L27: - forwards the receipt capsule JSON to the verifier
L28: 
L29: ### Incorrect Architecture
L30: 
L31: ```
L32: Browser → AI Receipts POST /api/verify
L33: ```
L34: 
L35: This leaks the key to:
L36: - end users
L37: - browser extensions
L38: - logs / crash reports
L39: - proxy tooling
L40: - "View Source" + devtools
L41: 
L42: ---
L43: 
L44: ## When to Use Public Endpoints
L45: 
L46: The public endpoints are read-only verification and proof retrieval by receipt id:
L47: 
L48: - `GET /api/public/receipts/:id/verify`
L49: - `GET /api/public/receipts/:id/proof`
L50: 
L51: Use these when:
L52: - a receipt already exists
L53: - you want to display verification status or fetch proof material
L54: - you are building auditor / reader workflows
L55: 
L56: **Do not use public endpoints as a submission path.**
L57: Submission is privileged by design.
L58: 
L59: ---
L60: 
L61: ## Proxy Pattern: Reference Implementations
L62: 
L63: ### Express (Node) Proxy
L64: 
L65: Create a server endpoint in your client app (example: `/api/receipts/verify`) that forwards to AI Receipts.
L66: 
L67: **Environment variables (server-side only):**
L68: - `AI_RECEIPTS_API_URL` (example: `https://receipts.example.com`)
L69: - `AI_RECEIPTS_API_KEY` (your secret key)
L70: 
L71: ```javascript
L72: import express from "express";
L73: 
L74: const app = express();
L75: app.use(express.json({ limit: "1mb" }));
L76: 
L77: app.post("/api/receipts/verify", async (req, res) => {
L78:   const apiUrl = process.env.AI_RECEIPTS_API_URL;
L79:   const apiKey = process.env.AI_RECEIPTS_API_KEY;
L80: 
L81:   if (!apiUrl || !apiKey) {
L82:     return res.status(500).json({
L83:       error: "server_misconfigured",
L84:       message: "Missing AI_RECEIPTS_API_URL or AI_RECEIPTS_API_KEY"
L85:     });
L86:   }
L87: 
L88:   const upstream = await fetch(`${apiUrl}/api/verify`, {
L89:     method: "POST",
L90:     headers: {
L91:       "Content-Type": "application/json",
L92:       "x-api-key": apiKey
L93:     },
L94:     body: JSON.stringify(req.body)
L95:   });
L96: 
L97:   const contentType = upstream.headers.get("content-type") || "application/json";
L98:   const text = await upstream.text();
L99: 
L100:   res.status(upstream.status).type(contentType).send(text);
L101: });
L102: 
L103: app.listen(3000);
L104: ```
L105: 
L106: **Frontend code calls the proxy:**
L107: 
L108: ```javascript
L109: await fetch("/api/receipts/verify", {
L110:   method: "POST",
L111:   headers: { "Content-Type": "application/json" },
L112:   body: JSON.stringify(receiptCapsule)
L113: });
L114: ```
L115: 
L116: ---
L117: 
L118: ### Next.js (App Router) Proxy
L119: 
L120: Create a server route: `app/api/receipts/verify/route.ts`
L121: 
L122: ```typescript
L123: export async function POST(req: Request) {
L124:   const apiUrl = process.env.AI_RECEIPTS_API_URL;
L125:   const apiKey = process.env.AI_RECEIPTS_API_KEY;
L126: 
L127:   if (!apiUrl || !apiKey) {
L128:     return new Response(
L129:       JSON.stringify({ error: "server_misconfigured", message: "Missing AI_RECEIPTS_API_URL or AI_RECEIPTS_API_KEY" }),
L130:       { status: 500, headers: { "Content-Type": "application/json" } }
L131:     );
L132:   }
L133: 
L134:   const body = await req.text();
L135: 
L136:   const upstream = await fetch(`${apiUrl}/api/verify`, {
L137:     method: "POST",
L138:     headers: {
L139:       "Content-Type": "application/json",
L140:       "x-api-key": apiKey
L141:     },
L142:     body
L143:   });
L144: 
L145:   const text = await upstream.text();
L146:   return new Response(text, {
L147:     status: upstream.status,
L148:     headers: { "Content-Type": upstream.headers.get("content-type") ?? "application/json" }
L149:   });
L150: }
L151: ```
L152: 
L153: Frontend calls `/api/receipts/verify`.
L154: 
L155: ---
L156: 
L157: ## Security Requirements (Non-Negotiable)
L158: 
L159: ### Do
L160: 
L161: - Store verifier keys only in server-side secrets / environment variables.
L162: - Use a verify-only key where possible (least privilege).
L163: - Rate-limit your proxy endpoint.
L164: - Enforce payload size limits (receipt capsules should be small).
L165: - Log only request IDs / status codes; do not log raw capsule content by default.
L166: 
L167: ### Do Not
L168: 
L169: - Do not embed `x-api-key` in browser JS.
L170: - Do not place the API key inside the receipt capsule JSON.
L171: - Do not forward raw transcripts if your design claims "no transcript export."
L172: - Do not treat verifier output as "truth." It is integrity/authenticity checking only.
L173: 
L174: ---
L175: 
L176: ## Operational Guidance
L177: 
L178: ### Key Scope
L179: 
L180: If you support multiple keys, prefer:
L181: - **VERIFY_ONLY** keys for client integrations
L182: - **ADMIN** keys for internal tooling (saving, publishing, governance actions)
L183: 
L184: ### Rate Limiting
L185: 
L186: Proxy endpoints should be rate limited by:
L187: - IP
L188: - session/user identity
L189: - and/or receipt size
L190: 
L191: The verifier should be treated as an expensive, security-sensitive operation.
L192: 
L193: ---
L194: 
L195: ## Minimal Server-to-Server Test
L196: 
L197: From your client backend environment:
L198: 
L199: ```bash
L200: curl -X POST "$AI_RECEIPTS_API_URL/api/verify" \
L201:   -H "Content-Type: application/json" \
L202:   -H "x-api-key: $AI_RECEIPTS_API_KEY" \
L203:   -d @receipt_capsule.json
L204: ```
L205: 
L206: If authentication is correct, the verifier returns a structured JSON response (verification status + proof material as applicable).
L207: 
L208: ---
L209: 
L210: ## Integration Checklist
L211: 
L212: Before shipping your client integration, verify:
L213: 
L214: 1. [ ] API key is stored in server-side environment variables only
L215: 2. [ ] Frontend code calls your proxy endpoint, not the verifier directly
L216: 3. [ ] Proxy endpoint enforces rate limiting
L217: 4. [ ] Proxy endpoint enforces payload size limits (< 1MB)
L218: 5. [ ] No API key appears in browser devtools Network tab
L219: 6. [ ] No API key appears in client-side bundle (search for key value)
L220: 7. [ ] Error responses do not leak upstream authentication details
L221: 8. [ ] Logs do not contain raw transcript content
L222: 9. [ ] Public endpoints used only for read-only verification display
L223: 10. [ ] Team understands: verification proves integrity, not truth

--- FILE: docs/COMPETITIVE_COMPARISON.md ---
L1: # AI Receipts: Competitive Landscape
L2: 
L3: **Version**: 0.2.0  
L4: **Last updated**: 2026-02-12
L5: 
L6: ---
L7: 
L8: ## Category Definition
L9: 
L10: AI Receipts occupies the intersection of **AI governance tooling** and
L11: **cryptographic audit infrastructure**. The closest comparisons fall into
L12: three categories:
L13: 
L14: 1. AI observability platforms (monitoring AI model behavior)
L15: 2. Audit log / tamper-evidence systems (securing event records)
L16: 3. AI governance / compliance frameworks (policy and documentation)
L17: 
L18: No existing product combines all three in a single forensic verification
L19: system with offline-verifiable cryptographic proofs.
L20: 
L21: ---
L22: 
L23: ## Comparison Matrix
L24: 
L25: | Capability | AI Receipts | AI Observability (e.g. Langfuse, Helicone) | Audit/Immutable Logs (e.g. Immudb, Amazon QLDB) | AI Governance (e.g. Credo AI, IBM OpenPages) |
L26: |---|---|---|---|---|
L27: | SHA-256 hash chain | Yes (c14n-v1 canonical) | No | Yes (varies) | No |
L28: | Ed25519 signed checkpoints | Yes (with chain continuity) | No | No (DB-level integrity) | No |
L29: | Offline verification | Yes (standalone verifier) | No | No (DB-dependent) | No |
L30: | External anchoring (S3 WORM, TSA) | Yes (dual anchor) | No | Partial (DB-native) | No |
L31: | Forensic export packs | Yes (self-contained JSON) | Partial (data export) | Partial (query-based) | No |
L32: | LLM content observation | Yes (non-judgmental sensors) | Yes (evals, scoring) | No | Partial (policy checks) |
L33: | Kill switch (irreversible) | Yes | No | No | No |
L34: | Append-only interpretations | Yes (FACT/INTERPRETATION/UNCERTAINTY) | No | No | No |
L35: | Regulatory alignment mapping | Yes (6 frameworks) | No | Partial (SOC 2) | Yes (primary focus) |
L36: | Key rotation with proof | Yes (key ring + playbooks) | N/A | Partial | N/A |
L37: | Tamper detection + evidence | Yes (bit-level mutation) | No | Yes (DB-level) | No |
L38: | Post-quantum migration path | Documented (ML-DSA) | N/A | No | No |
L39: | Open proof bundle format | Yes (CI-generated, signed) | No | No | No |
L40: 
L41: ---
L42: 
L43: ## Detailed Comparisons
L44: 
L45: ### vs. AI Observability Platforms (Langfuse, Helicone, Weights & Biases)
L46: 
L47: **What they do well**: Real-time monitoring of LLM calls, token usage tracking,
L48: prompt/response logging, latency metrics, A/B testing of prompts, cost tracking.
L49: 
L50: **Where AI Receipts differs**:
L51: 
L52: - Observability platforms trust their own database. AI Receipts produces
L53:   cryptographic proofs that a third party can verify without trusting anyone.
L54: - Observability platforms evaluate output quality (scoring, evals). AI Receipts
L55:   explicitly avoids truth judgments, providing integrity verification only.
L56: - Observability platforms are designed for developers optimizing AI systems.
L57:   AI Receipts is designed for operators who need to prove records were not altered.
L58: - No observability platform produces a standalone artifact that can be independently
L59:   verified years later without access to the original system.
L60: 
L61: **Complementary**: AI Receipts can ingest transcripts logged by observability
L62: platforms and provide the cryptographic integrity layer they lack.
L63: 
L64: ### vs. Immutable Database Systems (Amazon QLDB, Immudb)
L65: 
L66: **What they do well**: Append-only ledgers with cryptographic verification,
L67: SQL query interfaces, built-in tamper detection, managed infrastructure.
L68: 
L69: **Where AI Receipts differs**:
L70: 
L71: - QLDB/Immudb verification requires access to the running database. AI Receipts
L72:   verification is fully offline with a standalone binary.
L73: - These systems provide generic immutable storage. AI Receipts adds domain-specific
L74:   features: interpretation taxonomy, kill switches, LLM sensors, forensic detectors.
L75: - External anchoring to S3 WORM and RFC3161 TSA provides trust boundaries independent
L76:   of the database provider. QLDB's trust boundary is Amazon itself.
L77: - AI Receipts publishes signed proof bundles through CI with reproducibility gates
L78:   and Sigstore signing. Database systems don't produce distributable proof artifacts.
L79: 
L80: **Complementary**: AI Receipts could use QLDB or Immudb as its storage backend
L81: while adding the AI-specific forensic layer on top.
L82: 
L83: ### vs. AI Governance Platforms (Credo AI, IBM OpenPages, Holistic AI)
L84: 
L85: **What they do well**: Policy management, risk assessment frameworks, bias
L86: detection, model documentation, compliance workflow automation, stakeholder
L87: reporting.
L88: 
L89: **Where AI Receipts differs**:
L90: 
L91: - Governance platforms focus on policy and process. AI Receipts focuses on
L92:   cryptographic evidence that policies were followed.
L93: - Governance platforms produce reports and dashboards. AI Receipts produces
L94:   verifiable proof artifacts.
L95: - Governance platforms require trust in the platform. AI Receipts proofs are
L96:   independently verifiable.
L97: - AI Receipts does not compete on policy management, risk scoring, or bias
L98:   detection. It provides the tamper-evident evidence layer that governance
L99:   frameworks can reference.
L100: 
L101: **Complementary**: Governance platforms define what should happen. AI Receipts
L102: proves what did happen, with cryptographic evidence that the record was not
L103: altered.
L104: 
L105: ---
L106: 
L107: ## Unique Positioning
L108: 
L109: AI Receipts is the only system that combines:
L110: 
L111: 1. **Cryptographic hash chain** with deterministic canonicalization (not just
L112:    database-level integrity)
L113: 2. **Signed checkpoints** with chain continuity and key rotation support
L114: 3. **Offline-verifiable proof bundles** that work without any network access
L115: 4. **External dual anchoring** to independent trust boundaries
L116: 5. **AI-domain-specific forensics** (LLM sensors, interpretation taxonomy,
L117:    kill switches) that generic audit systems lack
L118: 6. **Non-judgmental design** that proves integrity without claiming truth
L119: 
L120: The competitive moat is the combination: no single competitor addresses the
L121: full stack from cryptographic primitives through AI-specific forensic features
L122: to regulatory alignment documentation.
L123: 
L124: ---
L125: 
L126: ## Target Audience Contrast
L127: 
L128: | Audience | Typical Tool | AI Receipts |
L129: |---|---|---|
L130: | ML Engineer optimizing prompts | Langfuse, Helicone | Not primary audience |
L131: | Compliance Officer documenting controls | Credo AI, IBM OpenPages | Uses AI Receipts proofs as evidence |
L132: | Security Team verifying audit integrity | Immudb, QLDB, Splunk | Direct use case |
L133: | Legal/Regulatory requiring tamper-proof records | Custom solutions, paper trails | Direct use case |
L134: | Third-party auditor needing independent verification | Manual review, attestation | Verifier release + proof bundles |
L135: 
L136: ---
L137: 
L138: ## Limitations to Acknowledge
L139: 
L140: - AI Receipts does not provide real-time monitoring or dashboards (observability gap)
L141: - No built-in bias detection or fairness metrics (governance gap)
L142: - Single-operator model limits multi-tenant enterprise deployment
L143: - No managed SaaS offering (self-hosted only)
L144: - LLM sensor observations are descriptive, not evaluative (by design)
L145: 
L146: These are deliberate scope boundaries, not accidental omissions. See
L147: [NON_GOALS.md](NON_GOALS.md) for the full boundaries document.

--- FILE: docs/CRYPTO_AGILITY.md ---
L1: # Cryptographic Agility Plan
L2: 
L3: Version: 1.0
L4: Last Updated: 2026-02-12
L5: 
L6: This document describes the cryptographic primitives used by AI Receipts, the abstraction boundaries that enable algorithm migration, and the roadmap for post-quantum cryptography (PQC) readiness.
L7: 
L8: ---
L9: 
L10: ## Current Cryptographic Primitives
L11: 
L12: | Function | Algorithm | Key Size | Standard | Module |
L13: |----------|-----------|----------|----------|--------|
L14: | Hash chain | SHA-256 | 256-bit | FIPS 180-4 | `server/audit-canon.ts` |
L15: | Checkpoint signing | Ed25519 | 256-bit | RFC 8032 | `server/checkpoint-signer.ts` |
L16: | Canonicalization | c14n-v1 (custom) | N/A | Internal | `server/audit-canon.ts` |
L17: | Receipt verification | SHA-256 | 256-bit | FIPS 180-4 | `server/c14n.ts` |
L18: 
L19: ---
L20: 
L21: ## Abstraction Boundaries
L22: 
L23: ### Hash Function Abstraction
L24: 
L25: The hash function is isolated in `server/audit-canon.ts`:
L26: 
L27: ```typescript
L28: hashAuditPayload(payload: AuditPayload): string
L29: ```
L30: 
L31: All audit event hashing flows through this single function. To migrate to a new hash algorithm:
L32: 
L33: 1. Add a new `hashAuditPayloadV2()` function using the replacement algorithm
L34: 2. Update `payloadV` field to `2` for new events
L35: 3. The verifier already dispatches on `payloadV`, so old events remain verifiable
L36: 4. Update the offline verifier to support both hash versions
L37: 
L38: ### Signature Algorithm Abstraction
L39: 
L40: The checkpoint signer in `server/checkpoint-signer.ts` isolates all signing operations:
L41: 
L42: ```typescript
L43: signCheckpoint(payload: string): { signature: string; publicKeyId: string; signatureAlg: string }
L44: verifyCheckpointSignature(payload: string, signature: string, publicKey: string): boolean
L45: ```
L46: 
L47: The `signatureAlg` field is stored per-checkpoint, enabling mixed-algorithm verification. To migrate:
L48: 
L49: 1. Implement a new signer module (e.g., `checkpoint-signer-dilithium.ts`)
L50: 2. Set `signatureAlg` to the new algorithm identifier (e.g., `ML-DSA-65`)
L51: 3. Update the verifier to dispatch on `signatureAlg`
L52: 4. Old checkpoints remain verifiable with their original algorithm
L53: 
L54: ### Canonicalization Versioning
L55: 
L56: The `payloadV` field on every audit event records which canonicalization version produced the hash:
L57: 
L58: - `payloadV: 1` uses `auditPayloadV1()` with `stableStringifyStrict`
L59: - Future versions increment `payloadV` and use updated canonicalization
L60: 
L61: This ensures forward and backward compatibility: old events are always verifiable using their recorded canonicalization version.
L62: 
L63: ---
L64: 
L65: ## Migration Path: SHA-256 to SHA-3
L66: 
L67: SHA-256 is not known to be vulnerable, but SHA-3 (Keccak) provides algorithmic diversity.
L68: 
L69: ### Steps
L70: 
L71: 1. Add `hashAuditPayloadV2()` using SHA3-256 from Node.js `crypto` module
L72: 2. Increment `payloadV` to `2` for new events
L73: 3. Update CI golden tests to cover both v1 and v2 payloads
L74: 4. Update forensic pack manifest to list both algorithms
L75: 5. Update offline verifier to dispatch on `payloadV`
L76: 
L77: ### Compatibility
L78: 
L79: - Existing chain links remain SHA-256 (payloadV: 1)
L80: - New events use SHA3-256 (payloadV: 2)
L81: - The chain itself is not broken: each event stores its own hash, and `prevHash` references the prior event's hash regardless of algorithm
L82: - Segment verification checks `payloadV` before selecting the hash function
L83: 
L84: ### Timeline
L85: 
L86: - No immediate action required
L87: - Implement when SHA-256 deprecation guidance is issued by NIST
L88: 
L89: ---
L90: 
L91: ## Migration Path: Ed25519 to Post-Quantum Signatures
L92: 
L93: NIST standardized ML-DSA (FIPS 204, formerly CRYSTALS-Dilithium) in 2024 as the primary PQC signature scheme.
L94: 
L95: ### Phase 1: Dual-Sign Preparation (Current)
L96: 
L97: The `signatureAlg` field already supports per-checkpoint algorithm identification. No code changes needed for this phase.
L98: 
L99: ### Phase 2: Hybrid Signing
L100: 
L101: Implement dual signatures on each checkpoint:
L102: 
L103: ```
L104: signatureAlg: "Ed25519+ML-DSA-65"
L105: signature: base64(Ed25519_sig || ML-DSA-65_sig)
L106: ```
L107: 
L108: Verification succeeds only if both signatures are valid. This provides:
L109: - Backward compatibility (Ed25519 remains trusted)
L110: - Forward security (ML-DSA-65 protects against quantum adversaries)
L111: 
L112: ### Phase 3: PQC-Only Signing
L113: 
L114: Once confidence in ML-DSA is established:
L115: 
L116: ```
L117: signatureAlg: "ML-DSA-65"
L118: signature: base64(ML-DSA-65_sig)
L119: ```
L120: 
L121: Old checkpoints remain verifiable with their recorded `signatureAlg`.
L122: 
L123: ### Key Size Considerations
L124: 
L125: | Algorithm | Public Key | Signature | Security Level |
L126: |-----------|-----------|-----------|---------------|
L127: | Ed25519 | 32 bytes | 64 bytes | ~128-bit classical |
L128: | ML-DSA-44 | 1,312 bytes | 2,420 bytes | NIST Level 2 |
L129: | ML-DSA-65 | 1,952 bytes | 3,309 bytes | NIST Level 3 |
L130: | ML-DSA-87 | 2,592 bytes | 4,627 bytes | NIST Level 5 |
L131: 
L132: Recommendation: ML-DSA-65 (NIST Level 3) balances security and size for checkpoint signing.
L133: 
L134: ### Node.js Support
L135: 
L136: - Node.js does not yet natively support ML-DSA
L137: - When available in `crypto` module or via `liboqs` bindings, integrate as a drop-in signer
L138: - Monitor: https://github.com/nicktimko/liboqs-node and Node.js crypto roadmap
L139: 
L140: ### Timeline
L141: 
L142: | Phase | Target | Trigger |
L143: |-------|--------|---------|
L144: | Phase 1 (current) | Complete | Algorithm field already in schema |
L145: | Phase 2 (hybrid) | 2026-2027 | Node.js PQC support or liboqs bindings available |
L146: | Phase 3 (PQC-only) | 2028+ | Industry consensus on ML-DSA maturity |
L147: 
L148: ---
L149: 
L150: ## Hash Chain Integrity Under Algorithm Migration
L151: 
L152: The hash chain's integrity model supports algorithm transitions because:
L153: 
L154: 1. Each event stores its own `hash` and `payloadV`
L155: 2. The `prevHash` field references the prior event's stored hash, not a recomputed value
L156: 3. Verification replays use the `payloadV` to select the correct hash function
L157: 4. Chain continuity is maintained across algorithm boundaries
L158: 
L159: This means a chain can contain:
L160: ```
L161: Event 1: payloadV=1, hash=SHA-256(...)
L162: Event 2: payloadV=1, hash=SHA-256(...), prevHash=Event1.hash
L163: ...
L164: Event N: payloadV=2, hash=SHA3-256(...), prevHash=Event(N-1).hash
L165: ```
L166: 
L167: The transition event (N) uses the new algorithm but its `prevHash` still correctly references the prior event's SHA-256 hash.
L168: 
L169: ---
L170: 
L171: ## Canonicalization Stability
L172: 
L173: The `stableStringifyStrict` function in `server/audit-canon.ts` is the canonical serializer. It is protected by:
L174: 
L175: 1. **Golden tests**: 35 deterministic test cases in `golden-audit-chain.test.ts`
L176: 2. **CI drift guard**: `scripts/ci-canon-drift-guard.sh` detects unauthorized modifications
L177: 3. **Payload versioning**: `payloadV` field ensures old events use old canonicalization
L178: 
L179: Any change to canonicalization logic requires incrementing `payloadV` and adding a new payload builder.
L180: 
L181: ---
L182: 
L183: ## Threat Model Integration
L184: 
L185: See `docs/THREAT_MODEL.md` for cryptographic threat scenarios including:
L186: 
L187: - Hash collision attacks against SHA-256
L188: - Key compromise for Ed25519 checkpoint signing
L189: - Quantum computing threats to current primitives
L190: 
L191: This crypto agility plan addresses the "quantum computing" threat by providing a concrete migration path with no chain-breaking transitions.
L192: 
L193: ---
L194: 
L195: ## References
L196: 
L197: - NIST FIPS 180-4: Secure Hash Standard (SHA-256)
L198: - NIST FIPS 204: Module-Lattice-Based Digital Signature Standard (ML-DSA)
L199: - RFC 8032: Edwards-Curve Digital Signature Algorithm (Ed25519)
L200: - NIST SP 800-131A Rev. 2: Transitioning the Use of Cryptographic Algorithms
L201: - NIST AI 100-1: AI Risk Management Framework

--- FILE: docs/DEMO_README.md ---
L1: # HALO-RECEIPTS Demo Guide
L2: 
L3: **For evaluators, investors, and compliance reviewers.**
L4: 
L5: This guide walks you through the full verification flow in under 5 minutes. No trust required -- you can verify every claim yourself.
L6: 
L7: ---
L8: 
L9: ## What This System Does
L10: 
L11: HALO-RECEIPTS provides cryptographic verification for AI conversation transcripts. It proves whether a transcript has been tampered with, without exposing the transcript content.
L12: 
L13: **It does NOT:**
L14: - Judge whether AI responses are true or correct
L15: - Determine if conversations are complete
L16: - Moderate content
L17: - Monitor conversations in real time
L18: 
L19: See [PROOF_SPINE.md](./PROOF_SPINE.md) for the full invariant specification.
L20: 
L21: ---
L22: 
L23: ## 5-Minute Evaluator Walkthrough
L24: 
L25: ### Step 1: Open the App
L26: 
L27: Navigate to the deployed HALO-RECEIPTS application.
L28: 
L29: ### Step 2: Verify a Demo Receipt (ProofPack Lookup)
L30: 
L31: 1. On the **Verify** page, toggle the **"ProofPack lookup"** switch
L32: 2. Click the **"Try Demo"** button -- this pre-fills a known-good receipt ID
L33: 3. Click **Verify**
L34: 4. You should see three green badges:
L35:    - **HASH**: MATCH (SHA-256 integrity confirmed)
L36:    - **SIGNATURE**: Shows signature status
L37:    - **AUDIT CHAIN**: Shows chain link status
L38: 
L39: **What just happened**: The system looked up a pre-verified receipt and returned its cryptographic proof status. No transcript content was returned.
L40: 
L41: ### Step 3: Try the Proof-Gated Lantern
L42: 
L43: 1. Navigate to the **Lantern** page (sidebar)
L44: 2. Click **"Try Demo"** -- pre-fills the demo receipt ID
L45: 3. Type any question (e.g., "What was discussed?")
L46: 4. Click **Send**
L47: 5. Lantern responds because the receipt is VERIFIED
L48: 
L49: **What just happened**: Lantern checked the receipt's proof status before responding. If the receipt were tampered with or unverified, Lantern would refuse.
L50: 
L51: ### Step 4: Test the Proof Gate (Optional)
L52: 
L53: 1. On the **Lantern** page, clear the receipt ID
L54: 2. Enter a fake ID like `fake-receipt-999`
L55: 3. Type a question and click Send
L56: 4. Lantern refuses with a clear error: "Receipt not found"
L57: 
L58: This demonstrates the proof gate -- Lantern cannot be tricked into responding about unverified data.
L59: 
L60: ---
L61: 
L62: ## Verification Kit (HALO-RECEIPTS Repo)
L63: 
L64: For offline, independent verification:
L65: 
L66: ```bash
L67: git clone <repo-url>
L68: npm install
L69: npm test           # 72 tests -- golden tests, E2E, key rotation, anchoring
L70: npm run e2e        # End-to-end proof generation
L71: ```
L72: 
L73: The verification kit runs entirely offline. No API keys, no network calls, no trust assumptions.
L74: 
L75: ---
L76: 
L77: ## Architecture at a Glance
L78: 
L79: ```
L80: Receipt Capsule (JSON)
L81:        |
L82:        v
L83:   Canonicalize (c14n-v1)
L84:        |
L85:        v
L86:   SHA-256 Hash  -----> Compare with declared hash
L87:        |
L88:        v
L89:   Ed25519 Verify -----> Check against key registry
L90:        |
L91:        v
L92:   Chain Link ---------> Verify previous receipt hash
L93:        |
L94:        v
L95:   ProofPack (no transcript, proofs only)
L96:        |
L97:        v
L98:   Lantern (proof-gated conversations)
L99: ```
L100: 
L101: ---
L102: 
L103: ## What "VERIFIED" Means Operationally
L104: 
L105: | Check | Method | What It Proves |
L106: |-------|--------|----------------|
L107: | Hash | SHA-256 of c14n-v1 canonical form | Transcript bytes unchanged since capture |
L108: | Signature | Ed25519 with registered key | Signer identity matches registered key |
L109: | Chain | Previous receipt hash linkage | Ordering integrity across receipts |
L110: 
L111: **VERIFIED** = all three checks pass.
L112: 
L113: **VERIFIED does NOT prove**: truth, completeness, authorship intent, AI correctness.
L114: 
L115: ---
L116: 
L117: ## Key Endpoints
L118: 
L119: | Endpoint | Purpose | Auth Required |
L120: |----------|---------|---------------|
L121: | `GET /api/proofpack/:id` | Proof status lookup | None |
L122: | `POST /api/lantern/followup` | Proof-gated Q&A | None |
L123: | `GET /api/health` | System liveness | None |
L124: | `GET /api/ready` | Readiness + DB check | None |
L125: 
L126: ---
L127: 
L128: ## Security Properties
L129: 
L130: - **No transcript exposure**: ProofPack returns hashes and status, never message content
L131: - **Append-only audit trail**: Every operation is logged with hash chain
L132: - **Ed25519 signed checkpoints**: Periodic audit snapshots signed with asymmetric keys
L133: - **Rate limiting**: Per-IP burst and sustained limits on all endpoints
L134: - **Kill switch**: Irreversible lockout of individual receipts
L135: 
L136: ---
L137: 
L138: ## Questions?
L139: 
L140: See:
L141: - [PROOF_SPINE.md](./PROOF_SPINE.md) -- System invariants
L142: - [NON_GOALS.md](./NON_GOALS.md) -- What this system explicitly does not do
L143: - [OBJECTIONS_AND_PRECISE_ANSWERS.md](./OBJECTIONS_AND_PRECISE_ANSWERS.md) -- Stakeholder Q&A
L144: - [WHAT_THIS_PROVES.md](./WHAT_THIS_PROVES.md) -- Detailed proof claims

--- FILE: docs/DEMO_SPINE.md ---
L1: # Demo Spine — Operator Walkthrough (6-8 minutes)
L2: 
L3: **Audience:** CISO, GC, CTO, procurement reviewer
L4: **Premise:** This system proves *integrity of AI outputs/events* without claiming semantic truth. It provides tamper-evidence and operator-grade verification, not truth arbitration.
L5: 
L6: ---
L7: 
L8: ## Setup
L9: 
L10: All authenticated endpoints require the `x-api-key` header. In development, use `dev-test-key-12345`.
L11: 
L12: ```bash
L13: export API_KEY="dev-test-key-12345"
L14: export BASE="http://localhost:5000"
L15: ```
L16: 
L17: ---
L18: 
L19: ## Minute 0-1: Premise
L20: 
L21: > "This system proves integrity of AI outputs without claiming semantic truth. We do tamper-evidence and operator-grade verification, not truth arbitration."
L22: 
L23: Key distinction to establish upfront:
L24: - **"Verified"** means cryptographically intact, not factually correct
L25: - **"Broken"** means tampered or corrupted, not necessarily wrong
L26: 
L27: ---
L28: 
L29: ## Minute 1-2: Health vs Ready
L30: 
L31: ### Liveness (no DB dependency)
L32: 
L33: ```bash
L34: curl -s $BASE/api/health | jq .
L35: ```
L36: 
L37: Expected:
L38: ```json
L39: {
L40:   "status": "ok",
L41:   "time": "2026-02-12T...",
L42:   "version": "replit-node-verifier/0.1.0"
L43: }
L44: ```
L45: 
L46: **If you see this:** system process is alive. No database dependency.
L47: 
L48: ### Readiness (DB + audit head)
L49: 
L50: ```bash
L51: curl -s $BASE/api/ready | jq .
L52: ```
L53: 
L54: Expected (healthy):
L55: ```json
L56: {
L57:   "status": "ok",
L58:   "ready": true,
L59:   "time": "2026-02-12T...",
L60:   "version": "replit-node-verifier/0.1.0",
L61:   "db": { "ok": true },
L62:   "audit": { "ok": true }
L63: }
L64: ```
L65: 
L66: **If you see `"status": "degraded"`:** DB is up but audit head is inconsistent. System still serves traffic (anti-flap design prevents load balancer churn).
L67: 
L68: **If you get HTTP 503:** DB is unreachable. Only then does the system report not-ready.
L69: 
L70: ---
L71: 
L72: ## Minute 2-4: Audit Chain Verification
L73: 
L74: ### Verify full chain
L75: 
L76: ```bash
L77: curl -s -H "x-api-key: $API_KEY" "$BASE/api/audit/verify" | jq .
L78: ```
L79: 
L80: Expected (healthy chain):
L81: ```json
L82: {
L83:   "ok": true,
L84:   "status": "LINKED",
L85:   "checked": 42,
L86:   "checkedEvents": 42,
L87:   "totalEvents": 42,
L88:   "partial": false,
L89:   "head": { "seq": 42, "hash": "a1b2c3..." },
L90:   "expectedHead": { "seq": 42, "hash": "a1b2c3..." },
L91:   "firstBadSeq": null,
L92:   "break": null
L93: }
L94: ```
L95: 
L96: **Interpretation guide:**
L97: | Field | Meaning |
L98: |-------|---------|
L99: | `ok: true` | No integrity violations found |
L100: | `status: "LINKED"` | Multiple events, all hash-chained correctly |
L101: | `partial: false` | All events were checked (no cap hit) |
L102: | `firstBadSeq: null` | No corruption detected |
L103: 
L104: ### Verify a segment (cursor-based)
L105: 
L106: ```bash
L107: curl -s -H "x-api-key: $API_KEY" "$BASE/api/audit/verify?fromSeq=1&toSeq=10" | jq .
L108: ```
L109: 
L110: **If you see `"partial": true`:** Only the requested segment was verified, not the full chain. This is expected for cursor queries.
L111: 
L112: ### Detect tampering
L113: 
L114: After intentionally corrupting a row (e.g., modifying a hash in the DB), re-run verify:
L115: 
L116: ```bash
L117: curl -s -H "x-api-key: $API_KEY" "$BASE/api/audit/verify" | jq .
L118: ```
L119: 
L120: Expected (broken chain):
L121: ```json
L122: {
L123:   "ok": false,
L124:   "status": "BROKEN",
L125:   "checked": 5,
L126:   "checkedEvents": 5,
L127:   "totalEvents": 42,
L128:   "partial": false,
L129:   "head": { "seq": 5, "hash": "..." },
L130:   "firstBadSeq": 6,
L131:   "break": {
L132:     "seq": 6,
L133:     "reason": "hash_mismatch",
L134:     "expectedHash": "abc...",
L135:     "foundHash": "xyz..."
L136:   }
L137: }
L138: ```
L139: 
L140: **If you see `firstBadSeq: 6`:** The chain is intact through event 5, but event 6 has been tampered with. The system pinpoints exactly where corruption begins.
L141: 
L142: ---
L143: 
L144: ## Minute 4-5: Boundary Invariant (Trust Boundary)
L145: 
L146: ### The rule
L147: 
L148: Wire format (HTTP API) uses `observation_type` (snake_case).
L149: Internal code uses `observationType` (camelCase).
L150: Conversion happens ONLY in `server/llm/wire-boundary.ts`.
L151: 
L152: ### CI enforcement
L153: 
L154: The GitHub Actions CI pipeline includes a boundary drift guard:
L155: - Grep-based check that `observation_type` only appears in explicitly allowed files
L156: - Build fails if any new file uses `observation_type` outside the boundary
L157: 
L158: ### Test coverage (7 boundary tests)
L159: 
L160: - Valid wire values map correctly to internal enum
L161: - Invalid wire values throw at the boundary
L162: - Round-trip: internal -> wire -> internal produces identical value
L163: - No internal pipeline object contains `observation_type`
L164: - Adapter options correctly wrap the wire field
L165: 
L166: ---
L167: 
L168: ## Minute 5-6: Error Discipline / Operator Ergonomics
L169: 
L170: ### JSON 404 for unknown API routes
L171: 
L172: ```bash
L173: curl -s $BASE/api/nonexistent | jq .
L174: ```
L175: 
L176: Expected:
L177: ```json
L178: {
L179:   "error": {
L180:     "code": 404,
L181:     "message": "Not found",
L182:     "detail": "No route matches GET /api/nonexistent"
L183:   }
L184: }
L185: ```
L186: 
L187: **Why this matters:** No HTML fallback. Every `/api/*` route returns structured JSON, even on 404. External integrations never get confused by SPA HTML.
L188: 
L189: ### Rate limiting on verify
L190: 
L191: ```bash
L192: curl -s -D- -H "x-api-key: $API_KEY" "$BASE/api/audit/verify" 2>&1 | grep -i "x-ratelimit"
L193: ```
L194: 
L195: Expected headers:
L196: ```
L197: x-ratelimit-limit: 50
L198: x-ratelimit-remaining: 49
L199: x-ratelimit-reset: 1707700000
L200: ```
L201: 
L202: ### Metrics (operator telemetry)
L203: 
L204: ```bash
L205: curl -s -H "x-api-key: $API_KEY" "$BASE/api/health/metrics" | jq .
L206: ```
L207: 
L208: Expected:
L209: ```json
L210: {
L211:   "counters": {
L212:     "audit.append.ok": 42,
L213:     "audit.verify.ok": 3,
L214:     "http.GET./api/audit/verify.200": 3
L215:   }
L216: }
L217: ```
L218: 
L219: ---
L220: 
L221: ## Minute 6-8: Guarantees + Non-Goals
L222: 
L223: ### What this system guarantees
L224: 
L225: 1. **Integrity** — SHA-256 hash chain detects any modification, deletion, or reordering of audit events
L226: 2. **Reproducibility** — Deterministic canonicalization (`stableStringifyStrict`) ensures identical inputs always produce identical hashes
L227: 3. **Verification contract** — `ok: true` means the chain is intact; `ok: false` means it is not; `firstBadSeq` pinpoints where
L228: 4. **Partial coverage honesty** — `partial: true` is always reported when the full chain was not checked
L229: 5. **Boundary discipline** — Wire and internal naming conventions are enforced at build time
L230: 
L231: ### What this system does NOT guarantee
L232: 
L233: 1. **Semantic truth** — "Verified" means cryptographically intact, not factually correct
L234: 2. **Completeness beyond cap** — Server-side cap (50,000 events) means very large chains require segmented verification
L235: 3. **AI correctness** — LLM observations describe content; they never judge truth
L236: 4. **Full-privilege DB defense** — An admin who rewrites ALL rows + head simultaneously can forge a valid chain. External anchoring (WORM storage, signed checkpoints) is recommended for that threat level.
L237: 
L238: ---
L239: 
L240: ## Running the Demo
L241: 
L242: ```bash
L243: # From the project root:
L244: bash scripts/demo.sh
L245: ```
L246: 
L247: The script runs each step above with formatted output and pauses between sections.

--- FILE: docs/EXECUTIVE_SUMMARY.md ---
L1: # AI Receipts: Executive Summary
L2: 
L3: ## What It Is
L4: 
L5: AI Receipts is a forensic verification system that provides cryptographic proof that records of AI conversations have not been tampered with. Think of it as a tamper-evident seal for AI interaction logs.
L6: 
L7: ## The Problem
L8: 
L9: Organizations using AI assistants (for healthcare, legal, financial decisions) need to prove that the AI's output record is intact -- that nobody changed what the AI said after the fact. This is not about whether the AI was correct; it's about whether the record is trustworthy.
L10: 
L11: ## How It Works
L12: 
L13: **1. Every action creates a chained record.**  
L14: Each operator action (verify a receipt, add an interpretation, export data) is logged as an audit event. Each event includes a cryptographic hash of the previous event, forming an unbreakable chain. Modify any single record and every subsequent hash mismatches.
L15: 
L16: **2. Signed checkpoints anchor the chain.**  
L17: At regular intervals, the system creates Ed25519 signed checkpoints -- digital signatures that bind the chain to a verifiable key. These checkpoints are themselves chained, creating a secondary integrity layer.
L18: 
L19: **3. Offline verification requires no trust.**  
L20: A self-contained verifier (runs on any computer with Node.js) replays the entire chain independently. No database access, no API keys, no network connection required. If the chain is intact, it reports PASS. If any record was modified, it pinpoints the exact location of the tampering.
L21: 
L22: ## What It Proves (and What It Doesn't)
L23: 
L24: | It Proves | It Does Not Prove |
L25: |-----------|-------------------|
L26: | The record has not been modified since creation | The AI output was factually correct |
L27: | Events occurred in the recorded order | The conversation actually happened |
L28: | Checkpoints were signed by a specific key | The participants are who they claim to be |
L29: | Tampering is detectable and localized | The system is secure against all threats |
L30: 
L31: ## Key Differentiators
L32: 
L33: - **Self-contained verification**: No vendor lock-in. Download the verifier, run it yourself.
L34: - **Tamper detection, not prevention**: The system doesn't try to prevent tampering (impossible in software). Instead, it makes tampering detectable -- the honest version of the problem.
L35: - **No epistemic overreach**: The system never claims "this AI output is true." It claims "this record is intact." This distinction matters for regulatory compliance.
L36: 
L37: ## Regulatory Relevance
L38: 
L39: The system maps to requirements in:
L40: 
L41: - **21 CFR Part 11** (FDA electronic records): Audit trails, integrity, electronic signatures
L42: - **HIPAA**: Data integrity controls for healthcare-adjacent AI
L43: - **SOC 2 Type II**: Security, processing integrity, monitoring
L44: - **EU AI Act**: Traceability, transparency, accountability for AI systems
L45: - **ISO 27001**: Information security management controls
L46: 
L47: See the [Regulatory Matrix](./REGULATORY_MATRIX_EXCERPT.md) for the 10-row compliance officer quick reference.
L48: 
L49: ## Deliverables
L50: 
L51: | Artifact | What It Contains |
L52: |----------|-----------------|
L53: | Forensic Pack (JSON) | Complete audit trail with hash chain + signed checkpoints |
L54: | Proof Bundle | End-to-end evidence that the cryptographic machinery works |
L55: | Verifier Release (ZIP) | Standalone offline verifier + public key + README |
L56: | Documentation | Threat model, regulatory alignment, proof bundle spec |
L57: 
L58: ## Contact
L59: 
L60: For pilot discussions, technical deep-dives, or regulatory alignment reviews, reach out to the project team.

--- FILE: docs/EXTERNAL_ANCHORING.md ---
L1: # External Anchoring: What It Prevents and What It Does Not
L2: 
L3: **Version**: 0.2.0  
L4: **Last updated**: 2026-02-12
L5: 
L6: ---
L7: 
L8: ## Purpose
L9: 
L10: External anchoring pushes cryptographic evidence outside the primary system's
L11: trust boundary. If the database, operator account, or signing keys are
L12: compromised, external anchors provide independent proof of what existed and when.
L13: 
L14: ---
L15: 
L16: ## What External Anchoring Prevents
L17: 
L18: ### DB Superuser Rewrite (without collusion)
L19: 
L20: **Threat**: A database administrator rewrites audit events, recomputes the hash
L21: chain, and re-signs checkpoints with the same key.
L22: 
L23: **Without anchoring**: The rewritten chain is internally consistent and passes
L24: all offline verification.
L25: 
L26: **With S3 WORM anchoring**: The original checkpoint hashes are stored in an S3
L27: bucket with Object Lock. The rewritten checkpoints produce different hashes that
L28: do not match the S3 objects. Reconciliation fails.
L29: 
L30: **With RFC3161 TSA anchoring**: The original checkpoint hashes have third-party
L31: timestamps. Rewritten checkpoints would need new timestamps, revealing the
L32: modification timeline.
L33: 
L34: **Requirement for attack success**: Attacker must compromise both the database
L35: AND the S3 account (or TSA) -- collusion across independent trust boundaries.
L36: 
L37: ### Backdated Modifications
L38: 
L39: **Threat**: An operator modifies a receipt and claims it was always that way.
L40: 
L41: **With TSA anchoring**: Each checkpoint has a third-party timestamp that proves
L42: when the hash was first recorded. Backdating requires forging a TSA token from
L43: a trusted authority, which is infeasible.
L44: 
L45: ### Silent Key Rotation Attacks
L46: 
L47: **Threat**: An operator rotates the signing key, re-signs old checkpoints with
L48: the new key, and rewrites the chain.
L49: 
L50: **With anchoring**: Anchor receipts include the original `kid` (key identifier)
L51: and checkpoint hash. Even if checkpoints are re-signed, the anchor receipts
L52: preserve the original hashes and kid values.
L53: 
L54: ---
L55: 
L56: ## What External Anchoring Does NOT Prevent
L57: 
L58: ### Pre-Ingestion Tampering
L59: 
L60: Anchoring proves that data has not been modified *after* it was ingested and
L61: checkpointed. It cannot verify that data was accurately captured in the first
L62: place. If false data enters the system, anchoring preserves the false data
L63: with cryptographic integrity.
L64: 
L65: ### Malicious Operator (from day one)
L66: 
L67: If the operator controls both the signing key and the anchor account from the
L68: start, they can fabricate the entire chain including anchors. External anchoring
L69: adds collusion cost, not impossibility. The mitigation is organizational: anchor
L70: accounts should be under separate administrative control.
L71: 
L72: ### Anchor Provider Compromise
L73: 
L74: If the S3 account is compromised (including Object Lock bypass) or the TSA
L75: issues fraudulent tokens, the anchor trust boundary fails. Dual anchoring
L76: (S3 + TSA simultaneously) raises the bar by requiring compromise of both
L77: independent systems.
L78: 
L79: ### Selective Omission
L80: 
L81: An operator can choose not to anchor certain checkpoints. The verifier reports
L82: anchor coverage (e.g., "3/5 checkpoints anchored") to make gaps visible, but
L83: cannot force the operator to anchor everything.
L84: 
L85: ---
L86: 
L87: ## Anchor Backends
L88: 
L89: ### S3 Object Lock (WORM)
L90: 
L91: **Trust boundary**: AWS (or compatible S3 provider)
L92: 
L93: **Retention modes**:
L94: - **GOVERNANCE**: Can be bypassed by principals with `s3:BypassGovernanceRetention`.
L95:   Suitable for pilot/staging. The DB admin must NOT have this permission.
L96: - **COMPLIANCE**: Cannot be bypassed by anyone, including the root account.
L97:   Suitable for production when retention period is well-understood.
L98: 
L99: **What auditors will probe**:
L100: - "Can an admin bypass Object Lock?" -- Only with Governance mode + explicit
L101:   permission. The IAM policy must deny this.
L102: - "Is the S3 account the same as the DB account?" -- Ideally not. Separate
L103:   accounts prevent single-point compromise.
L104: 
L105: ### RFC3161 Timestamp Authority
L106: 
L107: **Trust boundary**: The TSA provider (e.g., DigiCert, FreeTSA, Sectigo)
L108: 
L109: **What it proves**: That a specific hash existed at a specific time, attested
L110: by a trusted third party.
L111: 
L112: **Offline verifiability**: Timestamp tokens can be verified against the TSA's
L113: certificate chain without contacting the TSA.
L114: 
L115: **What it does not prove**: That the hash corresponds to honest data. The TSA
L116: timestamps whatever hash you send it.
L117: 
L118: ---
L119: 
L120: ## Recommended Deployment
L121: 
L122: ### Pilot / Staging
L123: 
L124: ```
L125: CHECKPOINT_ANCHOR_TYPE=both
L126: CHECKPOINT_ANCHOR_S3_BUCKET=audit-anchors-staging
L127: CHECKPOINT_ANCHOR_S3_RETENTION_MODE=GOVERNANCE
L128: CHECKPOINT_ANCHOR_S3_RETENTION_DAYS=30
L129: CHECKPOINT_ANCHOR_TSA_URL=https://freetsa.org/tsr
L130: ```
L131: 
L132: ### Production
L133: 
L134: ```
L135: CHECKPOINT_ANCHOR_TYPE=both
L136: CHECKPOINT_ANCHOR_S3_BUCKET=audit-anchors-prod
L137: CHECKPOINT_ANCHOR_S3_RETENTION_MODE=COMPLIANCE
L138: CHECKPOINT_ANCHOR_S3_RETENTION_DAYS=365
L139: CHECKPOINT_ANCHOR_S3_CROSS_ACCOUNT_ID=<separate-aws-account-id>
L140: CHECKPOINT_ANCHOR_TSA_URL=https://timestamp.digicert.com
L141: CHECKPOINT_ANCHOR_TSA_FINGERPRINTS=sha256:<digicert-root-fingerprint>
L142: ```
L143: 
L144: ### CI / Proof Runs
L145: 
L146: ```
L147: # Dev: log-only (fast, no external dependencies)
L148: npx tsx scripts/proof_run.ts --anchors=optional
L149: 
L150: # Staging: require real anchors
L151: npx tsx scripts/proof_run.ts --anchors=required
L152: ```
L153: 
L154: ---
L155: 
L156: ## Minimal IAM Policy for S3 Anchor Account
L157: 
L158: The anchor account should have the minimum permissions needed:
L159: 
L160: ```json
L161: {
L162:   "Version": "2012-10-17",
L163:   "Statement": [
L164:     {
L165:       "Sid": "AllowAnchorWrite",
L166:       "Effect": "Allow",
L167:       "Action": [
L168:         "s3:PutObject",
L169:         "s3:PutObjectRetention",
L170:         "s3:GetObject",
L171:         "s3:GetObjectRetention",
L172:         "s3:ListBucket"
L173:       ],
L174:       "Resource": [
L175:         "arn:aws:s3:::audit-anchors-prod",
L176:         "arn:aws:s3:::audit-anchors-prod/*"
L177:       ]
L178:     },
L179:     {
L180:       "Sid": "DenyBypassGovernanceRetention",
L181:       "Effect": "Deny",
L182:       "Action": "s3:BypassGovernanceRetention",
L183:       "Resource": "arn:aws:s3:::audit-anchors-prod/*"
L184:     },
L185:     {
L186:       "Sid": "DenyDeleteAndOverwrite",
L187:       "Effect": "Deny",
L188:       "Action": [
L189:         "s3:DeleteObject",
L190:         "s3:DeleteObjectVersion",
L191:         "s3:PutBucketObjectLockConfiguration"
L192:       ],
L193:       "Resource": [
L194:         "arn:aws:s3:::audit-anchors-prod",
L195:         "arn:aws:s3:::audit-anchors-prod/*"
L196:       ]
L197:     }
L198:   ]
L199: }
L200: ```
L201: 
L202: **Key properties**:
L203: - Write-only: can create objects but not delete or overwrite
L204: - Cannot bypass governance retention
L205: - Cannot modify the Object Lock configuration itself
L206: - Separate from the DB admin's IAM principal
L207: 
L208: **Cross-account setup**: The anchor bucket should be in a separate AWS account.
L209: The DB system's IAM role is granted cross-account access via bucket policy,
L210: but the DB account has no ability to modify the bucket's Object Lock settings.
L211: 
L212: ---
L213: 
L214: ## Threat Model Delta
L215: 
L216: **Before external anchoring** (DB + signing key only):
L217: 
L218: | Attacker | Can they forge the chain? | Detection |
L219: |----------|--------------------------|-----------|
L220: | DB viewer | No (read-only) | N/A |
L221: | DB writer (no key) | No (can't re-sign) | Signature verification fails |
L222: | DB writer + key holder | Yes | None (internally consistent) |
L223: | DB admin (superuser) + key | Yes | None |
L224: 
L225: **After external anchoring** (DB + signing key + S3 WORM + TSA):
L226: 
L227: | Attacker | Can they forge the chain? | Detection |
L228: |----------|--------------------------|-----------|
L229: | DB viewer | No | N/A |
L230: | DB writer (no key) | No | Signature verification fails |
L231: | DB writer + key holder | No | Anchor hashes mismatch |
L232: | DB admin (superuser) + key | No | Anchor hashes mismatch |
L233: | DB admin + key + S3 account | Possible | TSA timestamps reveal backdating |
L234: | DB admin + key + S3 + TSA | Theoretically possible | Requires compromise of 4 independent systems |
L235: 
L236: The minimum collusion for undetectable forgery goes from **2 parties** (DB + key)
L237: to **4 parties** (DB + key + S3 account + TSA provider).

--- FILE: docs/FORENSIC_EXPORT_PACK.md ---
L1: # Forensic Export Pack
L2: 
L3: A portable evidence bundle that lets an external reviewer verify audit chain integrity without access to the running system.
L4: 
L5: ---
L6: 
L7: ## Quick Start
L8: 
L9: ### Export a pack
L10: 
L11: ```bash
L12: # Export all audit events
L13: npx tsx scripts/export_forensic_pack.ts --output my_pack.json
L14: 
L15: # Export a specific segment
L16: npx tsx scripts/export_forensic_pack.ts --from 1 --to 100 --output segment_pack.json
L17: ```
L18: 
L19: ### Verify a pack offline
L20: 
L21: ```bash
L22: # No database needed — pure cryptographic replay
L23: npx tsx scripts/verify_forensic_pack.ts my_pack.json
L24: 
L25: # With Ed25519 checkpoint signature verification
L26: npx tsx scripts/verify_forensic_pack.ts my_pack.json --public-key checkpoint_public.pem
L27: ```
L28: 
L29: Expected output (healthy):
L30: ```
L31: Forensic Pack Offline Verifier
L32: ==============================
L33: Format:      ai-receipts-forensic-pack/1.2
L34: Exported at: 2026-02-12T07:24:28.920Z
L35: Segment:     seq 1-23 (23 events)
L36: DB total:    23 events at export time
L37: Algorithm:   SHA-256
L38: 
L39: Pack integrity: OK (pack hash matches)
L40: 
L41: RESULT: PASS (hash chain)
L42:   Chain status: LINKED
L43:   Checked:      23/23 events
L44:   Segment:      seq 1-23
L45:   Head:         seq=23 hash=6584417cf0e22870...
L46:   Coverage:     FULL
L47:   Head match:   OK (matches head at export time)
L48: 
L49: Checkpoints:    2 found in pack
L50:   Chain:        2 checkpoints linked
L51:   Anchors:      2 checkpoint-event hashes match
L52:   Signatures:   2/2 Ed25519 signatures VERIFIED
L53: 
L54: Anchors:        2 anchor receipts in pack
L55:   Types:        log-only
L56:   Verified:     2/2 anchor hashes match
L57:   Log-only:     2 anchors (structured log only, no external trust boundary)
L58: ```
L59: 
L60: Expected output (tampered):
L61: ```
L62: FAIL: Pack integrity check failed.
L63:   Expected pack hash: 6494ebc4...
L64:   Computed pack hash: 21f3dfcf...
L65:   The pack file itself has been modified after export.
L66: ```
L67: 
L68: ---
L69: 
L70: ## Pack Format (v1.2)
L71: 
L72: ```json
L73: {
L74:   "format": "ai-receipts-forensic-pack/1.2",
L75:   "exportedAt": "ISO 8601 timestamp",
L76: 
L77:   "segment": {
L78:     "fromSeq": 1,
L79:     "toSeq": 23,
L80:     "eventCount": 23,
L81:     "totalEventsInDb": 23
L82:   },
L83: 
L84:   "headAtExportTime": {
L85:     "seq": 23,
L86:     "hash": "6584417c..."
L87:   },
L88: 
L89:   "verification": {
L90:     "algorithm": "SHA-256",
L91:     "canonicalization": "stableStringifyStrict (sorted keys, strict type rejection)",
L92:     "payloadVersion": 1,
L93:     "chainStatus": "LINKED",
L94:     "ok": true,
L95:     "checkedEvents": 23,
L96:     "firstBadSeq": null,
L97:     "breakReason": null
L98:   },
L99: 
L100:   "system": {
L101:     "semver": "0.2.0",
L102:     "commit": "abc123def456",
L103:     "engineId": "replit-node-verifier/0.2.0",
L104:     "auditPayloadVersion": 1
L105:   },
L106: 
L107:   "manifest": {
L108:     "toolVersion": "replit-node-verifier/0.2.0",
L109:     "exportScript": "scripts/export_forensic_pack.ts",
L110:     "verifyScript": "scripts/verify_forensic_pack.ts",
L111:     "hashAlgorithm": "SHA-256",
L112:     "signatureAlgorithm": "Ed25519",
L113:     "canonicalizationSpec": "Deterministic JSON: sorted keys, strict type rejection..."
L114:   },
L115: 
L116:   "events": [
L117:     {
L118:       "seq": 1,
L119:       "ts": "ISO 8601",
L120:       "action": "VERIFY_STORED",
L121:       "actor": "operator",
L122:       "receiptId": "uuid or null",
L123:       "exportId": "null",
L124:       "savedViewId": "null",
L125:       "payload": "{\"key\":\"value\"}",
L126:       "ip": "127.0.0.1",
L127:       "userAgent": "curl/8.0",
L128:       "prevHash": "GENESIS",
L129:       "hash": "sha256 hex",
L130:       "schemaVersion": "audit/1.1",
L131:       "payloadV": 1
L132:     }
L133:   ],
L134: 
L135:   "checkpoints": [
L136:     {
L137:       "id": "uuid",
L138:       "seq": 100,
L139:       "hash": "sha256 hex matching event at seq 100",
L140:       "ts": "ISO 8601",
L141:       "prevCheckpointId": "null or uuid",
L142:       "prevCheckpointHash": "null or sha256 hex",
L143:       "signatureAlg": "Ed25519",
L144:       "publicKeyId": "key identifier",
L145:       "signature": "base64 Ed25519 signature",
L146:       "signedPayload": "canonical string that was signed",
L147:       "eventCount": 100
L148:     }
L149:   ],
L150: 
L151:   "anchorReceipts": [
L152:     {
L153:       "anchorType": "log-only | s3-worm | rfc3161",
L154:       "anchorId": "unique anchor identifier",
L155:       "anchoredAt": "ISO 8601",
L156:       "anchorHash": "sha256 of canonical anchorPayload",
L157:       "anchorPayload": {
L158:         "_v": 1,
L159:         "engine_id": "replit-node-verifier/0.2.0",
L160:         "audit_payload_version": 1,
L161:         "checkpoint_id": "uuid",
L162:         "checkpoint_seq": 100,
L163:         "event_seq": 100,
L164:         "event_hash": "sha256 hex",
L165:         "checkpoint_hash": "sha256 hex",
L166:         "kid": "key identifier",
L167:         "created_at": "ISO 8601"
L168:       },
L169:       "checkpointId": "uuid",
L170:       "checkpointSeq": 100,
L171:       "proof": {}
L172:     }
L173:   ],
L174: 
L175:   "packHash": "sha256 of the pack JSON before this field was added"
L176: }
L177: ```
L178: 
L179: ---
L180: 
L181: ## Verification Algorithm
L182: 
L183: The offline verifier replays the exact same hash chain algorithm used by the running system:
L184: 
L185: ### Step 1: Pack integrity
L186: 
L187: Recompute SHA-256 of the pack JSON (without the `packHash` field). If it doesn't match `packHash`, the file has been modified.
L188: 
L189: ### Step 2: Sequence continuity
L190: 
L191: Events must have contiguous sequence numbers starting from `fromSeq`.
L192: 
L193: ### Step 3: Hash chain replay
L194: 
L195: For each event:
L196: 
L197: 1. Build the canonical payload using `auditPayloadV1()`:
L198:    ```json
L199:    {
L200:      "_v": 1,
L201:      "schemaVersion": "audit/1.1",
L202:      "seq": N,
L203:      "ts": "...",
L204:      "action": "...",
L205:      "actor": "...",
L206:      "receiptId": null,
L207:      "exportId": null,
L208:      "savedViewId": null,
L209:      "payload": { parsed JSON },
L210:      "ip": "...",
L211:      "userAgent": "...",
L212:      "prevHash": "..."
L213:    }
L214:    ```
L215: 
L216: 2. Canonicalize using `stableStringifyStrict()`:
L217:    - Sort all object keys alphabetically
L218:    - Reject undefined, BigInt, Date, Map, Set, RegExp