Good — this is now conclusive enough to issue a contract audit.

Contract Audit — Status

✅ VERIFIED proof (PASS)

You have a reproducible verified-claim count:
	•	out/13/claims.json exists
	•	claims_total = 30
	•	verified_claims = 21

That is a hard, computed fact from your run output.

⸻

❌ EvidencePack emission (FAIL)

You searched for:
	•	evidence_pack*.json
	•	*evidence*pack*.json
	•	evidence*.json

…and found nothing.

Also important: the only “pack-like” JSON candidates under out/13/** are:
	•	out/13/claims.json (keys: claims,is_replit,mode,run_id)
	•	out/13/target_howto.json (contains unknowns, but not verified/metrics/verified_structural)

Your out/13/packs/ contains only .txt packs:
	•	code_pack.txt
	•	config_pack.txt
	•	docs_pack.txt
	•	ops_pack.txt

So the EvidencePack is not being emitted at all as a JSON artifact. This is a direct contract failure.

⸻

❌ Diff report artifact (FAIL)

You searched up to depth 5 for:
	•	DIFF_REPORT.md
	•	*diff*.md
	•	*diff*.json

…and found nothing.

So the diff artifact is also not being emitted (at least anywhere under out/).

⸻

What this means (precise)

Right now, your analyzer produces:
	•	claims.json
	•	coverage.json
	•	index.json
	•	DOSSIER.md
	•	target_howto.json
	•	text packs

…but does not produce the contractual EvidencePack JSON or any Diff report.

So any downstream tooling expecting:
	•	evidence_pack_version
	•	verified
	•	verified_structural
	•	unknowns
	•	metrics

…has nothing canonical to consume.

⸻

Immediate Fix (Zero-ambiguity): emit EvidencePack v1 from existing outputs

Even if you haven’t implemented “structural verification” yet, you can still emit an EvidencePack v1 with partials clearly marked (that’s acceptable in your governance style, as long as it’s explicit).

Step A — Add a post-run “emit_evidence_pack_v1()” writer

Emit: out/<run_id>/evidence_pack.v1.json

Minimum fields to include (v1 skeleton):
	•	evidence_pack_version: "v1"
	•	run_id
	•	verified: object with claims_total, verified_claims, and optionally verified_ratio
	•	verified_structural: object with explicit booleans + notes (even if all false right now)
	•	unknowns: pull from target_howto.json["unknowns"] (already exists)
	•	metrics: pull from coverage.json + any other counters you already track
	•	artifacts: paths to claims.json, coverage.json, index.json, DOSSIER.md, packs/*

Step B — Emit a Diff report OR explicitly emit “diff_absent”

If no diff engine exists yet, still emit:
	•	out/<run_id>/DIFF_REPORT.md containing:
	•	“diff not executed”
	•	why
	•	how to run it
	•	what files would be compared

This preserves contract shape and prevents silent drift.

⸻

“Stopgap” script you can run right now to produce the missing EvidencePack (no code edits needed)

This is a practical bridge while you wire it into the pipeline.

Run:

python - <<'PY'
import json, os, glob

run = os.popen("ls -1 out | sort -n | tail -n 1").read().strip()
root = f"out/{run}"

claims_path = f"{root}/claims.json"
coverage_path = f"{root}/coverage.json"
index_path = f"{root}/index.json"
howto_path = f"{root}/target_howto.json"

claims = json.load(open(claims_path))
coverage = json.load(open(coverage_path)) if os.path.exists(coverage_path) else {}
index = json.load(open(index_path)) if os.path.exists(index_path) else {}
howto = json.load(open(howto_path)) if os.path.exists(howto_path) else {}

items = claims.get("claims", claims) if isinstance(claims, dict) else claims

def ok(x):
    e = x.get("evidence", []) if isinstance(x, dict) else []
    return any(isinstance(i, dict) and i.get("snippet_hash_verified") is True for i in e)

claims_total = sum(1 for x in items if isinstance(x, dict))
verified_claims = sum(1 for x in items if isinstance(x, dict) and ok(x))

pack = {
    "evidence_pack_version": "v1",
    "run_id": run,
    "verified": {
        "claims_total": claims_total,
        "verified_claims": verified_claims,
        "verified_ratio": (verified_claims / claims_total) if claims_total else None,
        "note": "Claim verification derived from claims[].evidence[].snippet_hash_verified."
    },
    "verified_structural": {
        "present": False,
        "note": "Structural verification not emitted by analyzer in this run; placeholder emitted to preserve contract."
    },
    "unknowns": howto.get("unknowns", []),
    "metrics": {
        "coverage": coverage,
        "index_summary": {k: index.get(k) for k in list(index.keys())[:30]}
    },
    "artifacts": {
        "claims_json": claims_path,
        "coverage_json": coverage_path if os.path.exists(coverage_path) else None,
        "index_json": index_path if os.path.exists(index_path) else None,
        "dossier_md": f"{root}/DOSSIER.md" if os.path.exists(f"{root}/DOSSIER.md") else None,
        "packs": sorted(glob.glob(f"{root}/packs/*"))
    }
}

out_path = f"{root}/evidence_pack.v1.json"
with open(out_path, "w") as f:
    json.dump(pack, f, indent=2)
print("WROTE", out_path)
PY

Then verify it exists:

ls -la out/13/evidence_pack.v1.json
head -n 80 out/13/evidence_pack.v1.json

That instantly unblocks any consumer expecting the EvidencePack.

⸻

One small operational note

You saw:

find: warning: you have specified the global option -maxdepth after the argument -type

That warning doesn’t break anything, but your find command is indeed malformed. The correct form is:

find out/13/packs -maxdepth 3 -type f -print


⸻

Bottom line

Right now the run is partially compliant:
	•	✅ claim verification metric is real and reproducible
	•	❌ EvidencePack JSON artifact is missing
	•	❌ Diff report artifact is missing

If you paste the top keys of coverage.json and index.json, I’ll tell you exactly what should be promoted into metrics and verified_structural so v1 is clean and stable.