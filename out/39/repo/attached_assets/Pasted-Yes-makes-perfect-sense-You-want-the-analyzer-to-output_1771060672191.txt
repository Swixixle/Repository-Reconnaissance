Yes—makes perfect sense. You want the analyzer to output **an operator’s manual for the target program**.

So the **Program Totality Analyzer** is doing two jobs at once:

1. **Explain what the target system is** (identity/architecture/capabilities/risk)
2. **Explain how to use the target system** (install, configure, run, integrate, verify, troubleshoot)

That “how to use it” is **about the program being analyzed**, not about our analyzer.

Key constraint: it can only “know how” to the degree the repo contains evidence (README, scripts, Dockerfile, CI, etc.). When missing, it must say **Unknown** and output “what evidence would be needed.”

Below is the corrected Replit Agents packet with that clarified and made central.

---

# REPLIT AGENTS — INITIAL INSTRUCTIONS (Blank Slate, Target-Program Operator Manual Edition)

## Project Name

**Program Totality Analyzer** (System Demystifier + Operator Manual Generator)

## Mission

Build a tool that ingests a software project (initially: **GitHub repo URL or uploaded ZIP**) and outputs an evidence-cited dossier that demystifies:

* **WHAT the target program/system is**
* **HOW the target program/system is used** (operator manual)
* **WHAT it would take to deploy/integrate/operate it**

This is not a summarizer. It is an **evidence-first analyzer**. No hallucinated certainty.

---

## Non-Negotiables

1. **Evidence-first**: every claim and every “how-to step” must cite evidence (file path + line range or stable snippet id). If no evidence, label as **inference/unknown** and lower confidence.
2. **Coverage accounting**: output what was scanned vs skipped. No “totality” if coverage is partial.
3. **Operator manual is mandatory**: The dossier must include **How to Use the Target System** with actionable instructions.
4. **Skeptic gating**: enforce confidence caps/downgrades deterministically in code.

---

## V1 Deliverable (must ship)

CLI-first analyzer.

Run:

```bash
python analyzer_cli.py analyze <repo_url_or_local_path>
```

Creates:

```
out/<run_id>/
  DOSSIER.md
  claims.json
  coverage.json
  index.json
  target_howto.json
  packs/
    docs_pack.txt
    api_pack.txt
    data_pack.txt
    sec_pack.txt
    ops_pack.txt
```

### Output definitions

* **DOSSIER.md**: human-readable, demystifying dossier about the **target program**
* **claims.json**: structured claims about the target program
* **target_howto.json**: structured “how to use the target program” manual (install/run/config/integrate/verify/troubleshoot)
* **coverage.json**: what was scanned/skipped and why
* **index.json**: inventory + detected manifests/entrypoints
* **packs/**: evidence packs with embedded line numbers

---

## Canonical DOSSIER Sections (must match)

1. **Identity of Target System** (what it is / is not)
2. **Purpose & Jobs-to-be-done**
3. **Capability Map**
4. **Architecture Snapshot**
5. **How to Use the Target System** ✅ (operator manual)
6. **Integration Surface**
7. **Data & Security Posture**
8. **Operational Reality**
9. **Maintainability & Change Risk**
10. **Perspective Layer**
11. **Receipts** (explicit vs inferred + evidence index)

---

## “How to Use the Target System” Requirements (Core Goal)

This is the meta part: our app must produce **instructions for using the system it analyzed**.

### target_howto.json schema (minimum)

* `prereqs[]` — required tools (node/python/docker/etc)
* `install_steps[]` — install/build commands
* `config[]` — env vars, config files, secrets, where referenced
* `run_dev[]` — start dev mode
* `run_prod[]` — start prod/deploy (docker/systemd/cloud), or “unknown”
* `usage_examples[]` — example CLI commands / API calls / workflows
* `integration_steps[]` — what an integrator must do (headers, schemas, endpoints, SDK usage)
* `verification_steps[]` — smoke tests: `curl`, unit tests, health endpoints, “verify” commands
* `common_failures[]` — symptom → cause → fix (evidence-cited if present; otherwise mark inference)
* `unknowns[]` — missing instructions and what evidence is needed to complete them

### Evidence sources (in priority order)

1. README “Installation / Usage / Quickstart / Getting Started”
2. package manifests (`package.json` scripts, `pyproject`, `requirements.txt`)
3. Makefile / Taskfile
4. Dockerfile / docker-compose
5. CI workflows (real build/test/run commands)
6. scripts/ folder
7. `.env.example` / config templates
8. inline help (`--help` references in code; CLI parsers)

### Absolute rule

If you cannot cite it, do not invent it. Mark it unknown and request evidence.

---

## Hard Contract: Skeptic Confidence Caps (apply to target_howto too)

| Condition                                | Confidence Cap | Label                  |
| ---------------------------------------- | -------------: | ---------------------- |
| Docs-only (no code/config corroboration) |           0.20 | Pure Intent            |
| How-to step lacks evidence               |           0.20 | How-To Speculation     |
| Touches unscanned area                   |           0.30 | Unverified Territory   |
| Security/crypto claim w/o direct pointer |           0.10 | Security Vibe          |
| Docs vs code conflict                    |           0.05 | Critical Contradiction |
| Weasel words/adjectives                  |           0.40 | Marketing Fluff        |

Enforcement rule: confidence = min(original, all caps).

---

## Tech Stack (V1)

* Python 3
* Libraries: `typer`, `rich`, `pydantic`, `jsonschema`, `python-dotenv`
* GitHub acquisition via `git clone` (private via `GITHUB_TOKEN`)
* No UI required for V1; CLI must work first.

---

## Repo Structure to Create (exact)

```
.
├── README.md
├── pyproject.toml
├── analyzer_cli.py
├── src/
│   ├── core/
│   │   ├── acquire.py
│   │   ├── index.py
│   │   ├── extract.py
│   │   ├── coverage.py
│   │   ├── target_howto.py    # extracts “how to use the TARGET system”
│   │   ├── claims.py
│   │   ├── synth.py
│   │   └── util.py
│   ├── lenses/
│   │   ├── cartographer.py
│   │   ├── product.py
│   │   ├── integrator.py
│   │   ├── security.py
│   │   ├── operator.py
│   │   └── skeptic.py
│   └── prompts/
│       ├── synthesizer.md
│       └── skeptic.md
├── schemas/
│   ├── claims.schema.json
│   ├── coverage.schema.json
│   ├── index.schema.json
│   └── target_howto.schema.json
└── out/ (gitignored)
```

---

## Implementation Plan (agents execute in order)

### Step 1 — Scaffold + CLI

* `pyproject.toml` dependencies
* `analyzer_cli.py` with Typer command `analyze <target>`

### Step 2 — Acquire + Index + Coverage

* clone/local
* generate `index.json`, `coverage.json`

### Step 3 — Evidence Packs

* create `docs_pack`, `api_pack`, `data_pack`, `sec_pack`, `ops_pack`
* each pack: headers + embedded line numbers + manifest mapping

### Step 4 — Extract “How to Use the Target System” (core/target_howto.py) ✅

* parse docs_pack for install/run/usage steps
* parse package.json scripts, Makefile, Docker configs
* parse CI workflows for real commands
* detect env vars from `.env.example` and code references
* emit `target_howto.json` with evidence pointers for each step
* populate `unknowns[]` for missing instructions

### Step 5 — Claims + Skeptic Enforcement

* generate baseline claims from manifests/packs
* implement skeptic caps and output `skeptic_report.json`
* synthesis must apply skeptic downgrades

### Step 6 — Synthesis to DOSSIER.md

* render all 11 sections
* “How to Use the Target System” must read like an operator manual
* include “Unknowns / What’s missing to be usable” explicitly

---

## Definition of Done (V1)

* `analyze <repo>` outputs all artifacts
* Dossier includes **operator manual for the TARGET system**
* No invented how-to steps
* Unknowns are explicit + missing evidence requests listed
* Skeptic caps applied deterministically

---

## Immediate Agent Task Assignments

1. Agent A: scaffold/CLI
2. Agent B: acquire/index/coverage
3. Agent C: evidence packs + `target_howto.json`
4. Agent D: claims + skeptic enforcement + dossier rendering

---

## Tone Requirement

The dossier should feel like:

* “Here’s what this system is.”
* “Here’s how you actually run/use it.”
* “Here’s what’s missing if you tried today.”
* “Here’s where the risk is.”

No hype. No vague adjectives.

---

If the repo doesn’t contain enough evidence to say how to run it, the correct output is: **Unknown**, plus a clear request for what’s missing (README steps, Dockerfile, env example, etc.).

That’s the product.

---

If you want, I can also add one more requirement: the analyzer should produce a **“Quickstart Script”** (generated bash commands) only when confidence is high and evidence exists (otherwise it refuses).
