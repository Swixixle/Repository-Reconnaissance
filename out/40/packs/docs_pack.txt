
--- FILE: Dockerfile ---
L1: FROM node:20-slim AS base
L2: WORKDIR /app
L3: 
L4: FROM base AS deps
L5: COPY package.json package-lock.json ./
L6: RUN npm ci --ignore-scripts
L7: 
L8: FROM base AS build
L9: COPY --from=deps /app/node_modules ./node_modules
L10: COPY . .
L11: RUN npm run build
L12: 
L13: FROM base AS runtime
L14: COPY --from=deps /app/node_modules ./node_modules
L15: COPY --from=build /app/dist ./dist
L16: COPY package.json ./
L17: ENV NODE_ENV=production
L18: EXPOSE 5000
L19: HEALTHCHECK --interval=30s --timeout=5s --start-period=10s \
L20:   CMD curl -f http://localhost:5000/health || exit 1
L21: CMD ["npm", "start"]

--- FILE: README.md ---
L1: # Program Totality Analyzer
L2: 
L3: A static-artifact-anchored analysis tool that generates technical dossiers for software projects. It extracts what a system is, how to run it, what it needs, and what it cannot determine — with every claim citing `file:line` evidence backed by SHA-256 snippet hashes.
L4: 
L5: **Scope limitation:** PTA analyzes static artifacts only (source files, config, lockfiles). It does not observe runtime behavior, prove correctness, or guarantee security. Claims labeled VERIFIED mean "anchored to a hash-verified source snippet," not "proven true at runtime."
L6: 
L7: ## What It Does
L8: 
L9: Given a software project (GitHub repo, local folder, or Replit workspace), the analyzer produces:
L10: 
L11: | File | Contents |
L12: |------|----------|
L13: | `operate.json` | Operator dashboard: boot commands, integration points, deployment config, readiness scores, gaps with severity. Deterministic, evidence-bound. Every item is EVIDENCED, INFERRED, or UNKNOWN. |
L14: | `target_howto.json` | Legacy: evidence-scoped run steps. Prefer `operate.json` for operator workflows. |
L15: | `claims.json` | Verifiable claims about the system, each with file:line evidence and confidence scores |
L16: | `coverage.json` | Scan metadata: files scanned, files skipped, Replit detection evidence |
L17: | `replit_profile.json` | Replit-specific: port binding, secrets, external APIs, observability (only in Replit mode) |
L18: | `DOSSIER.md` | Human-readable markdown dossier summarizing all findings |
L19: | `index.json` | Full file index of scanned files |
L20: | `packs/` | Evidence packs (docs, config, code, ops) used during analysis |
L21: 
L22: ## Live Static CI Feed
L23: 
L24: Event-driven static analysis triggered by GitHub push and pull request events. When a webhook fires, PTA creates a CI run, shallow-clones the repo at the exact commit SHA, runs the analyzer, and stores artifacts under `out/ci/<run_id>/`. A web UI at `/ci` shows run status and results.
L25: 
L26: ### What it is
L27: 
L28: - Automated static analysis triggered by GitHub push/PR webhooks
L29: - Creates `ci_runs` + `ci_jobs` records, processes them via a background worker, stores artifacts under `out/ci/<run_id>/`
L30: - Provides a searchable UI feed at `/ci` with real-time polling
L31: - Supports manual enqueue for testing without webhooks
L32: - Deduplicates: same owner/repo/SHA within 6 hours reuses the existing run
L33: 
L34: ### What it is NOT
L35: 
L36: - Not runtime monitoring, tracing, or telemetry
L37: - Not a security scanner or SCA tool
L38: - Not a CI/CD runner replacement (it does not build, test, or deploy your code)
L39: - Not a compliance certification tool
L40: 
L41: ### Quick Setup
L42: 
L43: 1. **Set secrets**: `GITHUB_WEBHOOK_SECRET` (required), `GITHUB_TOKEN` (for private repos)
L44: 2. **Add GitHub webhook**: Point to `https://<your-app-domain>/api/webhooks/github` with content type `application/json`, secret matching your env var, events: Push + Pull requests
L45: 3. **Worker runs automatically**: The background worker starts on server boot and polls every 5 seconds
L46: 4. **View results**: Navigate to `/ci` in the web UI
L47: 
L48: See `replit.md` for detailed setup and troubleshooting, or `docs/API.md` for endpoint documentation.
L49: 
L50: ### API Endpoints
L51: 
L52: | Method | Path | Purpose |
L53: |--------|------|---------|
L54: | `POST` | `/api/webhooks/github` | GitHub webhook receiver (HMAC-SHA256 verified) |
L55: | `GET` | `/api/ci/runs?owner=&repo=&limit=` | List CI runs for a repo |
L56: | `GET` | `/api/ci/runs/:id` | Get single CI run details |
L57: | `POST` | `/api/ci/enqueue` | Manual trigger for testing |
L58: | `POST` | `/api/ci/worker/tick` | Process one queued job (fallback) |
L59: | `GET` | `/api/ci/health` | Job counts by status + last completed run |
L60: 
L61: ### Environment Variables
L62: 
L63: | Variable | Required | Purpose |
L64: |----------|----------|---------|
L65: | `GITHUB_WEBHOOK_SECRET` | Yes (for webhooks) | HMAC-SHA256 signature verification |
L66: | `GITHUB_TOKEN` | For private repos | Git clone authentication |
L67: | `CI_TMP_DIR` | No (default: `/tmp/ci`) | Temporary directory for cloned repos |
L68: | `ANALYZER_TIMEOUT_MS` | No (default: 600000) | Analyzer process timeout in ms |
L69: 
L70: ### Data Model
L71: 
L72: - **`ci_runs`**: `id` (uuid), `repoOwner`, `repoName`, `ref`, `commitSha`, `eventType`, `status` (QUEUED/RUNNING/SUCCEEDED/FAILED), `createdAt`, `startedAt`, `finishedAt`, `error`, `outDir`, `summaryJson`
L73: - **`ci_jobs`**: `id` (uuid), `runId` (fk), `status` (READY/LEASED/DONE/DEAD), `attempts`, `leasedUntil`, `lastError`
L74: 
L75: ### Operational Notes
L76: 
L77: - **Deduplication**: Same (owner, repo, SHA) within 6 hours returns the existing run instead of creating a duplicate
L78: - **Retry logic**: Jobs retry up to 3 attempts. After 3 failures, the job is marked DEAD and the run is marked FAILED
L79: - **Job leasing**: Uses `FOR UPDATE SKIP LOCKED` with 5-minute lease timeout for concurrency safety
L80: - **Signature verification**: Validates `X-Hub-Signature-256` header using HMAC-SHA256 with timing-safe comparison
L81: 
L82: ## Install
L83: 
L84: ```bash
L85: pip install -e .
L86: ```
L87: 
L88: This registers the `pta` command. Alternatively, run as a module or directly:
L89: 
L90: ```bash
L91: python -m server.analyzer.src --help
L92: python server/analyzer/analyzer_cli.py --help
L93: ```
L94: 
L95: ### Dependencies
L96: 
L97: - Python 3.11+
L98: - Required packages: `typer`, `rich`, `openai`, `gitpython`, `jsonschema`, `python-dotenv`, `pydantic`
L99: 
L100: ## Usage
L101: 
L102: ### Three Modes
L103: 
L104: **GitHub repository:**
L105: ```bash
L106: pta analyze https://github.com/user/repo -o ./output
L107: ```
L108: 
L109: **Local folder:**
L110: ```bash
L111: pta analyze ./path/to/project -o ./output
L112: ```
L113: 
L114: **Replit workspace (run from inside the workspace):**
L115: ```bash
L116: pta analyze --replit -o ./output
L117: ```
L118: 
L119: ### Deterministic Mode (`--no-llm`)
L120: 
L121: Skip all LLM calls and produce only deterministic, structurally-extracted outputs:
L122: 
L123: ```bash
L124: pta analyze --replit --no-llm -o ./output
L125: ```
L126: 
L127: This mode requires no API keys and produces reproducible results. It generates `operate.json` and readiness scoring without any LLM involvement. It extracts:
L128: - Package scripts (dev, build, start)
L129: - Lockfile-based install commands
L130: - Environment variable references (names only, never values)
L131: - Port binding configuration
L132: - External API usage
L133: - Replit platform detection
L134: - Operational gaps with severity ratings
L135: - Readiness scores (boot, integrate, deploy)
L136: 
L137: ### With LLM Analysis
L138: 
L139: For semantic analysis (architecture understanding, risk assessment, integration patterns):
L140: 
L141: ```bash
L142: pta analyze --replit -o ./output
L143: ```
L144: 
L145: Requires `AI_INTEGRATIONS_OPENAI_API_KEY` and `AI_INTEGRATIONS_OPENAI_BASE_URL` environment variables.
L146: 
L147: ### Scoping a Subdirectory
L148: 
L149: ```bash
L150: pta analyze https://github.com/user/monorepo --root packages/api -o ./output
L151: ```
L152: 
L153: ## Evidence Model
L154: 
L155: Every claim in the output cites structured evidence:
L156: 
L157: ```json
L158: {
L159:   "path": "server/index.ts",
L160:   "line_start": 92,
L161:   "line_end": 92,
L162:   "snippet_hash": "75d345a78f84",
L163:   "display": "server/index.ts:92"
L164: }
L165: ```
L166: 
L167: - `path` -- file path relative to project root
L168: - `line_start` / `line_end` -- 1-indexed line range (never 0)
L169: - `snippet_hash` -- first 12 hex chars of SHA-256 of the stripped line(s)
L170: - `display` -- human-readable location string
L171: 
L172: For file-existence evidence (e.g., lockfile detection):
L173: 
L174: ```json
L175: {
L176:   "kind": "file_exists",
L177:   "path": "package-lock.json",
L178:   "snippet_hash": "053150b640a7",
L179:   "display": "package-lock.json (file exists)"
L180: }
L181: ```
L182: 
L183: ### Gap Severity
L184: 
L185: Operational gaps in `operate.json` include a severity rating:
L186: - **high** — blocks boot or core execution
L187: - **medium** — impacts deployment maturity
L188: - **low** — best-practice or observability improvements
L189: 
L190: ### Verification
L191: 
L192: Snippet hashes are re-checked against source files: the analyzer re-reads the cited line range, strips whitespace, hashes the result, and confirms it matches the claimed hash. Claims that fail hash verification are capped at confidence 0.20 and marked `"status": "unverified"`.
L193: 
L194: **Important:** Hash verification confirms that a snippet exists at the cited location. It does not prove that the code behaves as described, is secure, or is free of bugs. PTA is not a security scanner, compliance certification tool, or correctness prover.
L195: 
L196: ### Whitespace Policy
L197: 
L198: Lines are stripped (trimmed) before hashing. This normalizes indentation differences across editors and formatters. Both evidence creation and verification use the same canonicalization.
L199: 
L200: ## Limitations
L201: 
L202: - **Static only**: PTA analyzes source files, config, and lockfiles. It cannot observe runtime behavior, network traffic, or live state.
L203: - **No security guarantees**: PTA is not a security scanner, vulnerability detector, or SCA tool. It reports structural observations, not security assessments.
L204: - **Evidence scope**: "EVIDENCED" means a snippet hash matched a source location. It does not mean the code works correctly, is secure, or meets any compliance standard.
L205: - **LLM outputs are interpretive**: When using LLM mode, semantic analysis outputs carry confidence scores and are labeled as LLM-generated. They are not deterministic and should not be treated as ground truth.
L206: - **CI Feed is static analysis**: The CI Feed triggers static analysis on push/PR events. It does not run tests, build artifacts, or deploy code.
L207: 
L208: ## Security & Trust
L209: 
L210: - **Webhook signature verification**: All incoming GitHub webhooks are verified using HMAC-SHA256 with the `X-Hub-Signature-256` header and timing-safe comparison. Invalid signatures are rejected with 401.
L211: - **No secrets in repo**: All sensitive values (`GITHUB_WEBHOOK_SECRET`, `GITHUB_TOKEN`, API keys) are set via environment variables / Replit Secrets. Never committed to source.
L212: - **Symlink protection**: Every path component is checked. If any component in the path tree is a symlink, the file is rejected.
L213: - **Path containment**: All resolved paths must remain within the project root (`relative_to()` check after `resolve()`).
L214: - **Binary detection**: Null bytes in the first 4KB trigger rejection before text decoding.
L215: - **Traversal prevention**: `..` segments and absolute paths are rejected.
L216: - **Secret safety**: Only environment variable names are extracted, never their values.
L217: - **Self-skip**: The analyzer excludes its own source files from analysis to prevent false-positive pattern matches.
L218: 
L219: ## Output Files
L220: 
L221: ### `operate.json`
L222: 
L223: Operator dashboard model with:
L224: - `boot` -- install, dev, prod commands and port bindings, each with evidence tier
L225: - `integrate` -- endpoints, env vars, auth mechanisms with evidence
L226: - `deploy` -- Docker, platform hints, CI/CD, build commands with evidence
L227: - `readiness` -- scores (0-100) for boot, integrate, deploy categories with reasons
L228: - `gaps` -- operational gaps with severity (high/medium/low), rank, and action
L229: - `runbooks` -- numbered step sequences for local_dev, production, and integration
L230: - `snapshot` -- observability, migrations, and architectural metadata
L231: 
L232: All items carry a status of EVIDENCED, INFERRED, or UNKNOWN. EVIDENCED items include file:line references and SHA-256 snippet hashes. UNKNOWN items include an `unknown_reason`.
L233: 
L234: ### `target_howto.json` (legacy)
L235: 
L236: Legacy operator manual. Prefer `operate.json` for operator workflows. Contains:
L237: - `prereqs` -- required runtimes
L238: - `install_steps` -- with commands and evidence
L239: - `config` -- environment variables with file:line references
L240: - `run_dev` / `run_prod` -- start commands with evidence
L241: - `replit_execution_profile` -- port binding, secrets, external APIs (Replit mode)
L242: - `unknowns` -- things the analyzer could not determine
L243: - `completeness` -- scoring with missing items
L244: 
L245: ### `claims.json`
L246: 
L247: Array of verifiable claims:
L248: - `statement` -- what is claimed
L249: - `confidence` -- 0.0 to 1.0
L250: - `evidence` -- array of evidence objects with `snippet_hash_verified: true/false`
L251: - `status` -- `"evidenced"` or `"unverified"`
L252: 
L253: ### `coverage.json`
L254: 
L255: - `mode_requested` / `mode` -- analysis mode
L256: - `scanned` / `skipped` -- file counts
L257: - `replit_detected` -- boolean
L258: - `replit_detection_evidence` -- evidence for Replit detection
L259: - `self_skip` -- analyzer self-exclusion details
L260: 
L261: ## Architecture
L262: 
L263: Two strictly separated layers:
L264: 
L265: 1. **Structural layer** (deterministic) — file indexing, pattern matching, evidence extraction. Outputs are reproducible and hash-verified against source artifacts.
L266: 2. **Semantic layer** (LLM-powered, optional) — architecture interpretation, risk assessment, integration analysis. Outputs are labeled as LLM-generated and carry confidence scores, not deterministic guarantees.
L267: 
L268: The `--no-llm` flag gives you only the structural layer. The semantic layer adds interpretation but is namespaced separately and never contaminates structural evidence.
L269: 
L270: See `docs/ARCHITECTURE.md` for a detailed component diagram including the CI Feed pipeline.
L271: 
L272: ## Troubleshooting
L273: 
L274: ### "No module named 'core'"
L275: 
L276: Run from the repo root, or install with `pip install -e .`
L277: 
L278: ### Missing `DATABASE_URL`
L279: 
L280: The analyzer itself does not need a database. `DATABASE_URL` appears in outputs because it detects the target project's database configuration. No action needed for the analyzer.
L281: 
L282: ### Missing OpenAI environment variables
L283: 
L284: Only required when running without `--no-llm`. Set:
L285: ```bash
L286: export AI_INTEGRATIONS_OPENAI_API_KEY=your-key
L287: export AI_INTEGRATIONS_OPENAI_BASE_URL=https://api.openai.com/v1
L288: ```
L289: 
L290: ### "Port already in use"
L291: 
L292: The analyzer does not bind any ports. If you see port errors, they come from the target project's web server, not the analyzer.
L293: 
L294: ### CI Feed: 401 on webhook delivery
L295: 
L296: The `GITHUB_WEBHOOK_SECRET` env var does not match the secret configured in GitHub, or the env var is not set. Verify both sides match exactly.
L297: 
L298: ### CI Feed: Jobs stuck in DEAD
L299: 
L300: The worker attempted 3 times and failed each time. Common causes: invalid commit SHA, private repo without `GITHUB_TOKEN`, analyzer crash. Check the `error` field on the run and the `lastError` on the job.
L301: 
L302: ### CI Feed: No runs showing
L303: 
L304: Verify you are searching with the correct owner/repo (case-sensitive). If using webhooks, check GitHub's webhook delivery log for successful 200 responses.
L305: 
L306: ## Running Tests
L307: 
L308: ```bash
L309: bash scripts/smoke_test.sh
L310: ```
L311: 
L312: ## License
L313: 
L314: MIT

--- FILE: replit.md ---
L1: # Overview
L2: 
L3: **Program Totality Analyzer** — a full-stack web application that ingests software projects (via GitHub URL, local path, or live Replit workspace) and produces evidence-cited technical dossiers. The dossier covers what a target system is, how it works, how to use it, and what risks/unknowns exist. It combines a React frontend for submitting analysis requests and viewing results with an Express backend that manages projects/analyses in PostgreSQL and spawns a Python-based analyzer CLI for the actual code analysis.
L4: 
L5: ## User Preferences
L6: 
L7: Preferred communication style: Simple, everyday language.
L8: 
L9: ## System Architecture
L10: 
L11: ### Monorepo Structure
L12: 
L13: The project follows a three-zone monorepo pattern:
L14: 
L15: - **`client/`** — React SPA (frontend)
L16: - **`server/`** — Express API (backend)
L17: - **`shared/`** — Shared types, schemas, and route definitions used by both client and server
L18: 
L19: This avoids type drift between frontend and backend by sharing Zod schemas and TypeScript types from a single source of truth.
L20: 
L21: ### Frontend (`client/src/`)
L22: 
L23: - **Framework**: React 18 with TypeScript
L24: - **Routing**: Wouter (lightweight client-side router)
L25: - **State/Data Fetching**: TanStack React Query with polling for analysis status updates
L26: - **UI Components**: shadcn/ui (new-york style) built on Radix UI primitives
L27: - **Styling**: Tailwind CSS with CSS variables for theming (dark mode, cyan/neon aesthetic)
L28: - **Animations**: Framer Motion for page transitions and loading states
L29: - **Markdown Rendering**: react-markdown for displaying analysis dossiers
L30: - **Build Tool**: Vite with React plugin
L31: 
L32: Key pages:
L33: - `/` — Home page with URL input form and "Analyze Replit" button
L34: - `/projects` — List of previous analyses
L35: - `/projects/:id` — Detailed view of a specific analysis with tabs for dossier, claims, operator dashboard (operate.json), coverage, and unknowns
L36: - `/ci` — Live Static CI Feed: searchable run list, manual enqueue, webhook setup info
L37: 
L38: Path aliases: `@/` maps to `client/src/`, `@shared/` maps to `shared/`, `@assets/` maps to `attached_assets/`.
L39: 
L40: ### Backend (`server/`)
L41: 
L42: - **Framework**: Express 5 on Node.js
L43: - **Language**: TypeScript, run via `tsx` in dev
L44: - **API Pattern**: REST API under `/api/` prefix, route definitions shared via `shared/routes.ts`
L45: - **Dev Server**: Vite middleware in development (HMR via `server/vite.ts`), static file serving in production (`server/static.ts`)
L46: - **Build**: esbuild bundles server to `dist/index.cjs`; Vite builds client to `dist/public/`
L47: 
L48: Key API routes (defined in `server/routes.ts`):
L49: - `GET /api/projects` — List all projects
L50: - `POST /api/projects` — Create a new project (with mode: github/local/replit)
L51: - `GET /api/projects/:id` — Get project details
L52: - `GET /api/projects/:id/analysis` — Get analysis results
L53: - `POST /api/projects/:id/analyze` — Trigger analysis (spawns Python CLI)
L54: 
L55: CI Feed API routes:
L56: - `POST /api/webhooks/github` — GitHub webhook receiver (HMAC-SHA256 verified)
L57: - `GET /api/ci/runs?owner=X&repo=Y&limit=N` — List CI runs for a repo
L58: - `GET /api/ci/runs/:id` — Get single CI run details
L59: - `POST /api/ci/enqueue` — Manual trigger {owner, repo, ref, commit_sha, event_type}
L60: - `POST /api/ci/worker/tick` — Process one queued job (fallback worker)
L61: - `GET /api/ci/health` — Job counts by status + last completed run
L62: 
L63: ### Python Analyzer (`server/analyzer/`)
L64: 
L65: - **CLI**: `analyzer_cli.py` using Typer, supports three input modes:
L66:   - GitHub URL (`analyze <url>`)
L67:   - Local path (`analyze <path>`)
L68:   - Replit workspace (`analyze --replit`)
L69: - **Core**: `server/analyzer/src/analyzer.py` — orchestrates file acquisition, indexing, and LLM-powered analysis
L70: - **Operate Module**: `server/analyzer/src/core/operate.py` — deterministic (no LLM) extraction of operational data into `operate.json`
L71:   - Extracts boot commands, ports, integration points (endpoints, env vars, auth), deployment config, and runbook steps
L72:   - Uses three evidence tiers: EVIDENCED (file:line + SHA-256 snippet hash), INFERRED, UNKNOWN (with unknown_reason)
L73:   - Computes readiness scores (0-100) for boot, integrate, deploy categories
L74:   - Identifies operational gaps with severity ratings
L75: - **LLM Integration**: OpenAI API (via Replit AI Integrations env vars: `AI_INTEGRATIONS_OPENAI_API_KEY`, `AI_INTEGRATIONS_OPENAI_BASE_URL`)
L76: - The Express server spawns the Python analyzer as a child process
L77: 
L78: ### Database
L79: 
L80: - **Engine**: PostgreSQL (required, referenced via `DATABASE_URL` env var)
L81: - **ORM**: Drizzle ORM with `drizzle-zod` for schema-to-Zod validation
L82: - **Schema** (`shared/schema.ts`):
L83:   - `projects` — id, url, name, mode (github/local/replit), status (pending/analyzing/completed/failed), createdAt
L84:   - `analyses` — id, projectId, dossier (markdown text), claims (jsonb), howto (jsonb), coverage (jsonb), unknowns (jsonb), operate (jsonb), createdAt
L85:   - `ci_runs` — id (uuid), repoOwner, repoName, ref, commitSha, eventType, status (QUEUED/RUNNING/SUCCEEDED/FAILED), timestamps, error, outDir, summaryJson
L86:   - `ci_jobs` — id (uuid), runId (fk→ci_runs), status (READY/LEASED/DONE/DEAD), attempts, leasedUntil, lastError
L87: - **Chat models** (`shared/models/chat.ts`):
L88:   - `conversations` — id, title, createdAt
L89:   - `messages` — id, conversationId, role, content, createdAt
L90: - **Migrations**: Drizzle Kit with `drizzle-kit push` for schema sync
L91: - **Storage Layer**: `server/storage.ts` implements `IStorage` interface with `DatabaseStorage` class
L92: 
L93: ### Replit Integrations (`server/replit_integrations/` and `client/replit_integrations/`)
L94: 
L95: Pre-built integration modules for AI features:
L96: - **Chat** — Text-based conversation routes and storage using OpenAI
L97: - **Audio** — Voice recording, playback, speech-to-text, text-to-speech with AudioWorklet
L98: - **Image** — Image generation and editing via `gpt-image-1`
L99: - **Batch** — Rate-limited batch processing with retries for LLM calls
L100: 
L101: These are utility modules that can be registered on the Express app as needed.
L102: 
L103: ### Key Design Decisions
L104: 
L105: 1. **Shared route definitions** — `shared/routes.ts` defines API contracts (paths, input schemas, response schemas) used by both frontend hooks and backend handlers. This ensures type safety across the stack.
L106: 
L107: 2. **Python + Node hybrid** — The analyzer logic lives in Python (better ecosystem for code analysis, rich CLI output) while the web layer is Node/Express. The server spawns Python as a child process rather than using a microservice architecture, keeping deployment simple.
L108: 
L109: 3. **Evidence-first analysis** — The analyzer is designed to cite file paths and line ranges for every claim. When evidence is missing, it must label findings as inference/unknown rather than hallucinate.
L110: 
L111: 4. **Polling for status** — The frontend polls project status every 2 seconds while analysis is in progress, switching to static once completed/failed.
L112: 
L113: 5. **Live Static CI Feed** — GitHub webhooks trigger automated static analysis runs. Static analysis only (no runtime telemetry). See `docs/ARCHITECTURE.md` for detailed component diagrams and `docs/API.md` for endpoint documentation.
L114: 
L115: ## Live Static CI Feed — Operator Runbook
L116: 
L117: ### What it is
L118: 
L119: Event-driven static analysis triggered by GitHub push/PR events. Creates `ci_runs` + `ci_jobs`, processes in background worker, stores artifacts under `out/ci/<run_id>/`. Provides a UI feed at `/ci`.
L120: 
L121: ### What it is NOT
L122: 
L123: - Not runtime monitoring, tracing, or telemetry
L124: - Not a security scanner or SCA tool
L125: - Not a CI/CD runner replacement
L126: 
L127: ### Secrets (Environment Variables)
L128: 
L129: Set these in the Replit Secrets tab (lock icon in sidebar):
L130: 
L131: | Variable | Required | Purpose |
L132: |----------|----------|---------|
L133: | `GITHUB_WEBHOOK_SECRET` | Yes (for webhooks) | HMAC-SHA256 signature verification |
L134: | `GITHUB_TOKEN` | For private repos | Git clone authentication |
L135: | `CI_TMP_DIR` | No (default: `/tmp/ci`) | Temp directory for cloned repos |
L136: | `ANALYZER_TIMEOUT_MS` | No (default: 600000) | Analyzer process timeout in ms |
L137: 
L138: To generate a strong webhook secret:
L139: 
L140: ```bash
L141: python -c "import secrets; print(secrets.token_hex(32))"
L142: ```
L143: 
L144: ### GitHub Webhook Setup
L145: 
L146: In the GitHub repo: **Settings > Webhooks > Add webhook**
L147: 
L148: | Setting | Value |
L149: |---------|-------|
L150: | Payload URL | `https://<your-app-domain>/api/webhooks/github` |
L151: | Content type | `application/json` |
L152: | Secret | Must match `GITHUB_WEBHOOK_SECRET` exactly |
L153: | Events | Select "Let me select individual events", check **Pushes** and **Pull requests** |
L154: | Active | Checked |
L155: 
L156: ### Worker Operation
L157: 
L158: The background worker starts automatically on server boot and polls every 5 seconds (`server/ci-worker.ts`). No additional setup is needed.
L159: 
L160: Fallback: If the background loop is not running, you can manually process jobs:
L161: 
L162: ```bash
L163: curl -X POST https://<your-app-domain>/api/ci/worker/tick
L164: ```
L165: 
L166: ### Verification Steps
L167: 
L168: 1. Enqueue a test run:
L169:    ```bash
L170:    curl -X POST https://<your-app-domain>/api/ci/enqueue \
L171:      -H "Content-Type: application/json" \
L172:      -d '{"owner":"<owner>","repo":"<repo>","ref":"main","commit_sha":"<real-sha>","event_type":"manual"}'
L173:    ```
L174: 2. Check health:
L175:    ```bash
L176:    curl https://<your-app-domain>/api/ci/health
L177:    ```
L178: 3. View the `/ci` page in the browser — you should see the run transition from QUEUED to RUNNING to SUCCEEDED/FAILED
L179: 4. Check output artifacts in `out/ci/<run_id>/`
L180: 
L181: ### Troubleshooting
L182: 
L183: | Symptom | Cause | Fix |
L184: |---------|-------|-----|
L185: | 401 on webhook delivery | `GITHUB_WEBHOOK_SECRET` mismatch or not set | Verify the secret matches in both Replit Secrets and GitHub webhook config |
L186: | Jobs stuck in DEAD | Clone failure, bad SHA, or analyzer crash | Check the `error` field on the run and `lastError` on the job. Common: private repo without `GITHUB_TOKEN` |
L187: | Feed shows no runs | Wrong owner/repo query, or no runs triggered | Verify owner/repo are correct (case-sensitive). Check GitHub webhook delivery log for 200 responses |
L188: | Run stays QUEUED | Worker not running | Check server logs for `[CI Worker] Starting background loop`. Restart the server if needed |
L189: | Analyzer timeout | Large repo or slow LLM calls | Increase `ANALYZER_TIMEOUT_MS` or use `--no-llm` mode |
L190: 
L191: ### API Endpoints
L192: 
L193: See `docs/API.md` for full documentation with request/response examples.
L194: 
L195: | Method | Path | Purpose |
L196: |--------|------|---------|
L197: | `POST` | `/api/webhooks/github` | GitHub webhook receiver (HMAC-SHA256 verified) |
L198: | `GET` | `/api/ci/runs?owner=&repo=&limit=` | List CI runs for a repo |
L199: | `GET` | `/api/ci/runs/:id` | Get single CI run details |
L200: | `POST` | `/api/ci/enqueue` | Manual trigger for testing |
L201: | `POST` | `/api/ci/worker/tick` | Process one queued job (fallback) |
L202: | `GET` | `/api/ci/health` | Job counts by status + last completed run |
L203: 
L204: ### Operational Notes
L205: 
L206: - **Deduplication**: Same (owner, repo, SHA) within 6 hours returns existing run
L207: - **Retry logic**: Max 3 attempts per job, then DEAD. Run marked FAILED.
L208: - **Job leasing**: `FOR UPDATE SKIP LOCKED` with 5-minute lease for concurrency safety
L209: - **Signature verification**: HMAC-SHA256 with `X-Hub-Signature-256` header, timing-safe comparison
L210: 
L211: ## External Dependencies
L212: 
L213: ### Required Services
L214: - **PostgreSQL** — Primary database, must be provisioned with `DATABASE_URL` environment variable
L215: - **OpenAI API** (via Replit AI Integrations) — Powers the code analysis LLM calls
L216:   - `AI_INTEGRATIONS_OPENAI_API_KEY` — API key
L217:   - `AI_INTEGRATIONS_OPENAI_BASE_URL` — Base URL for API
L218: 
L219: ### Key NPM Packages
L220: - `express` v5 — HTTP server
L221: - `drizzle-orm` + `drizzle-kit` — Database ORM and migrations
L222: - `@tanstack/react-query` — Client-side data fetching and caching
L223: - `wouter` — Client-side routing
L224: - `react-markdown` — Markdown rendering for dossiers
L225: - `framer-motion` — Animations
L226: - `zod` + `drizzle-zod` — Runtime validation
L227: - `vite` — Frontend build and dev server
L228: - `esbuild` — Server build
L229: 
L230: ### Key Python Packages
L231: - `typer` — CLI framework
L232: - `openai` — LLM API client
L233: - `rich` — Console output formatting
L234: - `python-dotenv` — Environment variable loading
L235: 
L236: ### Dev/Build Tools
L237: - `tsx` — TypeScript execution for development
L238: - `tailwindcss` + `postcss` + `autoprefixer` — CSS toolchain
L239: - `@replit/vite-plugin-runtime-error-modal` — Dev error overlay

--- FILE: attached_assets/Pasted-This-passes-You-now-have-repo-wide-doc-coverage-that-is_1771157647862.txt ---
L1: This passes. You now have **repo-wide doc coverage** that is internally consistent *and* matches code:
L2: 
L3: * Files exist with fresh timestamps: `README.md`, `replit.md`, `docs/API.md`, `docs/ARCHITECTURE.md`.
L4: * CI Feed microcopy updated in `client/src/pages/ci-feed.tsx` (static-only line present).
L5: * Env vars documented match actual usage in `server/routes.ts`, `server/ci-worker.ts`, and analyzer (`GITHUB_TOKEN`), plus `CI_TMP_DIR` + `ANALYZER_TIMEOUT_MS`.
L6: * Endpoint lists in **all docs** match exactly what’s registered in `server/routes.ts`.
L7: 
L8: Two concrete fixes / cleanups I recommend before you ship:
L9: 
L10: ## 1) Remove the duplicate `API.md` / `ARCHITECTURE.md` at repo root (or make them stubs)
L11: 
L12: Your `ls` output shows both:
L13: 
L14: * `docs/API.md` and also `API.md`
L15: * `docs/ARCHITECTURE.md` and also `ARCHITECTURE.md`
L16: 
L17: That will confuse users and future you.
L18: 
L19: **Pick one canonical location.** I’d keep `docs/*` as canonical and make root files either:
L20: 
L21: * deleted, or
L22: * tiny stubs that link to `docs/*`.
L23: 
L24: **Option A (delete root duplicates):**
L25: 
L26: ```bash
L27: rm -f API.md ARCHITECTURE.md
L28: git add -A
L29: git commit -m "docs: remove duplicate root API/ARCHITECTURE docs"
L30: ```
L31: 
L32: **Option B (keep root as pointers):**
L33: Replace `API.md` content with:
L34: 
L35: ```md
L36: # API
L37: 
L38: Canonical API documentation: `docs/API.md`
L39: ```
L40: 
L41: Replace `ARCHITECTURE.md` content with:
L42: 
L43: ```md
L44: # Architecture
L45: 
L46: Canonical architecture documentation: `docs/ARCHITECTURE.md`
L47: ```
L48: 
L49: Then:
L50: 
L51: ```bash
L52: git add API.md ARCHITECTURE.md
L53: git commit -m "docs: make root API/ARCHITECTURE pointers to docs/"
L54: ```
L55: 
L56: ## 2) Verify what changed in those “4 commits” (sanity)
L57: 
L58: Right now you didn’t show `git show --stat -1`, so we can’t see what’s in the latest commit(s). Do:
L59: 
L60: ```bash
L61: git log --oneline -5
L62: git show --stat -1
L63: ```
L64: 
L65: This is just to confirm the last commit truly is documentation-only and not unexpected code edits.
L66: 
L67: ---
L68: 
L69: # Ship it
L70: 
L71: You’re ahead of origin by 4 commits. Push:
L72: 
L73: ```bash
L74: git push origin main
L75: ```
L76: 
L77: ---
L78: 
L79: If you do the “duplicate docs” cleanup, run this final check after:
L80: 
L81: ```bash
L82: rg -n "docs/API.md|docs/ARCHITECTURE.md|API.md|ARCHITECTURE.md" README.md replit.md
L83: ```
L84: 
L85: and make sure references point to exactly the canonical files you chose.

--- FILE: client/requirements.md ---
L1: ## Packages
L2: react-markdown | For rendering the analysis dossier
L3: framer-motion | For smooth page transitions and loading effects
L4: clsx | Utility for constructing className strings conditionally
L5: tailwind-merge | Utility for merging Tailwind classes efficiently
L6: 
L7: ## Notes
L8: - Theme: Technical, dark mode, "Program Totality Analyzer" aesthetic.
L9: - Fonts: Space Grotesk (headers), JetBrains Mono (code/data), Inter (UI).
L10: - Polling: Project status needs polling to show "Analyzing..." vs "Completed".

--- FILE: examples/README.md ---
L1: # Example Outputs
L2: 
L3: These are sample outputs from the Program Totality Analyzer running in `--no-llm` (deterministic) mode against its own workspace.
L4: 
L5: ## Files
L6: 
L7: - `out/operate.sample.json` — Operator dashboard: boot commands, integration points, deployment config, readiness scores, gaps with severity. Deterministic, evidence-bound.
L8: - `out/target_howto.sample.json` — Legacy operator manual. Prefer `operate.json` for operator workflows.
L9: - `out/coverage.sample.json` — Scan metadata: mode requested, files scanned/skipped, Replit detection evidence, self-skip configuration.
L10: 
L11: ## Generating Your Own
L12: 
L13: ```bash
L14: pta analyze --replit --no-llm -o ./my_output
L15: ```
L16: 
L17: Or for a GitHub repo:
L18: 
L19: ```bash
L20: pta analyze https://github.com/user/repo -o ./my_output
L21: ```
L22: 
L23: Or for a local folder:
L24: 
L25: ```bash
L26: pta analyze ./path/to/project -o ./my_output
L27: ```

--- FILE: docs/API.md ---
L1: # API Reference
L2: 
L3: Base URL: `https://<your-app-domain>/api`
L4: 
L5: All endpoints return JSON. Error responses include a `message` field.
L6: 
L7: ## Project Analysis
L8: 
L9: ### `GET /api/projects`
L10: 
L11: List all projects.
L12: 
L13: **Response:**
L14: ```json
L15: [
L16:   {
L17:     "id": 1,
L18:     "url": "https://github.com/user/repo",
L19:     "name": "repo",
L20:     "mode": "github",
L21:     "status": "completed",
L22:     "createdAt": "2025-01-01T00:00:00.000Z"
L23:   }
L24: ]
L25: ```
L26: 
L27: ### `POST /api/projects`
L28: 
L29: Create a new project.
L30: 
L31: **Request body:**
L32: ```json
L33: {
L34:   "url": "https://github.com/user/repo",
L35:   "name": "repo",
L36:   "mode": "github"
L37: }
L38: ```
L39: 
L40: `mode` must be one of: `github`, `local`, `replit`.
L41: 
L42: ### `GET /api/projects/:id`
L43: 
L44: Get a single project by ID.
L45: 
L46: ### `GET /api/projects/:id/analysis`
L47: 
L48: Get analysis results for a project.
L49: 
L50: ### `POST /api/projects/:id/analyze`
L51: 
L52: Trigger analysis for a project. Spawns the Python analyzer as a child process.
L53: 
L54: ---
L55: 
L56: ## Live Static CI Feed
L57: 
L58: ### `POST /api/webhooks/github`
L59: 
L60: GitHub webhook receiver. Validates HMAC-SHA256 signature from the `X-Hub-Signature-256` header against the `GITHUB_WEBHOOK_SECRET` env var.
L61: 
L62: **Accepted events:** `push`, `pull_request`
L63: 
L64: **Headers required:**
L65: - `X-Hub-Signature-256`: HMAC-SHA256 signature of the request body
L66: - `X-GitHub-Event`: Event type (`push` or `pull_request`)
L67: - `Content-Type`: `application/json`
L68: 
L69: **Response (success):**
L70: ```json
L71: {
L72:   "ok": true,
L73:   "run_id": "9c0ed034-9242-4b46-ae22-1f3baa72f4c8"
L74: }
L75: ```
L76: 
L77: **Response (deduplicated — same owner/repo/SHA within 6 hours):**
L78: ```json
L79: {
L80:   "ok": true,
L81:   "run_id": "9c0ed034-9242-4b46-ae22-1f3baa72f4c8",
L82:   "deduplicated": true
L83: }
L84: ```
L85: 
L86: **Response (signature invalid):**
L87: ```
L88: 401 Unauthorized
L89: ```
L90: 
L91: ### `GET /api/ci/runs`
L92: 
L93: List CI runs for a repository.
L94: 
L95: **Query parameters:**
L96: | Param | Required | Default | Description |
L97: |-------|----------|---------|-------------|
L98: | `owner` | Yes | — | Repository owner |
L99: | `repo` | Yes | — | Repository name |
L100: | `limit` | No | 50 | Max runs to return |
L101: 
L102: **Response:**
L103: ```json
L104: {
L105:   "ok": true,
L106:   "runs": [
L107:     {
L108:       "id": "9c0ed034-9242-4b46-ae22-1f3baa72f4c8",
L109:       "repoOwner": "octocat",
L110:       "repoName": "hello-world",
L111:       "ref": "main",
L112:       "commitSha": "abc123def456789...",
L113:       "eventType": "push",
L114:       "status": "SUCCEEDED",
L115:       "createdAt": "2025-01-01T00:00:00.000Z",
L116:       "startedAt": "2025-01-01T00:00:01.000Z",
L117:       "finishedAt": "2025-01-01T00:01:30.000Z",
L118:       "error": null,
L119:       "outDir": "out/ci/9c0ed034-9242-4b46-ae22-1f3baa72f4c8",
L120:       "summaryJson": {
L121:         "boot_commands": 3,
L122:         "endpoints": 5,
L123:         "gaps": 2
L124:       }
L125:     }
L126:   ]
L127: }
L128: ```
L129: 
L130: ### `GET /api/ci/runs/:id`
L131: 
L132: Get a single CI run by UUID.
L133: 
L134: **Response:** Same shape as a single item from the runs list above.
L135: 
L136: ### `POST /api/ci/enqueue`
L137: 
L138: Manually enqueue a CI run (useful for testing without webhooks).
L139: 
L140: **Request body:**
L141: ```json
L142: {
L143:   "owner": "octocat",
L144:   "repo": "hello-world",
L145:   "ref": "main",
L146:   "commit_sha": "abc123def456789abcdef1234567890abcdef123",
L147:   "event_type": "manual"
L148: }
L149: ```
L150: 
L151: **Response:**
L152: ```json
L153: {
L154:   "ok": true,
L155:   "run_id": "9c0ed034-9242-4b46-ae22-1f3baa72f4c8"
L156: }
L157: ```
L158: 
L159: If the same owner/repo/SHA was enqueued within the last 6 hours:
L160: ```json
L161: {
L162:   "ok": true,
L163:   "run_id": "9c0ed034-9242-4b46-ae22-1f3baa72f4c8",
L164:   "deduplicated": true
L165: }
L166: ```
L167: 
L168: ### `POST /api/ci/worker/tick`
L169: 
L170: Process one queued job. Fallback mechanism for environments where the background worker loop is not running.
L171: 
L172: **Response (job processed):**
L173: ```json
L174: {
L175:   "ok": true,
L176:   "processed": true,
L177:   "run_id": "9c0ed034-9242-4b46-ae22-1f3baa72f4c8"
L178: }
L179: ```
L180: 
L181: **Response (no jobs):**
L182: ```json
L183: {
L184:   "ok": true,
L185:   "processed": false
L186: }
L187: ```
L188: 
L189: ### `GET /api/ci/health`
L190: 
L191: Health check endpoint showing job queue status.
L192: 
L193: **Response:**
L194: ```json
L195: {
L196:   "ok": true,
L197:   "jobs": {
L198:     "READY": 2,
L199:     "LEASED": 1,
L200:     "DONE": 15,
L201:     "DEAD": 0
L202:   },
L203:   "last_completed": {
L204:     "id": "9c0ed034-9242-4b46-ae22-1f3baa72f4c8",
L205:     "repoOwner": "octocat",
L206:     "repoName": "hello-world",
L207:     "status": "SUCCEEDED",
L208:     "finishedAt": "2025-01-01T00:01:30.000Z"
L209:   }
L210: }
L211: ```
L212: 
L213: ---
L214: 
L215: ## Webhook Requirements
L216: 
L217: GitHub webhook configuration:
L218: 
L219: | Setting | Value |
L220: |---------|-------|
L221: | Payload URL | `https://<your-app-domain>/api/webhooks/github` |
L222: | Content type | `application/json` |
L223: | Secret | Must match `GITHUB_WEBHOOK_SECRET` env var exactly |
L224: | Events | Push, Pull requests |
L225: 
L226: Signature verification uses HMAC-SHA256 with timing-safe comparison. The server checks the `X-Hub-Signature-256` header against the computed signature of the raw request body.

--- FILE: docs/ARCHITECTURE.md ---
L1: # Architecture
L2: 
L3: ## High-Level Components
L4: 
L5: ```
L6: GitHub Webhook ──► Express API ──► ci_runs / ci_jobs (PostgreSQL)
L7:                                          │
L8:                                          ▼
L9:                                    Background Worker
L10:                                    (polls every 5s)
L11:                                          │
L12:                                          ▼
L13:                               ┌──────────────────────┐
L14:                               │  1. Shallow git clone │
L15:                               │  2. Run analyzer CLI  │
L16:                               │  3. Store artifacts   │
L17:                               └──────────────────────┘
L18:                                          │
L19:                                          ▼
L20:                               out/ci/<run_id>/
L21:                               (operate.json, DOSSIER.md, etc.)
L22: 
L23: React UI (/ci) ◄── polls ──► GET /api/ci/runs
L24: ```
L25: 
L26: ## Components
L27: 
L28: ### Web Layer (Express + React)
L29: 
L30: - **Express API** (`server/routes.ts`): REST endpoints for project analysis and the CI feed. Serves the React SPA via Vite middleware in development, static files in production.
L31: - **React SPA** (`client/src/`): Single-page application with pages for project analysis, results viewing, and the CI feed at `/ci`.
L32: 
L33: ### CI Feed Pipeline
L34: 
L35: The CI feed is an event-driven static analysis pipeline:
L36: 
L37: 1. **Intake**: GitHub webhook (`POST /api/webhooks/github`) or manual enqueue (`POST /api/ci/enqueue`)
L38: 2. **Storage**: Creates a `ci_runs` row (status: QUEUED) and a `ci_jobs` row (status: READY)
L39: 3. **Processing**: Background worker picks up READY jobs and runs the analyzer
L40: 4. **Output**: Artifacts stored to `out/ci/<run_id>/`, summary extracted from `operate.json`
L41: 
L42: ### Webhook Receiver
L43: 
L44: - Validates `X-Hub-Signature-256` using HMAC-SHA256 with `GITHUB_WEBHOOK_SECRET`
L45: - Checks buffer length equality before `crypto.timingSafeEqual` to avoid crypto errors
L46: - Extracts owner, repo, ref, commit SHA from push and pull_request event payloads
L47: - Deduplicates: if the same (owner, repo, SHA) was processed within 6 hours, returns the existing run
L48: 
L49: ### Background Worker (`server/ci-worker.ts`)
L50: 
L51: - Starts automatically on server boot via `setInterval` (every 5 seconds)
L52: - Also exposed as `POST /api/ci/worker/tick` for manual/fallback triggering
L53: 
L54: #### Job Leasing Strategy
L55: 
L56: ```sql
L57: SELECT j.id, j.run_id
L58: FROM ci_jobs j
L59: WHERE j.status = 'READY'
L60:    OR (j.status = 'LEASED' AND j.leased_until < NOW())
L61: ORDER BY j.id
L62: LIMIT 1
L63: FOR UPDATE SKIP LOCKED
L64: ```
L65: 
L66: - **`FOR UPDATE SKIP LOCKED`**: Prevents multiple workers from picking the same job. If a row is already locked by another transaction, it is skipped rather than blocking.
L67: - **Lease timeout**: 5 minutes (`leased_until`). If a worker crashes mid-job, the lease expires and another worker can pick it up.
L68: - **Max attempts**: 3. After 3 failures, the job is marked DEAD and the corresponding run is marked FAILED.
L69: 
L70: #### Processing Steps
L71: 
L72: 1. Lease the job (update status to LEASED, set `leased_until`)
L73: 2. Update the run to RUNNING
L74: 3. Clone the repo to a temp directory (`CI_TMP_DIR`, default `/tmp/ci`)
L75:    - Uses `git clone --depth 1` then `git checkout <sha>` for the exact commit
L76:    - If `GITHUB_TOKEN` is set, injects it into the clone URL for private repos
L77: 4. Spawn the Python analyzer CLI as a child process
L78: 5. On success: extract summary counts from `operate.json`, update run to SUCCEEDED, job to DONE
L79: 6. On failure: increment attempts, update error, check if max attempts reached
L80: 
L81: ### Analyzer (`server/analyzer/`)
L82: 
L83: Two strictly separated layers:
L84: 
L85: 1. **Structural layer** (deterministic): File indexing, pattern matching, evidence extraction with SHA-256 snippet hashes. Always runs. Outputs are reproducible.
L86: 2. **Semantic layer** (LLM-powered, optional): Architecture interpretation, risk assessment, integration analysis. Requires OpenAI API keys. Outputs carry confidence scores.
L87: 
L88: The `--no-llm` flag produces only structural output. The CI worker runs the analyzer with default settings (both layers if API keys are available).
L89: 
L90: ### Database (PostgreSQL)
L91: 
L92: Tables relevant to CI feed:
L93: 
L94: **`ci_runs`**
L95: | Column | Type | Description |
L96: |--------|------|-------------|
L97: | `id` | uuid | Primary key (generated) |
L98: | `repo_owner` | text | GitHub org/user |
L99: | `repo_name` | text | Repository name |
L100: | `ref` | text | Branch/tag |
L101: | `commit_sha` | text | Full 40-char SHA |
L102: | `event_type` | text | push, pull_request, manual |
L103: | `status` | text | QUEUED, RUNNING, SUCCEEDED, FAILED |
L104: | `created_at` | timestamp | When the run was created |
L105: | `started_at` | timestamp | When processing began |
L106: | `finished_at` | timestamp | When processing ended |
L107: | `error` | text | Error message if failed |
L108: | `out_dir` | text | Path to output artifacts |
L109: | `summary_json` | jsonb | Summary counts from operate.json |
L110: 
L111: **`ci_jobs`**
L112: | Column | Type | Description |
L113: |--------|------|-------------|
L114: | `id` | uuid | Primary key (generated) |
L115: | `run_id` | uuid | Foreign key to ci_runs |
L116: | `status` | text | READY, LEASED, DONE, DEAD |
L117: | `attempts` | integer | Number of processing attempts |
L118: | `leased_until` | timestamp | Lease expiry for concurrency |
L119: | `last_error` | text | Error from most recent attempt |
L120: 
L121: ### Failure Modes
L122: 
L123: | Scenario | Behavior |
L124: |----------|----------|
L125: | Invalid webhook signature | 401 returned, no run created |
L126: | Clone fails (bad SHA, no access) | Job error recorded, attempts incremented, retried up to 3x |
L127: | Analyzer timeout | Process killed after `ANALYZER_TIMEOUT_MS` (default 10 min), treated as failure |
L128: | Worker crashes mid-job | Lease expires after 5 min, another worker picks up the job |
L129: | 3 consecutive failures | Job marked DEAD, run marked FAILED with last error |
L130: | Duplicate webhook (same SHA) | Deduplicated, returns existing run ID |
L131: 
L132: ### Environment Variables
L133: 
L134: | Variable | Required | Default | Description |
L135: |----------|----------|---------|-------------|
L136: | `DATABASE_URL` | Yes | — | PostgreSQL connection string |
L137: | `GITHUB_WEBHOOK_SECRET` | For webhooks | — | HMAC-SHA256 signature verification |
L138: | `GITHUB_TOKEN` | For private repos | — | Git clone authentication |
L139: | `CI_TMP_DIR` | No | `/tmp/ci` | Temp directory for cloned repos |
L140: | `ANALYZER_TIMEOUT_MS` | No | `600000` (10 min) | Analyzer process timeout |
L141: | `AI_INTEGRATIONS_OPENAI_API_KEY` | For LLM mode | — | OpenAI API key |
L142: | `AI_INTEGRATIONS_OPENAI_BASE_URL` | For LLM mode | — | OpenAI API base URL |

--- FILE: docs/dossiers/lantern_program_totality_dossier.md ---
L1: ---
L2: title: Lantern Program Totality Dossier
L3: generated_by: PTA / Lantern
L4: mode: deterministic+curated
L5: date: 2026-02-14
L6: ---
L7: 
L8: # HALO-RECEIPTS: Program Totality Analyzer Dossier
L9: 
L10: ---
L11: 
L12: ## 1. **Identity of Target System**
L13: 
L14: **What it IS:**  
L15: HALO-RECEIPTS (AI Receipts) is a forensic verification system specifically designed for AI conversation transcripts. It provides cryptographic verification (SHA-256, Ed25519), immutable receipt storage, tamper-evident audit trails, forensic export capabilities, and extensive auditing for post-hoc analysis. This system is both the backend API server (Node.js/Express/PostgreSQL/Drizzle ORM) and a modern React UI, bundling forensic guarantees directly into receipt management and export (README.md:3,9,60–61; replit.md:4,11–12,14–24).
L16: 
L17: **What it is NOT:**  
L18: - **Not a real-time monitoring, content moderation, or multi-operator platform:** It is not designed for live chat moderation, direct truth judgment, or multi-user concurrency at the enforcement level (replit.md:55).
L19: - **Not a database engine:** Instead, it relies on PostgreSQL for durable state.
L20: - **Not a data lake or generic file archival platform.**
L21: - **Not a replacement for WORM-compliant log systems, but can integrate via checkpoint anchoring (see below).**
L22: - **Not a deployment framework:** The system expects to be deployed behind a reverse proxy or PaaS (README.md:24; SECURITY.md:54).
L23: 
L24: ---
L25: 
L26: ## 2. **Purpose & Jobs-to-be-done**
L27: 
L28: - **Forensic Conversation Integrity:** Operators can guarantee, via public proofs, that a transcript or "receipt" has not been tampered with since its recording date (README.md:9; replit.md:4).
L29: - **Immutable Logging & Audit Trails:** Ensures all receipt actions (append, lock, kill switch, export, audit actions) are tracked in an append-only, hash-linked audit log, detecting insertions, deletions, reordering, and version tampering (SECURITY.md:8–10; STATE.md:165–169).
L30: - **Cryptographic Verification:** Verifies that every receipt chain and audit log entry is both hash-linked (SHA-256) and checkpoint signed (Ed25519), with optional external anchoring (replit.md:36–46; STATE.md:116–121).
L31: - **Forensic Export and Proof Packs:** Allows export of forensic packs (JSON) that can be offline verified and admitted as tamper-evident evidence (STATE.md:122–125; scripts/ci-forensic-gate.sh).
L32: - **Regulatory Alignment:** System features mapped to compliance goals (21 CFR, HIPAA, SOC2, etc.) (replit.md:50, STATE.md:137).
L33: - **Operator/Evidence Reliability:** Designed to provide demonstrable evidence for courts, regulators, or internal review.
L34: 
L35: ---
L36: 
L37: ## 3. **Capability Map**
L38: 
L39: | Capability             | Mechanism / Implementation | Evidence                                    |
L40: |------------------------|---------------------------|---------------------------------------------|
L41: | SHA-256 Hash Verification   | Canonicalized (c14n-v1) JSON hash | README.md:73–74; STATE.md:17,20             |
L42: | Ed25519 Signatures     | Checkpoint signing, chain   | STATE.md:18,116–121; replit.md:36           |
L43: | Immutable Storage      | Receipt lock, no mutation  | README.md:75; STATE.md:21,158–159           |
L44: | Kill Switch            | Irreversible flag, disables outputs | README.md:76; STATE.md:22,157                |
L45: | Audit Logs             | Append-only, hash-chained table | STATE.md:25–29,162                           |
L46: | Forensic Export/Import | export_forensic_pack, verify_forensic_pack scripts | STATE.md:123–126                              |
L47: | Forensic Sensors       | Interpreter, summarizer, claim extractor | README.md:78; STATE.md:80,85                  |
L48: | Policy Enforcement     | Zod schemas, request shape limits | SECURITY.md:20–23                            |
L49: | API Rate Limiting      | Per-IP, in-memory          | SECURITY.md:26–29; STATE.md:94; package.json:61 |
L50: | Key Rotation & Anchoring | Multi-key support, anchor backends | replit.md:43–47                            |
L51: | Secure API Structure   | API_KEY in x-api-key header; private/public endpoints | SECURITY.md:15–16,72                           |
L52: | Client Auth Isolation  | LLMs see only transcript content | SECURITY.md:39–40; STATE.md:160,20           |
L53: | UI Export & Compare    | Side-by-side comparison, JSONL/CSV export | README.md:143; replit.md:25,23                |
L54: | Structured Logging     | JSON logs, in-memory counters | STATE.md:106–107,184                          |
L55: | Health Checks          | /api/health, /api/ready    | STATE.md:37,43,100–101; docs/API_CONTRACTS.md:11,24 |
L56: 
L57: ---
L58: 
L59: ## 4. **Architecture Snapshot**
L60: 
L61: - **Frontend:** React (wouter router), Tailwind, shadcn/ui (README.md:59, client/src/App.tsx)
L62: - **Backend:** Node.js 20, Express, Drizzle ORM (README.md:60–61; package.json)
L63: - **Database:** PostgreSQL 14+ (README.md:24, .env.example:5)
L64: - **Cryptography:** Node.js crypto (SHA-256), Ed25519 (STATE.md:18,116)
L65: - **Session:** express-session, connect-pg-simple (package.json:51,57)
L66: - **Audit Trail:** Hash-linking via prev_hash and payload_v, audit_head singleton row (STATE.md:9,25–29,48,52; drizzle.config.ts:9)
L67: - **Forensic Export:** TypeScript scripts in `/scripts`, results in JSON proof packs (STATE.md:123–126)
L68: - **CI/CD:** GitHub Actions, drift guard scripts, reproducible verifier zips (replit.md:39–42,57)
L69: 
L70: ---
L71: 
L72: ## 5. **How to Use the Target System**
L73: 
L74: ### **Operator Manual**
L75: 
L76: #### **A. Prerequisites**
L77: 
L78: 1. **Install:**
L79:    - Node.js 20+, npm (`npm -v`/`node -v`)
L80:    - PostgreSQL 14+ running and accessible (README.md:23–24)
L81:    - jq, unzip, python3 for export/ops scripts (see replit.nix, scripts/ci-forensic-gate.sh)
L82:    - TypeScript, tsx, drizzle-kit, installed via npm as devDependencies (package.json)
L83: 
L84: #### **B. Installation**
L85: 
L86: 1. **Clone Repository**
L87: 2. `npm install`  
L88:    Installs all dependencies (README.md:29)
L89: 3. `cp .env.example .env`  
L90:    Copy template env config (README.md:33)
L91: 4. **Edit `.env`**:  
L92:    Set `DATABASE_URL`, `API_KEY`, `SESSION_SECRET` and all other required variables suitably (README.md:34; .env.example:5,10,23)
L93: 5. **(Optional: Replit):**
L94:    - Use the "Run on Replit" badge or Replit sidebar GUI (README.md:17)
L95: 
L96: #### **C. Configuration**
L97: 
L98: Set the following in `.env` (names only, do not provide values):
L99: - **DATABASE_URL:** PostgreSQL connection string (.env.example:5)
L100: - **API_KEY:** Required for private endpoints (.env.example:10; SECURITY.md:15)
L101: - **SESSION_SECRET:** Strong, random string (.env.example:23; SECURITY.md:71)
L102: - **NODE_ENV:** development/production (.env.example:13)
L103: - **PORT:** Default 5000 (.env.example:14)
L104: - Other optional: TRANSCRIPT_MODE, CHECKPOINT_INTERVAL, CHECKPOINT_ANCHOR_TYPE, etc. (.env.example, STATE.md:118, replit.md:45)
L105: 
L106: #### **D. Database Init**
L107: 
L108: - Run: `npm run db:push`  
L109:   This applies the schema to PostgreSQL (README.md:39)
L110: 
L111: #### **E. Development Server**
L112: 
L113: - Run: `npm run dev`  
L114:   Runs server in development mode (README.md:44)
L115: - Visit: [http://localhost:5000](http://localhost:5000) (README.md:47)
L116: 
L117: #### **F. Production Build**
L118: 
L119: - Run: `npm run build`  
L120:   Compile frontend and backend (README.md:52)
L121: - Set `NODE_ENV=production`, then  
L122:   `npm run start` (README.md:53)
L123: - Server now on port specified in env (default 5000) (README.md:47, .replit:10,14, .env.example:14)
L124: 
L125: #### **G. Example API Usage**
L126: 
L127: - Health Check:  
L128:   `curl http://localhost:5000/api/health` (docs/API_CONTRACTS.md:11)
L129: - Readiness:  
L130:   `curl http://localhost:5000/api/ready` (docs/API_CONTRACTS.md:24)
L131: - Verify Audit (requires API_KEY):  
L132:   `curl -H "x-api-key: <API_KEY>" http://localhost:5000/api/audit/verify` (docs/API_CONTRACTS.md:64)
L133: - Create & Lock Receipts, Get all Receipts, Kill Switch, etc.:  
L134:   See usage_examples in HOWTO (docs/API_CONTRACTS.md)
L135: 
L136: #### **H. Verification & Forensics**
L137: 
L138: - Check schema: `npm run db:push` (README.md:39)
L139: - Verify audit chain:  
L140:   `curl -H "x-api-key: <API_KEY>" http://localhost:5000/api/audit/verify` (STATE.md:206)
L141: - Export forensic pack:**  
L142:   `npx tsx scripts/export_forensic_pack.ts --output <pack.json>`
L143: - Offline verify pack:  
L144:   `npx tsx scripts/verify_forensic_pack.ts <pack.json>` (scripts/ci-forensic-gate.sh:42)
L145: - Tamper detection: run the same after editing pack, expect fail (scripts/ci-forensic-gate.sh:61–84)
L146:   
L147: #### **I. Common Failures**
L148: 
L149: | Symptom                        | Cause                              | Fix                                                  | Evidence                |
L150: |--------------------------------|------------------------------------|------------------------------------------------------|-------------------------|
L151: | 401 Unauthorized               | Wrong/missing API_KEY              | Set correct `x-api-key` header, check .env           | SECURITY.md:15          |
L152: | DB connection errors/crash     | Bad DATABASE_URL, DB offline/wrong | Check credentials, service, version                  | drizzle.config.ts:3     |
L153: | Server not running on port     | App not started, port conflict     | Check logs, ensure PORT=5000, check if in use        | .replit:10              |
L154: | Forensic pack tamper undetected| Bug or script not installed        | Re-export, ensure proper script in place             | scripts/ci-forensic-gate.sh:61–84 |
L155: 
L156: ---
L157: 
L158: ## 6. **Integration Surface**
L159: 
L160: - **REST API:**  
L161:   Well-documented REST endpoints (`/api/health`, `/api/ready`, `/api/receipts`, `/api/audit/verify`, `/api/receipts/:id/lock`, `/api/receipts/:id/kill-switch`, etc.) (docs/API_CONTRACTS.md)
L162: - **API Authentication:**  
L163:   API key required for all non-public (write or sensitive) endpoints via `x-api-key` HTTP header (SECURITY.md:15, .env.example:10)
L164: - **Webhooks:**  
L165:   Unknown — evidence needed: No explicit webhook example or config found for outbound push/integrations.
L166: - **Data Formats:**  
L167:   JSON REST, all API schemas validated by Zod (STATE.md:95; SECURITY.md:20).
L168: - **Export/Import:**  
L169:   Forensic packs as canonical JSON, offline verifier script can process proof packs and verify signatures (STATE.md:123–125)
L170: - **SDKs:**  
L171:   None provided; interaction via HTTP API and TypeScript scripts.
L172: 
L173: ---
L174: 
L175: ## 7. **Data & Security Posture**
L176: 
L177: - **Data Storage:**  
L178:   All core state in PostgreSQL (receipts, audit trail, checkpoints; .env.example:5; README.md:61)
L179: - **Immutable guarantees:**  
L180:   Lock and kill-switch state prevent subsequent edits (README.md:75,76; STATE.md:21–22)
L181: - **Audit Log:**  
L182:   Hash-linked, append-only, verified at both write and operator demand (STATE.md:25–29)
L183: - **Cryptography:**  
L184:   Canonical JSON SHA-256 for all payloads, Ed25519 for checkpoint signing (STATE.md:17–19,116)
L185: - **External Anchoring:**  
L186:   Pluggable anchors (LogOnly, S3WormAnchor, Rfc3161TsaAnchor), config via `CHECKPOINT_ANCHOR_TYPE` (replit.md:45; STATE.md:170–173)
L187: - **Authentication:**  
L188:   API_KEY via header for all writes and sensitive queries, stored only as secret (SECURITY.md:15–17,72,84)
L189: - **Input Validation:**  
L190:   Zod schemas, 1MB max body, JSON only, UTF-8 validation (SECURITY.md:20–23)
L191: - **Rate Limiting:**  
L192:   Per-IP, endpoint and overall, in-memory only (SECURITY.md:26–29; STATE.md:94)
L193: - **Headers:**  
L194:   X-Content-Type-Options, X-Frame-Options, Referrer-Policy, Permissions-Policy (SECURITY.md:32–36; STATE.md:97)
L195: - **Session Security:**  
L196:   express-session with SESSION_SECRET, connect-pg-simple store (package.json:51,57; .env.example:23)
L197: - **No Logging of Secrets:**  
L198:   Explicitly forbidden, audit logs never include `API_KEY`/secrets (SECURITY.md:17,90)
L199: 
L200: ---
L201: 
L202: ## 8. **Operational Reality**
L203: 
L204: - **Server:**  
L205:   Runs via `npm run dev` (development) or production `npm run build && npm run start` (README.md:44,52–53; .replit:2,18)
L206: - **Port:**  
L207:   Default 5000 (README.md:47; .env.example:14; .replit:10,14)
L208: - **Database:**  
L209:   PostgreSQL 14+ required and always available; credentials in env (README.md:24; drizzle.config.ts)
L210: - **Secrets:**  
L211:   All secrets via `.env`; no default session or API key in production (SECURITY.md:84)
L212: - **No Built-in TLS:**  
L213:   Not exposed directly; deploy behind HTTPS proxy (SECURITY.md:54; README.md:24)
L214: - **CI/CD:**  
L215:   Github Actions runs type check, db:push, tests, drift guards, build, releases artifacts (STATE.md:111; replit.md:39–42)
L216: - **Counters/Rate-Limits:**  
L217:   In-memory, reset on process restart (STATE.md:107; SECURITY.md:62–64)
L218: - **Persistent Storage:**  
L219:   No special data directory—state in PostgreSQL only.
L220: - **Logs:**  
L221:   Structured JSON to console; location for interactive review unknown — evidence needed (STATE.md:106)
L222: 
L223: ---
L224: 
L225: ## 9. **Maintainability & Change Risk**
L226: 
L227: - **Well Bounded:**  
L228:   Security, business logic, and cryptography are isolated and formalized (STATE.md:149–162)
L229: - **Explicit Invariants and Forbidden Practices:**  
L230:   Canonicalization pipeline, no mock data, strict version/hashing rules (STATE.md:149–162,186–193)
L231: - **Codebase Size:**  
L232:   Large (`routes.ts` >2k lines); risk for routing/merge conflicts (STATE.md:183)
L233: - **Rate Limiter Limitation:**  
L234:   In-memory counters/rate-limiting, resets on restart; persistence is a punchlist item (STATE.md:184,219)
L235: - **Key rotation and signature abstraction:**  
L236:   Documented plan, proof-tested, rotation protocol in THREAT_MODEL.md (replit.md:43–44,60)
L237: - **Danger Areas:**  
L238:   Fully-privileged DB admin can bypass all checks without external anchor; risk documented (STATE.md:172–173).
L239: - **Tests:**  
L240:   42 tests (STATE.md:194). CI runs coverage, canonicalization, audit drift/adapter boundary guards (STATE.md:111–114)
L241: 
L242: ---
L243: 
L244: ## 10. **Replit Execution Profile**
L245: 
L246: - **Default Replit run:** `npm run dev` ([.replit:2])
L247: - **Modules:** Node 20, PostgreSQL 16, web via Replit ([.replit:1])
L248: - **PORT:** 5000 (mapped to external 80) ([.replit:10–11,14])
L249: - **Build on Deploy:** Runs `npm run build` ([.replit:19])
L250: - **Production Entrypoint:** `node ./dist/index.cjs` ([.replit:18])
L251: - **Replit Special Integration:** VITE_DEV_API_KEY set for development ([.replit:45]), GitHub integration possible ([.env.example:27–29])
L252: - **Nix Packages:** jq, unzip for ops scripts ([.replit:7])
L253: 
L254: ---
L255: 
L256: ## 11. **Unknowns / Missing Evidence**
L257: 
L258: | What is Missing | Why It Matters | Evidence Needed |
L259: |-----------------|----------------|----------------|
L260: | Production deployment details outside Replit (systemd/Docker/PM2) | Real-world ops, non-Replit platforms | Dockerfile, systemd service, or reverse proxy config |
L261: | Database initialization beyond drizzle-kit | Custom DB/user privileges, stateful setups | Database schema, role guides, SQL init scripts |
L262: | Key rotation/generation procedure for API_KEY or SESSION_SECRET | Long-term ops, secops | Step-by-step key management documentation |
L263: | Log path or viewing commands | Troubleshooting, support | Log file locations or sample log tail commands |
L264: | Standalone front-end hosting commands | Non-Replit, split hosting | Front-end build/start manual, production hosting instructions |
L265: | Webhooks or outbound events | Integration with SIEM/alerting | Outbound webhook config or documentation |
L266: 
L267: ---
L268: 
L269: ## 12. **Receipts (Evidence Index)**
L270: 
L271: **All claims above are strictly supported by:**
L272: 
L273: - **README.md:** lines 3,9,17–47,52–56,59–79,90.
L274: - **replit.md:** lines 4,11–61 (core capabilities, architecture).
L275: - **.env.example:** lines 5,10,13,14,18,23,27–29.
L276: - **STATE.md:** lines 17–23,25–48,49–212,149–162,165–193,194–214 (invariants, tests, capability inventory, ops, threat model, audit chain).
L277: - **SECURITY.md:** lines 8–17,20–29,32–36,39–41,54,62–64,71–90.
L278: - **drizzle.config.ts:** line 3 (database connectivity).
L279: - **.replit:** lines 2,10,14,18–19,45 (Replit settings).
L280: - **package.json:** dev/prod dependencies, scripts: "dev", "build", "start", "db:push" (lines 7–12,51,57,61).
L281: - **docs/API_CONTRACTS.md:** endpoints, usage examples.
L282: - **scripts/ci-forensic-gate.sh:** lines 42,61–84 (forensic scripts).
L283: - **client/src/App.tsx & components:** UI routes, health/audit banners.
L284:   
L285: **Specific references cross-index with HOWTO JSON, which extracts and hashes the underlying source text (e.g., README.md:39).**
L286: 
L287: ---
L288: 
L289: **End of Dossier.**

--- FILE: output/DOSSIER.md ---
L1: # Program Totality Analyzer — Deterministic Dossier
L2: 
L3: **Mode:** `--no-llm` (deterministic extraction only, no LLM calls)
L4: 
L5: ## 1. File Index Summary
L6: - Files scanned: see index.json
L7: - Self-skip: 26 analyzer files excluded
L8: 
L9: ## 2. Replit Execution Profile
L10: - **Is Replit:** True
L11: - **Run command:** `npm run dev`
L12: - **Language:** nodejs
L13: - **Port:** Uses PORT env var; actual port determined at runtime. In Replit, PORT is injected.
L14: - **Secrets (3):** DATABASE_URL, AI_INTEGRATIONS_OPENAI_API_KEY, AI_INTEGRATIONS_OPENAI_BASE_URL
L15: - **External APIs:** OpenAI
L16: 
L17: ## 3. Operator Manual (Deterministic)
L18: ```json
L19: {
L20:   "prereqs": [
L21:     "Node.js",
L22:     "Python"
L23:   ],
L24:   "install_steps": [
L25:     {
L26:       "step": "Install Node dependencies",
L27:       "command": "npm ci",
L28:       "evidence": {
L29:         "kind": "file_exists",
L30:         "path": "package-lock.json",
L31:         "snippet_hash": "053150b640a7",
L32:         "display": "package-lock.json (file exists)"
L33:       }
L34:     },
L35:     {
L36:       "step": "Install Python dependencies",
L37:       "command": "pip install .",
L38:       "evidence": {
L39:         "kind": "file_exists",
L40:         "path": "pyproject.toml",
L41:         "snippet_hash": "50c86b7ed8ac",
L42:         "display": "pyproject.toml (file exists)"
L43:       }
L44:     }
L45:   ],
L46:   "config": [
L47:     {
L48:       "name": "DATABASE_URL",
L49:       "purpose": "Secret referenced in code (see evidence)",
L50:       "evidence": {
L51:         "path": "drizzle.config.ts",
L52:         "line_start": 3,
L53:         "line_end": 3,
L54:         "snippet_hash": "a19790628fbe",
L55:         "display": "drizzle.config.ts:3"
L56:       }
L57:     },
L58:     {
L59:       "name": "AI_INTEGRATIONS_OPENAI_API_KEY",
L60:       "purpose": "Secret referenced in code (see evidence)",
L61:       "evidence": {
L62:         "path": "server/replit_integrations/audio/client.ts",
L63:         "line_start": 10,
L64:         "line_end": 10,
L65:         "snippet_hash": "05da5f1b1281",
L66:         "display": "server/replit_integrations/audio/client.ts:10"
L67:       }
L68:     },
L69:     {
L70:       "name": "AI_INTEGRATIONS_OPENAI_BASE_URL",
L71:       "purpose": "Secret referenced in code (see evidence)",
L72:       "evidence": {
L73:         "path": "server/replit_integrations/audio/client.ts",
L74:         "line_start": 11,
L75:         "line_end": 11,
L76:         "snippet_hash": "1f70e6a77d42",
L77:         "display": "server/replit_integrations/audio/client.ts:11"
L78:       }
L79:     }
L80:   ],
L81:   "run_dev": [
L82:     {
L83:       "step": "Start dev server",
L84:       "command": "npm run dev",
L85:       "evidence": {
L86:         "path": "package.json",
L87:         "line_start": 7,
L88:         "line_end": 7,
L89:         "snippet_hash": "fd240a9dc053",
L90:         "display": "package.json:7"
L91:       }
L92:     }
L93:   ],
L94:   "run_prod": [
L95:     {
L96:       "step": "Build for production",
L97:       "command": "npm run build",
L98:       "evidence": {
L99:         "path": "package.json",
L100:         "line_start": 8,
L101:         "line_end": 8,
L102:         "snippet_hash": "79d8bdf275d6",
L103:         "display": "package.json:8"
L104:       }
L105:     },
L106:     {
L107:       "step": "Start production",
L108:       "command": "npm start",
L109:       "evidence": {
L110:         "path": "package.json",
L111:         "line_start": 9,
L112:         "line_end": 9,
L113:         "snippet_hash": "020435ddf436",
L114:         "display": "package.json:9"
L115:       }
L116:     }
L117:   ],
L118:   "usage_examples": [],
L119:   "verification_steps": [],
L120:   "common_failures": [],
L121:   "unknowns": [
L122:     {
L123:       "what_is_missing": "Semantic analysis of code purpose and architecture",
L124:       "why_it_matters": "Cannot determine system intent, integration patterns, or risk factors without LLM analysis",
L125:       "what_evidence_needed": "Re-run without --no-llm flag for full analysis"
L126:     }
L127:   ],
L128:   "missing_evidence_requests": [],
L129:   "replit_execution_profile": {
L130:     "run_command": "npm run dev",
L131:     "language": "nodejs",
L132:     "port_binding": {
L133:       "port": null,
L134:       "binds_all_interfaces": true,
L135:       "uses_env_port": true,
L136:       "evidence": [
L137:         {
L138:           "path": "server/index.ts",
L139:           "line_start": 92,
L140:           "line_end": 92,
L141:           "snippet_hash": "75d345a78f84",
L142:           "display": "server/index.ts:92"
L143:         },
L144:         {
L145:           "path": "server/index.ts",
L146:           "line_start": 96,
L147:           "line_end": 96,
L148:           "snippet_hash": "9b7206f3d09a",
L149:           "display": "server/index.ts:96"
L150:         }
L151:       ]
L152:     },
L153:     "required_secrets": [
L154:       {
L155:         "name": "DATABASE_URL",
L156:         "referenced_in": [
L157:           {
L158:             "path": "drizzle.config.ts",
L159:             "line_start": 3,
L160:             "line_end": 3,
L161:             "snippet_hash": "a19790628fbe",
L162:             "display": "drizzle.config.ts:3"
L163:           },
L164:           {
L165:             "path": "drizzle.config.ts",
L166:             "line_start": 12,
L167:             "line_end": 12,
L168:             "snippet_hash": "1005be19f14a",
L169:             "display": "drizzle.config.ts:12"
L170:           },
L171:           {
L172:             "path": "server/db.ts",
L173:             "line_start": 7,
L174:             "line_end": 7,
L175:             "snippet_hash": "a19790628fbe",
L176:             "display": "server/db.ts:7"
L177:           },
L178:           {
L179:             "path": "server/db.ts",
L180:             "line_start": 13,
L181:             "line_end": 13,
L182:             "snippet_hash": "111f33de9945",
L183:             "display": "server/db.ts:13"
L184:           }
L185:         ]
L186:       },
L187:       {
L188:         "name": "AI_INTEGRATIONS_OPENAI_API_KEY",
L189:         "referenced_in": [
L190:           {
L191:             "path": "server/replit_integrations/audio/client.ts",
L192:             "line_start": 10,
L193:             "line_end": 10,
L194:             "snippet_hash": "05da5f1b1281",
L195:             "display": "server/replit_integrations/audio/client.ts:10"
L196:           },
L197:           {
L198:             "path": "server/replit_integrations/chat/routes.ts",
L199:             "line_start": 6,
L200:             "line_end": 6,
L201:             "snippet_hash": "05da5f1b1281",
L202:             "display": "server/replit_integrations/chat/routes.ts:6"
L203:           },
L204:           {
L205:             "path": "server/replit_integrations/image/client.ts",
L206:             "line_start": 6,
L207:             "line_end": 6,
L208:             "snippet_hash": "05da5f1b1281",
L209:             "display": "server/replit_integrations/image/client.ts:6"
L210:           }
L211:         ]
L212:       },
L213:       {
L214:         "name": "AI_INTEGRATIONS_OPENAI_BASE_URL",
L215:         "referenced_in": [
L216:           {
L217:             "path": "server/replit_integrations/audio/client.ts",
L218:             "line_start": 11,
L219:             "line_end": 11,
L220:             "snippet_hash": "1f70e6a77d42",
L221:             "display": "server/replit_integrations/audio/client.ts:11"
L222:           },
L223:           {
L224:             "path": "server/replit_integrations/chat/routes.ts",
L225:             "line_start": 7,
L226:             "line_end": 7,
L227:             "snippet_hash": "1f70e6a77d42",
L228:             "display": "server/replit_integrations/chat/routes.ts:7"
L229:           },
L230:           {
L231:             "path": "server/replit_integrations/image/client.ts",
L232:             "line_start": 7,
L233:             "line_end": 7,
L234:             "snippet_hash": "1f70e6a77d42",
L235:             "display": "server/replit_integrations/image/client.ts:7"
L236:           }
L237:         ]
L238:       }
L239:     ],
L240:     "external_apis": [
L241:       {
L242:         "api": "OpenAI",
L243:         "evidence_files": [
L244:           {
L245:             "path": "server/replit_integrations/audio/client.ts",
L246:             "line_start": 1,
L247:             "line_end": 1,
L248:             "snippet_hash": "1d3dd608c3bb",
L249:             "display": "server/replit_integrations/audio/client.ts:1"
L250:           },
L251:           {
L252:             "path": "server/replit_integrations/audio/routes.ts",
L253:             "line_start": 3,
L254:             "line_end": 3,
L255:             "snippet_hash": "2f87d29d3b03",
L256:             "display": "server/replit_integrations/audio/routes.ts:3"
L257:           },
L258:           {
L259:             "path": "server/replit_integrations/chat/routes.ts",
L260:             "line_start": 2,
L261:             "line_end": 2,
L262:             "snippet_hash": "4db7290b0afd",
L263:             "display": "server/replit_integrations/chat/routes.ts:2"
L264:           },
L265:           {
L266:             "path": "server/replit_integrations/image/routes.ts",
L267:             "line_start": 2,
L268:             "line_end": 2,
L269:             "snippet_hash": "7fd5b3abbeee",
L270:             "display": "server/replit_integrations/image/routes.ts:2"
L271:           },
L272:           {
L273:             "path": "server/replit_integrations/image/client.ts",
L274:             "line_start": 2,
L275:             "line_end": 2,
L276:             "snippet_hash": "1d3dd608c3bb",
L277:             "display": "server/replit_integrations/image/client.ts:2"
L278:           }
L279:         ]
L280:       }
L281:     ],
L282:     "deployment_assumptions": [
L283:       "Binds to 0.0.0.0 (all interfaces)",
L284:       "Requires 3 secret(s): DATABASE_URL, AI_INTEGRATIONS_OPENAI_API_KEY, AI_INTEGRATIONS_OPENAI_BASE_URL"
L285:     ],
L286:     "observability": {
L287:       "logging": true,
L288:       "health_endpoint": true,
L289:       "evidence": [
L290:         {
L291:           "path": "script/build.ts",
L292:           "line_start": 38,
L293:           "line_end": 38,
L294:           "snippet_hash": "2f74cc3fdab1",
L295:           "display": "script/build.ts:38"
L296:         },
L297:         {
L298:           "path": "server/routes.ts",
L299:           "line_start": 15,
L300:           "line_end": 15,
L301:           "snippet_hash": "f7ba68760271",
L302:           "display": "server/routes.ts:15"
L303:         },
L304:         {
L305:           "path": "shared/schema.ts",
L306:           "line_start": 10,
L307:           "line_end": 10,
L308:           "snippet_hash": "6ccc8e5d45a7",
L309:           "display": "shared/schema.ts:10"
L310:         }
L311:       ]
L312:     },
L313:     "limitations": [
L314:       "Deterministic mode (--no-llm): no semantic analysis performed"
L315:     ]
L316:   },
L317:   "completeness": {
L318:     "score": 62,
L319:     "max": 100,
L320:     "missing": [
L321:       "verification_steps: no step with both a runnable command and verified evidence",
L322:       "usage_examples: no examples with meaningful descriptions"
L323:     ],
L324:     "deductions": [
L325:       "-3 for 1 unknown(s)"
L326:     ],
L327:     "notes": "-3 for 1 unknown(s); 1 unknown(s) reported"
L328:   }
L329: }
L330: ```
L331: 
L332: ## 4. Limitations
L333: - This dossier was generated in `--no-llm` mode
L334: - No semantic analysis, claims extraction, or architecture inference was performed
L335: - For full analysis, re-run without `--no-llm`

--- FILE: output/REPORT_ENGINEER.md ---
L1: # Program Totality Report — Engineer View
L2: 
L3: **EvidencePack Version:** 1.0
L4: **Tool Version:** 0.1.0
L5: **Generated:** 2026-02-15T09:28:36.848729+00:00
L6: **Mode:** replit
L7: **Run ID:** 2e2b0d0dad53
L8: 
L9: ---
L10: 
L11: ## PTA Contract Audit — Run 2e2b0d0dad53
L12: 
L13: ### 1. System Snapshot
L14: 
L15: | Measure | Value |
L16: |---------|-------|
L17: | Files Analyzed | 169 |
L18: | Files Seen (incl. skipped) | 195 |
L19: | Files Skipped | 26 |
L20: | Claims Extracted | 17 |
L21: | Claims with Deterministic Evidence | 15 |
L22: | Unknown Governance Categories | 9 |
L23: | Verified Structural Categories | 0 |
L24: | Partial Coverage | Yes |
L25: 
L26: ### 2. Deterministic Coverage Index (DCI v1)
L27: 
L28: **Score:** 88.24%
L29: **Formula:** `verified_claims / total_claims`
L30: 
L31: 15 of 17 extracted claims contain hash-verified evidence.
L32: 
L33: This measures claim-to-evidence visibility only.
L34: It does not measure code quality, security posture, or structural surface coverage.
L35: 
L36: ### 3. Reporting Completeness Index (RCI)
L37: 
L38: **Score:** 50.08%
L39: **Formula:** `average(claims_coverage, unknowns_coverage, howto_completeness)`
L40: 
L41: | Component | Score |
L42: |-----------|-------|
L43: | claims_coverage | 88.24% |
L44: | unknowns_coverage | 0.00% |
L45: | howto_completeness | 62.00% |
L46: 
L47: RCI is a documentation completeness metric.
L48: It is not a security score and does not imply structural sufficiency.
L49: 
L50: ### 4. Structural Visibility (DCI v2)
L51: 
L52: **Status:** not_implemented
L53: **Formula (reserved):** `verified_structural_items / total_structural_surface`
L54: 
L55: Routes, dependencies, schemas, and enforcement extractors are not active.
L56: Structural surface visibility is intentionally reported as null rather than estimated.
L57: This prevents silent overstatement of governance posture.
L58: 
L59: ### 5. Epistemic Posture
L60: 
L61: PTA explicitly reports:
L62: - What is deterministically verified.
L63: - What is unknown.
L64: - What is not implemented.
L65: - What requires dedicated extractors.
L66: 
L67: There is no inference-based promotion from UNKNOWN to VERIFIED.
L68: 
L69: ---
L70: 
L71: ## Verified: Data & Security Posture
L72: 
L73: ### System requires 3 secret(s): DATABASE_URL, AI_INTEGRATIONS_OPENAI_API_KEY, AI_INTEGRATIONS_OPENAI_BASE_URL
L74: Confidence: 55%
L75: - Evidence: `drizzle.config.ts:3` (hash: `a19790628fbe`)
L76: - Evidence: `server/replit_integrations/audio/client.ts:10` (hash: `05da5f1b1281`)
L77: - Evidence: `server/replit_integrations/audio/client.ts:11` (hash: `1f70e6a77d42`)
L78: 
L79: ### Secret "DATABASE_URL" is referenced in 4 file(s)
L80: Confidence: 50%
L81: - Evidence: `drizzle.config.ts:3` (hash: `a19790628fbe`)
L82: - Evidence: `drizzle.config.ts:12` (hash: `1005be19f14a`)
L83: 
L84: ### Secret "AI_INTEGRATIONS_OPENAI_API_KEY" is referenced in 3 file(s)
L85: Confidence: 50%
L86: - Evidence: `server/replit_integrations/audio/client.ts:10` (hash: `05da5f1b1281`)
L87: - Evidence: `server/replit_integrations/chat/routes.ts:6` (hash: `05da5f1b1281`)
L88: 
L89: ### Secret "AI_INTEGRATIONS_OPENAI_BASE_URL" is referenced in 3 file(s)
L90: Confidence: 50%
L91: - Evidence: `server/replit_integrations/audio/client.ts:11` (hash: `1f70e6a77d42`)
L92: - Evidence: `server/replit_integrations/chat/routes.ts:7` (hash: `1f70e6a77d42`)
L93: 
L94: ## Verified: How to Use the Target System
L95: 
L96: ### npm script "dev" runs: NODE_ENV=development tsx server/index.ts
L97: Confidence: 60%
L98: - Evidence: `package.json:7` (hash: `fd240a9dc053`)
L99: 
L100: ### npm script "build" runs: tsx script/build.ts
L101: Confidence: 60%
L102: - Evidence: `package.json:8` (hash: `79d8bdf275d6`)
L103: 
L104: ### npm script "start" runs: NODE_ENV=production node dist/index.cjs
L105: Confidence: 60%
L106: - Evidence: `package.json:9` (hash: `020435ddf436`)
L107: 
L108: ### Server binds to 0.0.0.0 (all interfaces)
L109: Confidence: 55%
L110: - Evidence: `server/index.ts:92` (hash: `75d345a78f84`)
L111: - Evidence: `server/index.ts:96` (hash: `9b7206f3d09a`)
L112: 
L113: ### Server port is configured via environment variable
L114: Confidence: 55%
L115: - Evidence: `server/index.ts:92` (hash: `75d345a78f84`)
L116: - Evidence: `server/index.ts:96` (hash: `9b7206f3d09a`)
L117: 
L118: ### Replit run command: npm run dev
L119: Confidence: 55%
L120: - Evidence: `.replit:2` (hash: `96fa2e5505e4`)
L121: - Evidence: `.replit:5` (hash: `550b970621c2`)
L122: 
L123: ## Verified: Integration Surface
L124: 
L125: ### Key dependencies: drizzle-orm, express, openai, react
L126: Confidence: 50%
L127: - Evidence: `package.json:13` (hash: `f7eebadd079d`)
L128: 
L129: ### External API dependency: OpenAI
L130: Confidence: 45%
L131: - Evidence: `server/replit_integrations/audio/client.ts:1` (hash: `1d3dd608c3bb`)
L132: - Evidence: `server/replit_integrations/audio/routes.ts:3` (hash: `2f87d29d3b03`)
L133: 
L134: ### Database schema/migration files detected: drizzle.config.ts, server/db.ts, shared/schema.ts
L135: Confidence: 40%
L136: - Evidence: `drizzle.config.ts:1` (hash: `1f5c93c3d974`)
L137: - Evidence: `server/db.ts:1` (hash: `3d66d6ea5af3`)
L138: 
L139: ## Verified: What the Target System Is
L140: 
L141: ### The project is named "rest-express" (from package.json)
L142: Confidence: 60%
L143: - Evidence: `package.json:2` (hash: `a1f1a980b4b8`)
L144: 
L145: ### Python project named "program-totality-analyzer" (from pyproject.toml)
L146: Confidence: 50%
L147: - Evidence: `pyproject.toml:2` (hash: `f0d4a96fe7d6`)
L148: 
L149: ## Verified Structural (deterministic extractors only)
L150: 
L151: - **dependencies**: not_implemented: requires lockfile parser (package-lock.json, requirements.txt, etc.)
L152: - **enforcement**: not_implemented: requires auth/middleware pattern detector over source files
L153: - **routes**: not_implemented: requires AST/regex route extractor over source files
L154: - **schemas**: not_implemented: requires migration/model file parser
L155: 
L156: ## Known Unknown Surface
L157: 
L158: | Category | Status | Notes |
L159: |----------|--------|-------|
L160: | tls_termination | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L161: | encryption_at_rest | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L162: | secret_management | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L163: | deployment_topology | UNKNOWN | Candidate artifact files found (Dockerfile) but artifact detector not yet implemented — cannot read/hash/verify file content |
L164: | runtime_iam | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L165: | logging_sink | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L166: | monitoring_alerting | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L167: | backup_retention | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L168: | data_residency | UNKNOWN | No matching infrastructure/config artifacts found in file index |
L169: 
L170: ## Snippet Hashes (20 total)
L171: 
L172: - `020435ddf436`
L173: - `053150b640a7`
L174: - `05da5f1b1281`
L175: - `1005be19f14a`
L176: - `1d3dd608c3bb`
L177: - `1f5c93c3d974`
L178: - `1f70e6a77d42`
L179: - `2f87d29d3b03`
L180: - `3d66d6ea5af3`
L181: - `50c86b7ed8ac`
L182: - `550b970621c2`
L183: - `75d345a78f84`
L184: - `79d8bdf275d6`
L185: - `96fa2e5505e4`
L186: - `9b7206f3d09a`
L187: - `a19790628fbe`
L188: - `a1f1a980b4b8`
L189: - `f0d4a96fe7d6`
L190: - `f7eebadd079d`
L191: - `fd240a9dc053`

--- FILE: output/packs/docs_pack.txt ---
L1: 
L2: --- FILE: replit.md ---
L3: L1: # Overview
L4: L2: 
L5: L3: **Program Totality Analyzer** — a full-stack web application that ingests software projects (via GitHub URL, local path, or live Replit workspace) and produces evidence-cited technical dossiers. The dossier covers what a target system is, how it works, how to use it, and what risks/unknowns exist. It combines a React frontend for submitting analysis requests and viewing results with an Express backend that manages projects/analyses in PostgreSQL and spawns a Python-based analyzer CLI for the actual code analysis.
L6: L4: 
L7: L5: ## User Preferences
L8: L6: 
L9: L7: Preferred communication style: Simple, everyday language.
L10: L8: 
L11: L9: ## System Architecture
L12: L10: 
L13: L11: ### Monorepo Structure
L14: L12: 
L15: L13: The project follows a three-zone monorepo pattern:
L16: L14: 
L17: L15: - **`client/`** — React SPA (frontend)
L18: L16: - **`server/`** — Express API (backend)
L19: L17: - **`shared/`** — Shared types, schemas, and route definitions used by both client and server
L20: L18: 
L21: L19: This avoids type drift between frontend and backend by sharing Zod schemas and TypeScript types from a single source of truth.
L22: L20: 
L23: L21: ### Frontend (`client/src/`)
L24: L22: 
L25: L23: - **Framework**: React 18 with TypeScript
L26: L24: - **Routing**: Wouter (lightweight client-side router)
L27: L25: - **State/Data Fetching**: TanStack React Query with polling for analysis status updates
L28: L26: - **UI Components**: shadcn/ui (new-york style) built on Radix UI primitives
L29: L27: - **Styling**: Tailwind CSS with CSS variables for theming (dark mode, cyan/neon aesthetic)
L30: L28: - **Animations**: Framer Motion for page transitions and loading states
L31: L29: - **Markdown Rendering**: react-markdown for displaying analysis dossiers
L32: L30: - **Build Tool**: Vite with React plugin
L33: L31: 
L34: L32: Key pages:
L35: L33: - `/` — Home page with URL input form and "Analyze Replit" button
L36: L34: - `/projects` — List of previous analyses
L37: L35: - `/projects/:id` — Detailed view of a specific analysis with tabs for dossier, claims, operator dashboard (operate.json), coverage, and unknowns
L38: L36: 
L39: L37: Path aliases: `@/` maps to `client/src/`, `@shared/` maps to `shared/`, `@assets/` maps to `attached_assets/`.
L40: L38: 
L41: L39: ### Backend (`server/`)
L42: L40: 
L43: L41: - **Framework**: Express 5 on Node.js
L44: L42: - **Language**: TypeScript, run via `tsx` in dev
L45: L43: - **API Pattern**: REST API under `/api/` prefix, route definitions shared via `shared/routes.ts`
L46: L44: - **Dev Server**: Vite middleware in development (HMR via `server/vite.ts`), static file serving in production (`server/static.ts`)
L47: L45: - **Build**: esbuild bundles server to `dist/index.cjs`; Vite builds client to `dist/public/`
L48: L46: 
L49: L47: Key API routes (defined in `server/routes.ts`):
L50: L48: - `GET /api/projects` — List all projects
L51: L49: - `POST /api/projects` — Create a new project (with mode: github/local/replit)
L52: L50: - `GET /api/projects/:id` — Get project details
L53: L51: - `GET /api/projects/:id/analysis` — Get analysis results
L54: L52: - `POST /api/projects/:id/analyze` — Trigger analysis (spawns Python CLI)
L55: L53: 
L56: L54: ### Python Analyzer (`server/analyzer/`)
L57: L55: 
L58: L56: - **CLI**: `analyzer_cli.py` using Typer, supports three input modes:
L59: L57:   - GitHub URL (`analyze <url>`)
L60: L58:   - Local path (`analyze <path>`)
L61: L59:   - Replit workspace (`analyze --replit`)
L62: L60: - **Core**: `server/analyzer/src/analyzer.py` — orchestrates file acquisition, indexing, and LLM-powered analysis
L63: L61: - **Operate Module**: `server/analyzer/src/core/operate.py` — deterministic (no LLM) extraction of operational data into `operate.json`
L64: L62:   - Extracts boot commands, ports, integration points (endpoints, env vars, auth), deployment config, and runbook steps
L65: L63:   - Uses three evidence tiers: EVIDENCED (file:line + SHA-256 snippet hash), INFERRED, UNKNOWN (with unknown_reason)
L66: L64:   - Computes readiness scores (0-100) for boot, integrate, deploy categories
L67: L65:   - Identifies operational gaps with severity ratings
L68: L66: - **LLM Integration**: OpenAI API (via Replit AI Integrations env vars: `AI_INTEGRATIONS_OPENAI_API_KEY`, `AI_INTEGRATIONS_OPENAI_BASE_URL`)
L69: L67: - The Express server spawns the Python analyzer as a child process
L70: L68: 
L71: L69: ### Database
L72: L70: 
L73: L71: - **Engine**: PostgreSQL (required, referenced via `DATABASE_URL` env var)
L74: L72: - **ORM**: Drizzle ORM with `drizzle-zod` for schema-to-Zod validation
L75: L73: - **Schema** (`shared/schema.ts`):
L76: L74:   - `projects` — id, url, name, mode (github/local/replit), status (pending/analyzing/completed/failed), createdAt
L77: L75:   - `analyses` — id, projectId, dossier (markdown text), claims (jsonb), howto (jsonb), coverage (jsonb), unknowns (jsonb), operate (jsonb), createdAt
L78: L76: - **Chat models** (`shared/models/chat.ts`):
L79: L77:   - `conversations` — id, title, createdAt
L80: L78:   - `messages` — id, conversationId, role, content, createdAt
L81: L79: - **Migrations**: Drizzle Kit with `drizzle-kit push` for schema sync
L82: L80: - **Storage Layer**: `server/storage.ts` implements `IStorage` interface with `DatabaseStorage` class
L83: L81: 
L84: L82: ### Replit Integrations (`server/replit_integrations/` and `client/replit_integrations/`)
L85: L83: 
L86: L84: Pre-built integration modules for AI features:
L87: L85: - **Chat** — Text-based conversation routes and storage using OpenAI
L88: L86: - **Audio** — Voice recording, playback, speech-to-text, text-to-speech with AudioWorklet
L89: L87: - **Image** — Image generation and editing via `gpt-image-1`
L90: L88: - **Batch** — Rate-limited batch processing with retries for LLM calls
L91: L89: 
L92: L90: These are utility modules that can be registered on the Express app as needed.
L93: L91: 
L94: L92: ### Key Design Decisions
L95: L93: 
L96: L94: 1. **Shared route definitions** — `shared/routes.ts` defines API contracts (paths, input schemas, response schemas) used by both frontend hooks and backend handlers. This ensures type safety across the stack.
L97: L95: 
L98: L96: 2. **Python + Node hybrid** — The analyzer logic lives in Python (better ecosystem for code analysis, rich CLI output) while the web layer is Node/Express. The server spawns Python as a child process rather than using a microservice architecture, keeping deployment simple.
L99: L97: 
L100: L98: 3. **Evidence-first analysis** — The analyzer is designed to cite file paths and line ranges for every claim. When evidence is missing, it must label findings as inference/unknown rather than hallucinate.
L101: L99: 
L102: L100: 4. **Polling for status** — The frontend polls project status every 2 seconds while analysis is in progress, switching to static once completed/failed.
L103: L101: 
L104: L102: ## External Dependencies
L105: L103: 
L106: L104: ### Required Services
L107: L105: - **PostgreSQL** — Primary database, must be provisioned with `DATABASE_URL` environment variable
L108: L106: - **OpenAI API** (via Replit AI Integrations) — Powers the code analysis LLM calls
L109: L107:   - `AI_INTEGRATIONS_OPENAI_API_KEY` — API key
L110: L108:   - `AI_INTEGRATIONS_OPENAI_BASE_URL` — Base URL for API
L111: L109: 
L112: L110: ### Key NPM Packages
L113: L111: - `express` v5 — HTTP server
L114: L112: - `drizzle-orm` + `drizzle-kit` — Database ORM and migrations
L115: L113: - `@tanstack/react-query` — Client-side data fetching and caching
L116: L114: - `wouter` — Client-side routing
L117: L115: - `react-markdown` — Markdown rendering for dossiers
L118: L116: - `framer-motion` — Animations
L119: L117: - `zod` + `drizzle-zod` — Runtime validation
L120: L118: - `vite` — Frontend build and dev server
L121: L119: - `esbuild` — Server build
L122: L120: 
L123: L121: ### Key Python Packages
L124: L122: - `typer` — CLI framework
L125: L123: - `openai` — LLM API client
L126: L124: - `rich` — Console output formatting
L127: L125: - `python-dotenv` — Environment variable loading
L128: L126: 
L129: L127: ### Dev/Build Tools
L130: L128: - `tsx` — TypeScript execution for development
L131: L129: - `tailwindcss` + `postcss` + `autoprefixer` — CSS toolchain
L132: L130: - `@replit/vite-plugin-runtime-error-modal` — Dev error overlay
L133: 
L134: --- FILE: README.md ---
L135: L1: # Program Totality Analyzer
L136: L2: 
L137: L3: A static-artifact-anchored analysis tool that generates technical dossiers for software projects. It extracts what a system is, how to run it, what it needs, and what it cannot determine — with every claim citing `file:line` evidence backed by SHA-256 snippet hashes.
L138: L4: 
L139: L5: **Scope limitation:** PTA analyzes static artifacts only (source files, config, lockfiles). It does not observe runtime behavior, prove correctness, or guarantee security. Claims labeled VERIFIED mean "anchored to a hash-verified source snippet," not "proven true at runtime."
L140: L6: 
L141: L7: ## What It Does
L142: L8: 
L143: L9: Given a software project (GitHub repo, local folder, or Replit workspace), the analyzer produces:
L144: L10: 
L145: L11: | File | Contents |
L146: L12: |------|----------|
L147: L13: | `operate.json` | Operator dashboard: boot commands, integration points, deployment config, readiness scores, gaps with severity. Deterministic, evidence-bound. Every item is EVIDENCED, INFERRED, or UNKNOWN. |
L148: L14: | `target_howto.json` | Legacy: evidence-scoped run steps. Prefer `operate.json` for operator workflows. |
L149: L15: | `claims.json` | Verifiable claims about the system, each with file:line evidence and confidence scores |
L150: L16: | `coverage.json` | Scan metadata: files scanned, files skipped, Replit detection evidence |
L151: L17: | `replit_profile.json` | Replit-specific: port binding, secrets, external APIs, observability (only in Replit mode) |
L152: L18: | `DOSSIER.md` | Human-readable markdown dossier summarizing all findings |
L153: L19: | `index.json` | Full file index of scanned files |
L154: L20: | `packs/` | Evidence packs (docs, config, code, ops) used during analysis |
L155: L21: 
L156: L22: ## Install
L157: L23: 
L158: L24: ```bash
L159: L25: pip install -e .
L160: L26: ```
L161: L27: 
L162: L28: This registers the `pta` command. Alternatively, run as a module or directly:
L163: L29: 
L164: L30: ```bash
L165: L31: python -m server.analyzer.src --help
L166: L32: python server/analyzer/analyzer_cli.py --help
L167: L33: ```
L168: L34: 
L169: L35: ### Dependencies
L170: L36: 
L171: L37: - Python 3.11+
L172: L38: - Required packages: `typer`, `rich`, `openai`, `gitpython`, `jsonschema`, `python-dotenv`, `pydantic`
L173: L39: 
L174: L40: ## Usage
L175: L41: 
L176: L42: ### Three Modes
L177: L43: 
L178: L44: **GitHub repository:**
L179: L45: ```bash
L180: L46: pta analyze https://github.com/user/repo -o ./output
L181: L47: ```
L182: L48: 
L183: L49: **Local folder:**
L184: L50: ```bash
L185: L51: pta analyze ./path/to/project -o ./output
L186: L52: ```
L187: L53: 
L188: L54: **Replit workspace (run from inside the workspace):**
L189: L55: ```bash
L190: L56: pta analyze --replit -o ./output
L191: L57: ```
L192: L58: 
L193: L59: ### Deterministic Mode (`--no-llm`)
L194: L60: 
L195: L61: Skip all LLM calls and produce only deterministic, structurally-extracted outputs:
L196: L62: 
L197: L63: ```bash
L198: L64: pta analyze --replit --no-llm -o ./output
L199: L65: ```
L200: L66: 
L201: L67: This mode requires no API keys and produces reproducible results. It generates `operate.json` and readiness scoring without any LLM involvement. It extracts:
L202: L68: - Package scripts (dev, build, start)
L203: L69: - Lockfile-based install commands
L204: L70: - Environment variable references (names only, never values)
L205: L71: - Port binding configuration
L206: L72: - External API usage
L207: L73: - Replit platform detection
L208: L74: - Operational gaps with severity ratings
L209: L75: - Readiness scores (boot, integrate, deploy)
L210: L76: 
L211: L77: ### With LLM Analysis
L212: L78: 
L213: L79: For semantic analysis (architecture understanding, risk assessment, integration patterns):
L214: L80: 
L215: L81: ```bash
L216: L82: pta analyze --replit -o ./output
L217: L83: ```
L218: L84: 
L219: L85: Requires `AI_INTEGRATIONS_OPENAI_API_KEY` and `AI_INTEGRATIONS_OPENAI_BASE_URL` environment variables.
L220: L86: 
L221: L87: ### Scoping a Subdirectory
L222: L88: 
L223: L89: ```bash
L224: L90: pta analyze https://github.com/user/monorepo --root packages/api -o ./output
L225: L91: ```
L226: L92: 
L227: L93: ## Evidence Model
L228: L94: 
L229: L95: Every claim in the output cites structured evidence:
L230: L96: 
L231: L97: ```json
L232: L98: {
L233: L99:   "path": "server/index.ts",
L234: L100:   "line_start": 92,
L235: L101:   "line_end": 92,
L236: L102:   "snippet_hash": "75d345a78f84",
L237: L103:   "display": "server/index.ts:92"
L238: L104: }
L239: L105: ```
L240: L106: 
L241: L107: - `path` -- file path relative to project root
L242: L108: - `line_start` / `line_end` -- 1-indexed line range (never 0)
L243: L109: - `snippet_hash` -- first 12 hex chars of SHA-256 of the stripped line(s)
L244: L110: - `display` -- human-readable location string
L245: L111: 
L246: L112: For file-existence evidence (e.g., lockfile detection):
L247: L113: 
L248: L114: ```json
L249: L115: {
L250: L116:   "kind": "file_exists",
L251: L117:   "path": "package-lock.json",
L252: L118:   "snippet_hash": "053150b640a7",
L253: L119:   "display": "package-lock.json (file exists)"
L254: L120: }
L255: L121: ```
L256: L122: 
L257: L123: ### Gap Severity
L258: L124: 
L259: L125: Operational gaps in `operate.json` include a severity rating:
L260: L126: - **high** — blocks boot or core execution
L261: L127: - **medium** — impacts deployment maturity
L262: L128: - **low** — best-practice or observability improvements
L263: L129: 
L264: L130: ### Verification
L265: L131: 
L266: L132: Snippet hashes are re-checked against source files: the analyzer re-reads the cited line range, strips whitespace, hashes the result, and confirms it matches the claimed hash. Claims that fail hash verification are capped at confidence 0.20 and marked `"status": "unverified"`.
L267: L133: 
L268: L134: **Important:** Hash verification confirms that a snippet exists at the cited location. It does not prove that the code behaves as described, is secure, or is free of bugs. PTA is not a security scanner, compliance certification tool, or correctness prover.
L269: L135: 
L270: L136: ### Whitespace Policy
L271: L137: 
L272: L138: Lines are stripped (trimmed) before hashing. This normalizes indentation differences across editors and formatters. Both evidence creation and verification use the same canonicalization.
L273: L139: 
L274: L140: ## Security
L275: L141: 
L276: L142: - **Symlink protection**: Every path component is checked. If any component in the path tree is a symlink, the file is rejected.
L277: L143: - **Path containment**: All resolved paths must remain within the project root (`relative_to()` check after `resolve()`).
L278: L144: - **Binary detection**: Null bytes in the first 4KB trigger rejection before text decoding.
L279: L145: - **Traversal prevention**: `..` segments and absolute paths are rejected.
L280: L146: - **Secret safety**: Only environment variable names are extracted, never their values.
L281: L147: - **Self-skip**: The analyzer excludes its own source files from analysis to prevent false-positive pattern matches.
L282: L148: 
L283: L149: ## Output Files
L284: L150: 
L285: L151: ### `operate.json`
L286: L152: 
L287: L153: Operator dashboard model with:
L288: L154: - `boot` -- install, dev, prod commands and port bindings, each with evidence tier
L289: L155: - `integrate` -- endpoints, env vars, auth mechanisms with evidence
L290: L156: - `deploy` -- Docker, platform hints, CI/CD, build commands with evidence
L291: L157: - `read